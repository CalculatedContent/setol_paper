\vspace{-3mm}
\section{Conclusion}
\label{sxn:conclusion}
\vspace{-1mm}


A goal of the \HTSR~theory is to develop model quality metrics, like \ALPHA~and \ALPHAHAT, that can be applied to so-called pre-trained DNN models, by only watching the weights, i.e., without the need for access to training/testing data. 
Previous work demonstrated the utility of the \ALPHAHAT~metric when applied to a wide range of large scale and production, pre-trained CV and NLP models~\cite{MM20a_trends_NatComm}. 
Here, we seek to better understand why the \ALPHAHAT~metric works so well.

To accomplish this, we evaluated the \ALPHAHAT~metric and its subcomponent metrics, 
\ALPHA~and \SPECTRALNORM, on a set of publicly-available pre-trained models made available from a recent machine learning contest aimed at understanding causes of good generalization.
To our initial surprise, we identified a clear Simpson's paradox in the data.
Based on our exploration of that, we developed an improved understanding of the complementary roles of \SCALE metrics versus 
\SHAPE metrics in evaluating model quality.
%
Overall, our analysis explains the success of previously-introduced metrics that combine norm information and shape/correlational information~\cite{MM20a_trends_NatComm} as well as previously-observed peculiarities of norm-based metrics~\cite{JNBx19_fantastic_TR}.
Our results also highlight the need to go beyond one-size-fits-all metrics based on upper bounds from generalization theory to describe the performance of SOTA NNs, as well as the evaluation of models by expensive retraining on test data. 
Test data can be extremely expensive to acquire, and there is a need to evaluate models without (much or any) test data.  %~\cite{MM20a_trends_NatComm}.

%We aim to develop a theory of generalization that practioners can \emph{use} in their day-to-day training, fine-tuning, validation, and evaluation of models.  
%Practioners do not just want better regularizers, but also cheaper, systemtatic ways of adjusting individual layer hyperparameters; and
%practioners frequently use pre-trained models without access to training data.
Based on our findings, we expect that \SPECTRALNORM (and related \SCALE-based metrics) can capture coarse model trends (e.g., large-scale architectural changes, such as width or depth or perhaps adding residual connections and convolutional width) more generally; and that \ALPHA~(and related \SHAPE-based metrics) can capture fine-scale model properties (e.g., fine-scale optimizer/solver hyperparameters changes, such as batch size, step scale, etc.) more generally.
Understanding this better is an important future direction raised by our work.
Clearly, our results also have implications beyond generalization and even model training, to problems such as fine-tuning of pre-trained models, improved model engineering, and improved neural architecture search.
(See Appendix~\ref{app:additional-discussion} for some addidional discussion.)
These too are important future directions raised by our work.




%%ICML%%Just as the data-independent \SPECTRALNORM and \ALPHA~have complementary interpretations in terms of ``scale'' and ``shape'' parameters, so too
%%ICML%%the data-dependent metrics \SHARPNESS and \SVDSMOOTHING can be thought of as ``scale'' and ``shape'' parameters.
%%ICML%%\SHARPNESS controls the scale of the layer $\mathbf{W}$ by clipping large elements $W_{i,j}$.
%%ICML%%\SVDSMOOTHING captures the shape by keeping the eigenvectors corresponding to the tail of the layer ESD.
%%ICML%%%
%%ICML%%We expect this would be particularly useful in situations where test data is limited/unavailable, and/or when peeking at the test data could cause the model to overfit (as in quantitative finance / market prediction).
%%ICML%%
%%ICML%%Note that our use of low-rank approximation in \SVDSMOOTHING is very different than how low-rank approximations are used in training/validating models, and to our knowledge is has not been suggested before to estimate the test error. 
%%ICML%%Essentially, it involves taking a well-trained model, removing part of it, and using the rest to predict test performance.
%%ICML%%%
%%ICML%%While \SVDSMOOTHING uses a low-rank approximation to the weight matrix, this does \emph{not} assume that matrices are well-approximated by low-rank matrices.
%%ICML%%Indeed, the entire point of \HTSR~theory \cite{MM18_TR_JMLRversion,MM19_HTSR_ICML} is that the ESD has a heavy-tailed shape, but that a low-rank approximation to it tends to capture the most generalizing components of the layer $\mathbf{W}$.
%%ICML%%This is consistent with the observation that the shape of the tail of the layer ESD (through the PL exponent $\alpha$) can be used to predict trends in the test accuracies when varying regularization and/or solver hyperparameters.
%%ICML%%
%%ICML%%
%%ICML%%An important direction raised by our results has to do with the role of having access to data.
%%ICML%%%
%%ICML%%Neither \SPECTRALNORM nor \ALPHA~require access to the data to compute; they only require access to the trained model.
%%ICML%%%
%%ICML%%The model depends on the data, of course, but these two metrics can be evaluated without access to any training or testing data.
%%ICML%%This agrees with the setup of~\cite{MM20a_trends_NatComm}, but it is a more limited model of data access than in the contest~\cite{JNBx19_fantastic_TR}.
%%ICML%%%
%%ICML%%Intermediate between these two extremes, an important practical problem is to fine-tune a previously-trained model:
%%ICML%%can we improve transfer learning or model fine-tuning, e.g., by doing \SVDSMOOTHING before fine-tuning, e.g., in SOTA language models?
%%ICML%%%
%%ICML%%Another important practical problem has to do with developing new models: can we use these results---either data-independent \SPECTRALNORM or \ALPHA~or data-dependent \SHARPNESS or \SVDSMOOTHING to perform better architecture search, e.g., by using scale versus shape parameters to explore the search space?
%%ICML%%These are important questions raised by our results.


