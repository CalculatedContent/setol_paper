\subsection{\HTSR Theory.}

To apply \HTSR Theory in practice, one simply computes the shape of the ESDs of the NN layer weight matrices and measures the extent of the tail of each ESD, $\rho_{tail}(\lambda)$.
Generally speaking, models with heavier (i.e., larger) tails exhibit better (self-reported, empirical) performance. 
This suggests that one can predict trends in the generalization of real world DNNs simply by measuring the ``heaviness'' of the tail of their layer~ESDs.

Based on this suggestion, and following the general prescription of \HTSR~theory, \ALPHAHAT~$(\hat{\alpha})$ metric has been introduced as data-free quality metric for NNs,%
\footnote{The \ALPHAHAT~$(\hat{\alpha})$ metric is an average over DNN layer matrices of fitted power law exponents, weighted by the log spectral norm of the layer.  It is described in more detail below.} 
and it has been applied in a large meta-analysis of hundreds of SOTA pre-trained publicly-available NN models in CV and NLP~\cite{MM20a_trends_NatComm,YTHx22_TR,YTHx23_KDD}.
%
These empirical results show that the \ALPHAHAT metric can predict trends in the quality, or generalization accuracy, of SOTA NN models---\emph{even without access to any training or testing data}~\cite{MM20a_trends_NatComm}.
Generally speaking, the HTSR shape-based metrics outperforms all other metrics studied, including those from statistical learning theory (SLT), and including those that assume access to the training/testing data~\cite{YTHx22_TR,YTHx23_KDD}.
While \ALPHAHAT is based on the \HTSR~phenomenology, this leads to the deeper question: 
%
\begin{center}
\emph{Why does the \ALPHAHAT NN model quality metric work so well?}
\end{center}
%
To answer this question, we will derive the \ALPHAHAT metric, introducing a new \emph{semi-empirical approach to neural network theory}.
Our approach uses techniques from \STATMECH, \RMT and \HTRMT, and recent advances in the evaluation of so-called HCIZ random matrix integrals~\cite{potters_bouchaud_2020,Tanaka2008}.
\michael{Do we cite that new Tanaka paper.}
\charles{Yes, and we may even re-derive it in the appendix, using our notation}

Specifically, using the \HTSR~theory, and taking into account the extensive empirical results on SOTA NNs, we present a semi-empirical matrix generalization of the classic Student-Teacher model from \STATMECH for the generalization accuracy of a Perceptron~\cite{SST92, engle_complexity_2001,engel2001statistical,Opper01}.
While Student-Teacher models are well-known, our approach to it is different.
In particular---and importantly---by incorporating properties of empirical data from real world models, we can implicitly treat the ``strongly correlated'' nature of the training data, thereby providing a predictive model that only requires information from the spectral properties of the layer weight matrices.%
\footnote{More broadly, such an approach resembles in spirit semi-empirical theories of strongly correlated systems from quantum chemistry~\cite{Martin1996HighlyAA,martin1996redesigning, martin_reparametrizing_1998}.}
XXX.  MM NOTE: IF WE END THIS PAR WITH WHAT WE DO THAT IS DIFF, WE PROBABLY WANT TO END THE OTHER PARS WITH WHAT WE DO THAT IS DIFF.

