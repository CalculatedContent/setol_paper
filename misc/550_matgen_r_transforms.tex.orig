\subsection{Modeling the R-Transform}
\label{sxn:r_transforms}

\charles{NOTE: There is some subtly here to deal woith because of the branch cuts expected in $R(z)$
  We can derive \ALPHAHAT from the R-Transform, but its a bit lengthy.
  I will also work out the R transform for the truncated power law for $\alpha=2$ and maybe $3,4$ and explain}

In this section, we explain how to select the \RTransform $R(z)$ and
evaluate the~\GEN~$\GN$ under different modeling assumptions,
To apply \SETOL, the model satisfy the
\TRACELOG condition. For most cases of NN models, the
ESD are HT, and, in practice, one usually would select $R(x)$
that reflects this.  We analyze several cases, noting their applicability to real-world scenarios.
Most importantly, for the case if \IdealLearning, we derive expressions
that resemble the~\WW~\ALPHAHAT metric, at least formally.

\subsubsection{Elementary Random Matrix Theory}
We beging with some definitions from \RandomMatrixTheory.

Write the ESD $\rho(\lambda)$ as

\begin{equation}
\label{eqn:rgo}
\rho(\lambda):=\frac{1}{N}\sum_{i}\delta(\lambda-\lambda_{i})
\end{equation}

The \emph{\GreensFunction} (or \emph{\Cauchy}-Stieltjes transform) is given by
\footnote{Please notice our naming and sign convention in \EQN~\ref{eqn:Cz}.
Some authors equate the \GreensFunction $G(z)$ with
the \Cauchy-Stieltjes transform, whereas we define $C(z)=-G(z)$.}
\begin{equation}
\label{eqn:Gz}
G(z):=\int \mathrm{d}\lambda \frac{\rho(\lambda)}{z-\lambda}
\end{equation}

From $G(z)$, we can recover $\rho(\lambda)$ using the inversion relation
\begin{equation}
\label{eqn:GzInverse}
\rho(\lambda)=\lim_{\epsilon\rightarrow 0+}\frac{1}{\pi}\mathrm{IM}(C(\lambda+i\epsilon))
\end{equation}
where $\mathrm{IM}$ is the imaginary part of $G(z)$, and the $\lim_{\epsilon\rightarrow 0+}$
means to take the limit approaching from the upper half complex plane.

The \RTransform $R(z)$ is defined using the Blue function $B(z)$\cite{Zee1996},
\begin{equation}
\label{eqn:Rz}
R(z):=B(z)-\frac{1}{z}
\end{equation}

where the Blue function $B(z)$ is the functional inverse of the Greens Function $G(z)$, satisfying 
\begin{equation}
\label{eqn:GzRelation}
B[G(z)]=z
\end{equation}

The Blue function was first introduced by Zee\cite{Zee1996} to model , among other things, spectral broadening
in quantum systems. Briefly, given a deterministic Hamiltonian matrix $\mathbf{H}_{0}$, with eigenvalues $\lambda^{0}_{i}$,
one can model the spectral broadening of $\lambda^{0}_{i}$ by adding a random matrix $\mathbf{H}_{1}$ to $\mathbf{H}_{0}$:
$\mathbf{H}=\mathbf{H}_{0}+\mathbf{H}_{1}$.  The resulting eigenvalues of $\mathbf{H}$ now contain some randomness $\sigma$,
i.e $\lambda=\lambda^{0}+\sigma$.  To  model the ESD of $\mathbf{H}$ , one  then
specifies the invididual \RTransforms for $\mathbf{H}_{0}$ and $\mathbf{H}_{1}$; the full ESD of $\mathbf{H}$
can then be reconstructed by adding the two \RTransforms together $R(z)=R_{0}(z)+R_{1}(z)$.
\footnote{Zee also notes that $R(z)$  is the same as the self-energy $\Sigma(z)$ from quantum many body theory\cite{Zee1996}}

By specifying the $R(z)$ transform, this specifies complete spectral density (ESD) $\rho(\lambda)$.
Here, we are actually only interested in the tail of $\rho(\lambda)$, and we can accept errors in $R(z)$
that describe the bulk region inaccurately or improperly. That is, we can given $R(z)=R(z)_{tail}+R(z)_{bulk}$,
we only need  $R(z)\approx R(z)_{tail}$.

\charles{Describe $R(z)$ is a power series, and how we take $\int dz R(z)$ and why we restirct the integral, contours, etc}


\subsubsection{Known \RTransforms and Analytic (Formal) Models}

There only a few known analytic results for the explicit \RTransform $R(z)$.
Below, we review some of them, explaining what ESD they correspond to,
and what the resulting \GEN~$G(\lambda)$ would be if applied
as a model $R(x)$ in the \SETOL approach.

\charles{TODO: These needs to be checked; \\
  \\
  also, in the appendix, I will add   the $G(z)$ for PL fits with $\alpha=2,3,4$
  and some plots for the $R(z)$
  }

\nred{Note: removed \emph{Multiplicative-Wishart} }
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
  \hline
  Model & \HTSR Universality class & \RTransform $R(z)$\\  \hline
  \hline
  Discrete & Bulk+Spikes & $\tfrac{1}{M}\sum_{i=1}^{M}\lambda_{i}$   \\ \hline
  \hline
  Wishart Models & &\\ \hline
%  Multiplicative-Wishart & HT/VHT& $\dfrac{\epsilon\phi z^2}{2 - \epsilon\phi^2 z^2}$ \\  \hline
  \InverseWishart & HT/VHT &  $\dfrac{\kappa-\sqrt{\kappa(\kappa-2z)}}{z}$  \\  \hline
   \hline
  Levy \Wigner &   & \\  \hline
  General  $(\alpha_{l}\ne 1)$ & VHT/HT  & $a+bz^{\alpha_{l}-1}$ \\  \hline
  \Cauchy $\alpha_{l}=2, \beta=0$ & $\alpha=2$ & $a - i\gamma$ \\  \hline
\end{tabular}
\caption{Known \RTransforms for different matrix models.
%  The \emph{Multiplicative-Wishart} model has two real, non-zero parameters, $\epsilon$ and $\phi$; for more details, see \cite{Pennington2017}.
  For the \emph{\InverseWishart}, as given by Bun\cite{BunThesis}, $\kappa=\frac{1}{2}(Q-1)$ where, $q=\frac{1}{Q}=\frac{M}{N}\le 1$.
  The \emph{\LevyWigner} model describes \Wigner-like Square Random Matrices
  (as opposed to Wishart-like or Correlation Matrices), where the elements are drawn from a Levy-Stable distribution.
  The Levy-Stable $R(z)$ is parameterized by a (real) shift parameter $a$,
  a complex phase factor $b$ (that depends on 3 real parameters  $\alpha_{l}, \beta$, and $\gamma$),
  and, importantly,  a PL-like tail exponent $\alpha_{l}\in (0,2)$;
  For more details, the text, see~\cite{BJNx01_TR,BJNx06_TR,BJ09_TR}.
  \red{For our modeling purposes here, we make the association $\alpha_{l}\sim\alpha/2-2$. maybe not?}
 (Also, for simplicity, we assume the variance $\sigma=1$ for all models above, where appropriate)
}  
\end{table}

\charles{Discuss the Table briefly, and then each model in its own subsubsection.
We can create plots to show how these models can treat heavy tails, and explain the parameter fitting
}
\subsubsection{Discrete Model of the Bulk+Spikes}
In the Bulk+Spikes case, the tail of the ESD is just composed of Spikes, i.e eigenvalues $\lambda_{spike}\ge \lambda^{+}_{bulk}$,
where $\lambda^{+}_{bulk}$ is the postive edge of the \emph{\MPBulk} distribution\cite{MM18_TR_JMLRversion}.
We do not expect the \TRACELOG condition to hold here, but we can still evaluate $\QT$.
Let's say the tail has $M^{tail}$ eigenvalues that define the effective correlation space.
Write
\begin{equation}
\label{eqn:spikes_model_0}
\rho_{tail}(\lambda)=\frac{1}{\MECS}\sum_{i=1}^{\MECS}\delta(\lambda-\lambda_{i})
\end{equation}
The \GreensFunction $G(z)$ is then
\nred{Check $\tfrac{1}{\MECS}$ term--recall we use the $1/\MECS$ normalizaiton here,
  but this may be the wrong place or way to introduce it;}
\begin{equation}
\label{eqn:spikes_model_1}
G(z) = \int dz \dfrac{ \rho_{tail}(\lambda) }{z - \lambda} =
\dfrac{1}{\MECS}\sum_{i} \int dz \dfrac{\delta(\lambda-\lambda_{i}) }{z - \lambda} =
\dfrac{1}{\MECS}\sum_{i} \dfrac{1}{z-\lambda_{i}}
\end{equation}
The Blue function for each invividual term $i$ is  $\frac{1}{z-\lambda_{i}}$
$B(z)=\lambda_{i}+\frac{1}{z}$. Now, using the additive property of the \RTransform,
we can express the total $R(z)$ as the sum of the \RTransforms for the individual terms $i$, giving
\begin{equation}
\label{eqn:spikes_model_2}
R(z) =\dfrac{1}{\MECS}\sum_{i}\left(\lambda_{i}+ \dfrac{1}{z}\right) - \dfrac{1}{z}=
\dfrac{1}{\MECS}\sum_{i}\lambda_{i}
\end{equation}
This gives the \GEN~$\GN$ as
\begin{align}
\label{eqn:spikes_model_3} 
\GN = 
\int_{\lambda_{min}}^{\lambda}
\dfrac{1}{\MECS}
\sum_{i}\lambda_{i} dz=
\GN = \dfrac{1}{\MECS}\sum_{i}\lambda_{i} \int_{0}^{\lambda} 1 dz \\ \nonumber
= \dfrac{1}{\MECS}\left(\sum_{i}\lambda_{i}\right)\left(\sum_{i}\lambda_{i}\right)
\end{align}
This gives the \QualitySquared as
\begin{equation}
\label{eqn:spikes_model_4} 
\QT =\left(\dfrac{1}{\MECS}\sum_{i}\lambda_{i}\right)^{2}
\end{equation}
We now see that $\Q=\sqrt{\QT}$ is what we call a Tail Norm.
\nred{CHECK FOR ERRORS PLEASE; Also ?  not really a Trace norm since this is defined over singlular values ?}

%\subsubsection{Multiplicative-Wishart Model}
%The Multiplicative-Wishart (MW) model  has two real, adjustable parameters $\epsilon,\phi$.
%It treates $\mathbf{X}$ as resulting from product of random matrices, and is good for modeling an ESD with a very heavy tail.
%It has been used previously to model the heavy tail of the Hessian matrix in NNs\cite{Pennington2017}
%This model would work better for fitting HT ESDs that decay slower than a TPL.  We will not consider
%this model further here and leave this for future work.

%We will work out  the full details, going from $\rho(\lambda)\rightarrow G(z)\rightarrow R(z)\rightarrow \QT$
\subsubsection{Inverse-Wishart Model of \IdealLearning}

The \InverseWishart (IW) model is an excellent model for the ESD when $\alpha=2.0$ (and $Q=2$);
using this model, we can derive an expression for the \HTSR \ALPHAHAT \LayerQuality metric,
$\ALPHAHATEQN :=\ALPHAHATLONG)$ as a leading order term in the final expression for $\log_{10}\QT$.

This model treats the ESD of $\mathbf{X}^{-1}$ when the ESD of $\mathbf{X}$ itself is MP.
As a parametiric model, it can be quite effective at treating VHT and HT (or \FatTailed) ESDs when the far tail decays very rapidly, like a TPL, and/or for $\alpha\le 4$.
To do this, one simply considers the parameters $\kappa$ as an adjustable paremeter.

\begin{figure}[h]
    \centering
    \subfigure[IW Distribution, $\kappa=0.25$]{ 
      \includegraphics[width=5cm]{./img/IWplotQ1.5.png}
      \label{fig:IWplotQ15}                                                                                                      
    }                               
      \subfigure[IW Distribution, $\kappa=1.25$]{ 
      \includegraphics[width=5cm]{./img/IWplotQ3.5.png}
      \label{fig:IWplotQ35}                                                                                                      
      }
      \caption{Example Inverse-Wishart (IW) distributions for $\kappa=0.25$ and $\kappa=1.25$,  along with Power Law (PL) fits of the generated distribution. Plots on Log-Log scale.}
  \label{fig:IWplots}                                                                                                      
\end{figure}   

In Figure~\ref{fig:IWplots}, we fit some \Typical layer ESDs to an IW distribution.
When $\kappa=0.25$, the fitted $\alpha=1.91$, and the fit is a reasonably accurate model of the underlying Power Law
distribution and the ~\WW PL fit.
For  $\kappa=1.25$, the fitted $\alpha=2.92$ is larger, but the fit is not as good as a
model. Generally speaking, $\alpha$ scales with $\kappa$, but the  free cumulants scale inversely with $\kappa$.
So smaller $\alpha$ will give larger free cumulants and therefore a larger $\QT$.
Importantly, as seen in Figure~\ref{fig:IWplotQ15}, for $\alpha=\simeq 2.0$, the IW model (with $\kappa=0.25$)
is an effective simple model to illustrate the \SETOL case of \IdealLearning.

\begin{figure}[t]
    \centering
    \subfigure[IW $R(z)$, real $z$.]{
        \includegraphics[width=4cm]{./img/R.png}
        \label{fig:IW_R}
    }
    \subfigure[Branch cut at $z=\kappa/2$]{
        \includegraphics[width=4cm]{./img/branch-cut.png}
        \label{fig:IW_branch_cut}
    }
    \caption{(a) The function $R(z)$ of the Inverse Wishart model, with a singularity at $z = \kappa/2$. (b) The branch cut in the empirical spectral density, corresponding to the tail for $\kappa = 0.5$.}
    \label{fig:R_branch_cut_combined}
\end{figure}

Lets consider the $R(z)$ for the \InverseWishart model, denoted $R(z)[IW]$.
To integrate this function, we require that it be analytic.
At first glance, it may seem that that $R(z)[IW]$ is not analytic because it
has a pole at $z=0$ and because the square-root term $\sqrt{\kappa(\kappa-2z)}$  creates branch
cut at and $z=\kappa/2$ (and $z=\infty$).
Figure~\ref{fig:R_branch_cut_combined} presents this in two ways:
Figure~\ref{fig:IW_R} shows the R-transform $R(z)[IZ]$ for real $z$, highlighting its singular
behavior and the location of the branch cut at $z = \kappa/2$.
Figure~\ref{fig:IW_branch_cut} shows the corresponding branch cut in the ESD of the Inverse Wishart model (for $\kappa = 0.5$).
We select the branch cut starting at $z=\kappa/2$ and ending at $z=\infty$,
which allows us to at least formally defined the integral along the physically meaningful part of the ESD.

\begin{equation}
\label{eqn:IW_model_1} 
\GN[IW] := \int_{\LambdaECSmin}^{\lambda} R(z)[IW] dz
\end{equation}
noting that we expect $\LambdaECSmin\ge\kappa/2$.

It turns out, however, that due to the branch cut in $R(z)[IW]$,
$\GN[IW]$ is not analytic in the domain we need.  To correct for this, we will instead
model the \LayerQualitySquared using the modulus of $\GN[IW]$,
\begin{equation}
\label{eqn:IW_model_1} 
|\GN[IW]| := \sqrt{\GN[IW]^{*}\GN[IW]}
\end{equation}
where $\GN[IW]^{*}$ is the complex conjugate of $\GN[IW]$.
This is somewhat involved, so we present the full calculation in the Appendix, Section~\ref{sxn:IW}

Figure~\ref{fig:InverseWishartGx} plots $|\GN[IW]|$  on Lin-lin and Log-log plots,
on the range $\lambda\in(0.25, 100)$.  
and fits the it to a Power Law.  The fit follows the general trend
of the function, but it is not terribly accurate.
Still, from this plot, we can see that the \LayerQualitySquared
has the same general trend as $\ALPHAHAT$ and/or a Shatten Norm.

\begin{figure}[t]
  \centering
  \subfigure[$|\GN\lbrack IW\rbrack|$ Lin-lin plot and PL fit]{
    \includegraphics[width=0.45\textwidth]{./img/Gx.png}
        \label{fig:GxPlot}
    }
    \subfigure[$|\GN\lbrack IW\rbrack|$ Log-log plot and PL fit]{
        \includegraphics[width=0.45\textwidth]{./img/logGx.png}
        \label{fig:LogLogGxPlot}
    }
    \caption{Behavior of $\GN[IW]$ for the Inverse Wishart (IW) model,
       with a Power Law (PL) fit (red), $|\GN[IW]|\approx 1.138 \lambda^{0.539}$.
      (a)  Lin-lin plot. (b) Log-log plot.
}
    \label{fig:InverseWishartGx}
\end{figure}

%
%\begin{figure}[t]
%    \centering
%    % Subfigure (a): G(x) Plot
%    \subfigure[Inverse Wishart $\GN$]{
%        \includegraphics[width=0.45\textwidth]{./img/Gx.png}
%        \label{fig:GxPlot}
%    }
%    % Subfigure (b): Log-log plot and power-law fit
%    \subfigure[Log-log plot and power-law fit]{
%        \includegraphics[width=0.45\textwidth]{./img/logGx.png}
%        \label{fig:LogLogGxPlot}
%    }
%    \caption{Behavior of $\GN$ for the Inverse Wishart (IW) model. (a) $\GN$ lin-lin plot. (b) Log-log plot of $\GN$ with a Power Law (PL) fit, $\GN\sim 2.8\lambda^{-3.0}$ }
%    \label{fig:InverseWishartGx}
%\end{figure}
%
%Given an analytic expression for $\GN$ (and $\kappa=0.25$), we can plot the $\GN$ as a function of $\lambda$,
%and then fit this to a Power Law; this is depicted in Figure~\ref{fig:InverseWishartGx}.
%The resulting fit gives $\GN\simeq 2.8\lambda^{3.0}$.
%We can now associate $\ALPHAHAT$ with $\log_{10}\QT$ by keep the leading order term
%$(\lambda_{max}=\LambdaECS_{max}$)
%
%\begin{align}
%  \label{eqn:IW_alphahat}
%  \ALPHAHATEQN:=\ALPHAHATLONG\approx\log_{10}\QT=\log_{10}\sum_{\LambdaECS}\GNI\approx\log_{10}\mathcal{G}(\lambda_{max}\sim (\alpha+1)\log_{10}\lambda_{max}
%\end{align}
%So for $\alpha=2.0$, $\log_{10}\QT\sim (\alpha+1)\log_{10}\lambda_{max}$, which is very close to $\alpha\log_{10}\lambda_{max}$. QED.

%
\subsubsection{Levy-Wigner Models and the \ALPHAHAT Metric}
In  this section, we show how to obtain the \WW~\ALPHAHAT metric by modeling the near VHT cases with an approximation to a Levy distribution at $\alpha\approx 2$.  
We do this because the~\ALPHAHAT metric has been developed to adjust for \SCALE anomalies that arise from issues like \CorrelationTraps,
making \ALPHA smaller than expected.
The  \LevyWigner (LW) model treats  $\mathbf{X}$ as if it were a \Wigner matrix (and not actually a correlation  matrix), and the $\alpha$ is different but related to 
$\alpha$ above in \EQN~\ref{eqn:rhoX};
the ESD follows a Levy-Stable distribution, where $a$ is a shift parameter, and $b$ is a complex phase factor depending on 2 real factors, $\beta$ and $\gamma$.
Strictly the ESD for an LW model, $\rho_{LW}(\lambda)$, is defined by its characteristic function (i.e. the Fourier Transform of $\rho_{LW}(\lambda)$), but
we can note that the ESD is VHT, $\rho_{LW}(\lambda)\sim\lambda^{-\alpha-1}$, and that when $\alpha_{l}\approx 1$, the ESD resembles a PL HT ESD with $\alpha\approx 2$.

For case of \IdealLearning,  we choose to \emph{model} the \RTransform of our Fat-Tailed HT ESDs as
\begin{equation}
\label{eqn:LW_model_0} 
R(z)[HT] = bz^{\alpha-1},\;\alpha\simeq 2
\end{equation}
where $b$ is an unspecified constant (possibly negative and/or complex).
Notice that when $\alpha\approx 2$, our model is close to the LW model, $R(z)[HT]\approx R(z)[LW]$
(and gives a \Cauchy distribution if we choose $b=a-i\gamma$).

Untegrating $R(z)[HT]$, we obtain (formally)
\begin{equation}
\label{eqn:LW_model_0} 
\GN[HT] = \tfrac{b}{\alpha} \lambda^{\alpha}
\end{equation}

If we now choose $b=\alpha=2$, then  $\QT$ takes the form of a Shatten Norm (squared)
\begin{equation}
  \label{eqn:LW_model_1}
  \QT = \tfrac{1}{\MECS}\sum_{i}\lambda^{\alpha}
\end{equation}

Taking the logarithm of $\GN[HT] $, we obtain
\begin{equation}
\label{eqn:LW_model_2} 
\log \GN[HT] =  \log\tfrac{b}{\alpha} +\tfrac{\alpha}\log\lambda
\end{equation}

As with the \InverseWishart (IW) model, we can derive a formal expression for $\ALPHAHAT$ using the LW model.
Let us approxmate $\QT$ by the largest term in the
sum over $\GN$, and let $\lambda=\lambda_{max}$, giving
\begin{equation} 
\label{eqn:LW_model_3} 
\ALPHAHATEQN = \log_{10} \QT \approx  \alpha\log\lambda_{max} 
\end{equation}
We present this as a formal example, noting that is slightly different from the
result for the IW model, Eqn.~\ref{eqn:IW_alphahat}. We do not claim this is a valid empirical model as we
have not attempted to fit a real-world ESD to Levy-stable distribtion.  We leave this to
a future study, noting, however, there has been some work doing such fits.~\cite{li2024exploring}

Ideally, we would like to have an rigorous expression for $R(z)$ not just
in the case of \IdealLearning but also for the entire \FatTailed Universality class.
This is non-trivial to obtain and we will attempt this in a future work.
Fow now, we will take a different approach, and evaluate $R(z)$ explicitly using numerical techniques.

