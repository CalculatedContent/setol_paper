
\subsection{Outline}

Here is an outline of the sections.

Section~\ref{sxn:htsr}: \HeavyTailedSelfRegularization Theory

Section~\ref{sxn:setol}: Overview of \SETOL %Spuriously Heavy Tailed ESDs

Section~\ref{sxn:SMOG_main}: \StatisticalMechanics of Generalization

%Section~\ref{sxn:SETOL}: SETOL: A \SemiEmpirical Theory of (Deep) Learning

Section~\ref{sxn:matgen}: Matrix Generalization of the \StudentTeacher Model

Section~\ref{sxn:empirical}: Empirical Studies

Section~\ref{sxn:discussion}: Discussion and Conclusion

Appendix~\ref{sxn:appendix}: Appendix


\charles{references to add:
\cite{Bouchaud1998}
\cite{Bouchaud2022}
\cite{Yang2021}
\cite{Staats2022}
\cite{Roberts2021}
\cite{Tanaka2008}
\cite{Chialvo2010}
\cite{Liam_21_JSM_JRNL}
}

\michael{MM TO DO: 
Cite Hannon review and differentiate from that; 
cite Ganguli review and differentiate from that.
}

\michael{MM TO DO: where to put this par.:
In comparison, \SLT fares even worse than \STATMECH.
This has led to suggestions that the field must rethink generalization~\cite{Understanding16_TR}, or at least the field must revisit old ideas from \STATMECH~\cite{MM17_TR} to develop a new, modern theoretical approach to NNs.}

\michael{MM TO DO: Be sure to cite \cite{MM20a_trends_NatComm,YTHx23_KDD} in the intro.}

\michael{MM TO DO: Incorporate:
``These kind of early \STATMECH analyses provided some of the first insights into many ``modern phenomena observed in NNs, 
like Double descent \cite{SST92,Opper01} and \PowerLaw scaling with the data \cite{SST92}.
}

