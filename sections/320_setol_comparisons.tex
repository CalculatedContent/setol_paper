\subsection{Comparing SETOL with HTSR: Conditions for Ideal Learning}
\label{sxn:ideal_learning}

%Although \SETOL is a \SemiEmpirical theory based on \STATMECH applied in a novel way to the ST model, 

The \SETOL approach establishes a starting point for developing a first-principles theory for modern NNs. %understanding of generalization, 
Among other things, by connecting with the \HTSR \Phenomenology, it lets us identify conditions for an \Ideal state of learning for an individual NN layer, under the Single Layer Approximation.
By \Ideal, we mean that the layer is being used most effectively i.e., in some sense it is at its optimal data load, and thus it is conjectured to result in the best model quality.

\textbf{The \Ideal State of Learning} is conjectured to be characterized by the following three conditions:
\begin{enumerate} 
\item \label{itm:ideal_1}
  the tail of ESD, $\rho^{emp}_{tail}(\lambda)$, can be well fit to a PL of $\alpha\approx 2$: $\rho^{emp}_{tail}(\lambda)\sim\lambda^{-2}$;
\item \label{itm:ideal_2}
  the eigenvalues in the tail, $\lambda_{i}$, satisfy the \TRACELOG (i.e., Trace-Log) condition: $\sum_{i}\ln\lambda_{i}=0$; and
\item \label{itm:ideal_3}
  the generalizing components of the layer concentrate in the singular vectors associated with the tail of the ESD, (whose span we call the \EffectiveCorrelationSpace).
  \michaeladdressed{@charles: do we have a way to test this third one?}
  \charles{Seriously ?!}
\end{enumerate}
In Section~\ref{sxn:empirical}, we will test and justify this conjecture.


%Wee offer the following conjecture:
%\begin{quote}
%The HT Universality classes used in the \HTSR \Phenomenology can be used by the \SETOL approach to characterize the state of convergence of a layer in a trained NN.
%\end{quote}

%The \TRACELOG states that eigenvalues in the tail of the ESD, call them $\rho^{emp}_{tail}(\lambda)$, satisfy the following condition: 
%$\log\prod\lambda_{i}=\sum\ln\lambda_{i}=0$ for $\lambda_{i}\ge\lambda_{max}$.
%This condition is completely independent of \HTSR Theory, and it simply requires that the basic setup and premises of our \SemiEmpirical approach holds.
%When combined with empirical results below (see \michael{XXX-XXX}), we can now state the following.

These claims are fundamentally about NN learning itself. 
They are motivated by our formulation of the \SETOL approach in our search for a practical predictive theory behind the HTSR \Phenomenology.
When $(\ref{itm:ideal_1})$ and $(\ref{itm:ideal_2})$ conditions hold for any layer, we conjecture that $(3)$ holds as well.
Moreover, when $(\ref{itm:ideal_1}-\ref{itm:ideal_3})$ hold for all layers, we conjecture the
NN has the lowest \GeneralizationError (and highest \ModelQuality) possible for given model architecture and dataset.
\michaeladdressed{@charles: Does \GeneralizationError mean ``modely quality'' or ``generalization error'' there?}

\michael{This par should maybe be above.}
Previous results have shown that the \HTSR quality metrics (\ALPHA,  \ALPHAHAT, etc.)  correlate very well with reported test accuracies,
as well as model quality on an epoch-by-epoch basis~\cite{MM18_TR_JMLRversion,KFWB13}.
\michael{Refs. And clarify.}
These results hold because, as indicated by the \HTSR theory, the PL exponent $\alpha$ characterizes both the quality of the layer and provides an after-the-fact measure of the amount of regularization.%
\footnote{By ``after-the-fact'', we mean that it provides a measure of the regularization in a layer, along the lines of the self-regularization interpretation of \HTSR Theory~\cite{MM18_TR_JMLRversion}. However, we do \emph{not} recommend that it be used as an explicit regularization parameter. Informally, this is since the ``easiest way to obtain HT ESDs is to make weight matrices HT element-wise; but this is \emph{not} what is observed in practice, and thus this is precisely \emph{not} what \HTSR Theory and our new \SETOL approach are designed to model.}
However, the \HTSR approach says nothing about the \SETOL \TRACELOG condition; and neither does the \SETOL approach require a minimum of $\alpha=2$ to obtain the best model quality, as observed by the \HTSR \Phenomenology.
Remarkably, we can show that $(1)$ and $(2)$ do hold \emph{simultaneously}, both in carefully designed experiments on a small model,
as well as for many pre-trained, high quality open-source models (such as VGG, ResNet, Llama, Falcon, etc). 

%And generally speaking, smaller $\ALPHA$ corresponds to better model quality, and with the layer averaged $\langle\alpha\rangle\in[2,4]$ for the better performing models. 
%\michael{Redundant.}

%MIKE \michael{Can or should we connect our quality metric to a capacity metric from SLT?
%MIKE This \ProductNorm resembles a Capacity measure for NNs from traditional ML theory, where
%MIKE $\Vert\mathbf{W}\Vert$ may be, say, Frobenius Norm, the Spectral Norm, or even their ratio, the Stable Rank.
%MIKE Traditional theory, however, usually considers metrics that only measure the \Scale of the layers.%}

\HTSR theory, however, has been developed as a \Phenomenology describing the best-trained, most accurate open-source models available.
As such, it may be biased towards such models, and it may not describe less optimal learning scenarios.
\michael{Weave that in in some way; that is probably better in the intro.}
The keys goal of this work is to derive independent conditions, both theoretical and experimental, that can
identify the conditions for \IdealLearning, and to stress-test these conditions in carefully designed,
reproducible experiments.
\michaeladdressed{How does that sentence relate to \SETOL, since one could have a \SETOL for lower-quality models.}
\charles{it is doubtful that the \TRACELOG condition would hold for a low-quality model. }

