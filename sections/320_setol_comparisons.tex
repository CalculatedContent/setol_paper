\subsection{Comparing SETOL with HTSR: Conditions for Ideal Learning}
\label{sxn:ideal_learning}

%Although \SETOL is a \SemiEmpirical theory based on \STATMECH applied in a novel way to the ST model, 

The \SETOL approach establishes a starting point for developing a first-principles theory for modern NNs. %understanding of generalization, 
Among other things, by connecting with the \HTSR \Phenomenology, it lets us identify conditions for an \Ideal state of learning for an individual NN layer, under the Single Layer Approximation.
By \Ideal, we mean that the layer is being used most effectively i.e., in some sense it is at its optimal data load, and thus it is conjectured to result in the best model quality.

\textbf{The \Ideal State of Learning} is conjectured to be characterized by the following three conditions:
\begin{enumerate} 
\item \label{itm:ideal_1}
  the tail of ESD, $\rho^{emp}_{tail}(\lambda)$, can be well fit to a PL of $\alpha\approx 2$: $\rho^{emp}_{tail}(\lambda)\sim\lambda^{-2}$;
\item \label{itm:ideal_2}
  the eigenvalues in the tail, $\lambda_{i}$, satisfy the \TRACELOG (i.e., Trace-Log) condition: $\sum_{i}\ln\lambda_{i}=0$; and
\item \label{itm:ideal_3}
  the generalizing components of the layer concentrate in the singular vectors associated with the tail of the ESD, (whose span we call the \EffectiveCorrelationSpace).
\end{enumerate}
In Section~\ref{sxn:empirical}, we will test and justify this conjecture \red{by showing that the induced measures of each condition converge to one another at the same rate, and reach convergence at the exact point beyond which accuracy begins to degrade}.

These claims are fundamentally about NN learning itself. 
They are motivated by our formulation of the \SETOL approach in our search for a practical predictive theory behind the HTSR \Phenomenology.
When $(\ref{itm:ideal_1})$ and $(\ref{itm:ideal_2})$ conditions hold for any layer, we conjecture that $(3)$ holds as well.
Moreover, when $(\ref{itm:ideal_1}-\ref{itm:ideal_3})$ hold for all layers, we conjecture the
NN has the lowest \GeneralizationError (and highest \ModelQuality) possible for given model architecture and dataset.

Previous results have shown that the \HTSR quality metrics (\ALPHA,  \ALPHAHAT, etc.)  correlate very well with reported test accuracies,
as well as model quality on an epoch-by-epoch basis~\cite{MM18_TR_JMLRversion,KFWB13}.
These results hold because, as indicated by the \HTSR theory, the PL exponent $\alpha$ characterizes both the quality of the layer and provides an after-the-fact measure of the amount of regularization.%
\footnote{By ``after-the-fact'', we mean that it provides a measure of the regularization in a layer, along the lines of the self-regularization interpretation of \HTSR Theory~\cite{MM18_TR_JMLRversion}. However, we do \emph{not} recommend that it be used as an explicit regularization parameter. Informally, this is since the ``easiest way to obtain HT ESDs is to make weight matrices HT element-wise; but this is \emph{not} what is observed in practice, and thus this is precisely \emph{not} what \HTSR Theory and our new \SETOL approach are designed to model.}
However, the \HTSR approach says nothing about the \SETOL \TRACELOG condition; and neither does the \SETOL approach require a minimum of $\alpha=2$ to obtain the best model quality, as observed by the \HTSR \Phenomenology.
Remarkably, we can show that $(1)$ and $(2)$ do hold \emph{simultaneously}, both in carefully designed experiments on a small model,
as well as for many pre-trained, high quality open-source models (such as VGG, ResNet, Llama, Falcon, etc). 

\HTSR, however, has been developed as a \Phenomenology describing the best-trained, most accurate open-source models available.
As such, it may be biased towards such models, and it may not describe less optimal learning scenarios.
The key goal of this work is to derive independent conditions, both theoretical and experimental, that can
identify the conditions for \IdealLearning, and to stress-test these conditions in carefully designed,
reproducible experiments.

