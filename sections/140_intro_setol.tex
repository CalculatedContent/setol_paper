\subsection{A Semi-Empirical Theory of Learning (\SETOL)}

We propose \SETOL, a \SemiEmpirical Theory for Deep Learning Neural Networks (NNs),
as both a theoretical foundation for \HTSR \Phenomenology
and a novel framework for predicting the properties of complex NN models.
This unified framework offers a deeper understanding of DNN generalization
through a \SemiEmpirical approach inspired by many-body physics,
combined with a classic \STATMECH model for NN generalization.
Specifically, \SETOL combines theoretical and empirical insights to evaluate \ModelQuality,
showing that the weightwatcher layer \HTSR PL metrics (\ALPHA and \ALPHAHAT)
can be derived using a phenomenological \EffectiveHamiltonian approach.
This approach expresses the \HTSR \LayerQuality in terms of the RMT matrix cumulants
of the layer weight matrix $\WMAT$,
and is governed by a \ScaleInvariant transformation equivalent
to a single step of an Exact \RenormalizationGroup (\ERG) transformation.
Here, we derive this from first principles, requiring no previous knowledge of statistical physics.

The \SETOL approach unifies the \HTSR principles with
a broader theoretical framework for layer analysis.
The \HTSR theory identifies \Universality (e.g., $\alpha=2$) as a hallmark of the best-trained DNN layers,
and, here, our \SETOL introduces the closely related \emph{Exact Renormalization Group Condition}, a \ScaleInvariant or
\VolumePreserving transformation that reflects an underlying \emph{Conservation Principle}.
Together, these principles form the theoretical foundation for deriving \HTSR \LayerQuality metrics from first principles.
By leveraging techniques from \STATMECH and modern \RMT, \SETOL offers a rigorous framework
to connect empirical observations with theoretical predictions, advancing our understanding of generalization
in neural networks.

\begin{itemize}
\item
  \textbf{Derivation of the HTSR Layer Quality metrics $\ALPHA$ and  $\ALPHAHAT$}.
  The \SETOL approach takes as input the
  \EmpiricalSpectralDensity (ESD) of the layers
  of trained NN, and  derives an expression for the approximate \emph{\AverageGeneralizationAccuracy}
  of a multi-layer NN. We call this approximation the \emph{\ModelQuality}, denoted $\Q^{NN}$.
  This \ModelQuality is expressed as a product of individual \LayerQuality terms, $\Q^{NN}_{L}$,
  which themselves can then 
  be directly related to the \HTSR Power Law (PL) empirical $\ALPHA$ ($\alpha$)
  and $\ALPHAHAT$  ($\ALPHAHATEQN=\ALPHAHATLONG$) metrics.

  In particular, the \LayerQualitySquared, $\QT\approx[\Q^{NN}_{L}]^{2}$, is
  expressed as the logarithm of an \HCIZtext integral, which is the \ThermalAverage of an \EffectivePotential
  for a matrix-generalized form of the Linear Student-Teacher model of classical \STATMECH. This \HCIZtext
  integral evaluates into the sum of integrated \RTransforms
  from \RMT, or, equivalently, as a sum of integrated matrix cumulants.
  From this, the \HTSR $\ALPHAHAT$ metric can be derived in the special case of \IdealLearning.
  \footnote{The \SETOL approach to the \HTSR theory resembles
  in spirit the derivation of the \SemiEmpirical PPP models using
  the \EffectiveHamiltonian theory, where each phenomenological parameter is associated with a renormalized
  effective interaction, expressed as a sum of linked diagrams or clusters.\cite{Martin1996, Martin1998}}

   \item 
     \textbf{Discovery of a Mathematical Condition for Ideal Learning.}
     By \IdealLearning, we mean that the specific NN layer has optimally converged, capturing as
     much of the information as possible in the training data without overfitting to any part of it.
     In defining this and deriving our results, we have discovered (and are proposing) a new condition
     for \IdealLearning, which is associated with the \Universality of the \HTSR theory:
   \begin{itemize}
      \item 
        \textbf{\HTSR Condition for Ideal Learning.}
        This \HTSR theory states that a NN layer is \Ideal  when the ESD can be well fit to a
        Power Law (PL) distribution, with PL exponent $\alpha = 2$. Importantly, 
        this appears to be a Universal property of all well-trained NNs, independent of the training data,
        model architecture, and training procedure.
      \item 
        \textbf{\SETOL \TRACELOG Condition for Ideal Learning.}
        The \SETOL condition for \IdealLearning states that the 
        dominant eigencomponents associated with the ESD 
        of layer form a reduced-rank \emph{Effective Correlation Space} (\ECS) that satisfies
        a new kind of Conservation Principle,  \emph{\ScaleInvariant} \emph{\VolumePreservingTransformation} where the largest eigenvalues $\LambdaECS_{i}$ of the~\ECS satisfy
        the condition  $\ln \prod \LambdaECS_{i} = \sum \ln \LambdaECS_{i} = 0$.  
        This is called the \emph{\TRACELOG Condition}.  The \TRACELOG Condition is equivalent to the taking a single step of the Wilson Exact Renormalization (ERG).
   \end{itemize}

   The \HTSR Condition has been proposed and analyzed previously~\cite{MM18_TR_JMLRversion,MM20a_trends_NatComm,YTHx23_KDD}; but
   the \TRACELOG Condition is new, based on our \SETOL theory.
   When these two conditions align, we propose the NN layer is in the \Ideal state.

   \item 
   \textbf{Experimental Validation.} 
   We present detailed experimental results on a simple model, along with observations on large-scale pretrained NNs, to demonstrate that the \HTSR conditions for ideal learning $(\alpha = 2)$ are experimentally aligned with the independent \SETOL condition for ideal learning
   $(\Det{\XECS}=1)$. 
   See Section.~\ref{sxn:empirical-trace_log}.
   Our primary objective here is not to demonstrate performance improvements on SOTA NNs---this has been previously established \cite{NEURIPS2023_CHM}. 
   Instead, our aim is to \textbf{validate the theoretical assumptions} of \SETOL, test the \textbf{predictions of the \SETOL framework}, and examine the \textbf{new, independent learning conditions} we discovered---on a model that is sufficiently simple that we can evaluate and stress test the theory.

   \item 
   \textbf{Observations on Overfit Layers ($\alpha < 2$).} 
   Being a \SemiEmpirical theory, \SETOL can also identify violations of it's assumptions.
   For example, when empirical results show $\alpha < 2$ for a single layer, the layer's ESD falls into the \HTSR \VeryHeavyTailed (VHT) Universality class.
   (See Section~\ref{sxn:hysteresis_effect}.)
   When this happens, the layer may be slightly overfit to the training data, resulting in \textbf{suboptimal performance} and potentially even exhibiting \textbf{hysteresis-like effects} (memory effects)---that we observe empirically.
   These effects indicate that overfit layers may retain memory-like behavior, affecting learning dynamics and generalization.
\end{itemize}


