\subsection{Statistical Mechanics (\STATMECH) vs. Statistical Learning Theory (\SLT)}

Historically, there have been two competing theoretical frameworks for understanding NNs:
\emph{\StatisticalMechanics} (\STATMECH)~\cite{Eng01, EB01_BOOK, Gardner_1988,SST90, SST92, LTS90, Solla2023}; and 
\emph{\StatisticalLearningTheory} (\SLT)~\cite{Vapnik98}. 
\begin{itemize}
\item
\textbf{\StatisticalMechanics (\STATMECH).}
This framework has been foundational to the early development of NN models, such as the Hopfield Associative Memory (HAM)~\cite{Hop82}, Boltzmann Machines~\cite{AHS85},~\cite{HinSej86_relearn}, etc.
\STATMECH~has also been used to build early theories of learning, such as the \StudentTeacher model for the \Perceptron \GeneralizationError~\cite{Eng01,EB01_BOOK}, the Gardner model~\cite{Gardner_1988}, and many others.
Notably, the HAM was based on an idea by Little, who observed that, in a simple model, long-term memories are stored in the eigenvectors of transfer matrix~\cite{Lit74}.
(This general idea, but in a broader sense, is central to our approach below.)
Moreover, \STATMECH~predicts that NNs exhibit phase behavior.
This has recently been rediscovered as the Double Descent phenomenon~\cite{BHMM19,loog2020}, but it was known in \STATMECH long before it's recent rediscovery~\cite{Opper01}.
However, unlike other applied physics theories (e.g., \SemiEmpirical methods in quantum chemistry), \STATMECH only offers qualitative analogies, failing to provide testable quantitative predictions
about large, modern NN models.\cite{roberts2022principles}
\item
\textbf{\StatisticalLearningTheory (\SLT).}
\SLT and related approaches (VC theory, PAC bounds theory, etc.) have been developed within the context of traditional computational learning problems~\cite{Vapnik98}, and 
they are based on analyzing the convergence of frequencies to probabilities (over problem classes, etc.).
It was recognized early on, however, that they could not be directly applied to NNs~\cite{VLL94}.
Moreover, \SLT cannot even reproduce quantitative properties of learning curves~\cite{WRB93,DKST96} (whereas \STATMECH~is very successful at this~\cite{SST92}).
\SLT also failed to predict the ``Double Descent'' phenomenon~\cite{BHMM19}.
More recently, it has been shown that in practical settings
%, when trying to predict test accuracies,
\SLT can give vacuous~\cite{DR17_nonvacuous_TR} or even opposite results
to those actually observed~\cite{MM21a_simpsons_TR}.
\end{itemize}

\noindent
Technically, \SLT focuses on obtaining bounds on a modelâ€™s worst-case behavior, while \STATMECH seeks a probabilistic understanding of typical behaviors across different states or configurations.
Unfortunately, neither of these general theoretical frameworks has proven particularly useful to NN practitioners.
\SETOL combines insights from both.
Rather than being purely phenomenological like the \HTSR~approach, \SETOL is derived from first-principles, and in form of a \SemiEmpirical~theory.
As such, \SETOL offers a practical, \SemiEmpirical framework that bridges rigorous theoretical modeling and empirical observations for modern NNs.



