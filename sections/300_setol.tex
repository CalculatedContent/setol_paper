\section{A \SemiEmpirical Theory of (Deep) Learning (\SETOL)}
\label{sxn:setol}
Based on prior empirical results, and the success of the \ALPHA and \ALPHAHAT metrics that are based on the \HTSR \Phenomenology, this leads to the deeper question: 
%
\begin{quote}
\emph{Why do the \ALPHA and \ALPHAHAT metrics work so well as NN model quality metrics for SOTA NN~models?}
\end{quote}
That is, why do NN models with heavier-tailed layer ESDs tend to generalize better when compared to related models?
Relatedly, can we derive these metrics from first principles?
(If so, then under what conditions do they hold, and under what conditions do they fail?)


\noindent
To answer these questions, we will derive a general expression for the \LayerQuality, $\Q$, of an NN.
Although many modern NNs have many layers, we adopt a single-layer viewpoint (like a matrix-generalized Studentâ€“Teacher) because in \StatisticalMechanicsOfGeneralization (\SMOG) theory~\cite{SST92,STS90} the multi-layer generalization can be factorized or approximated.
For this, we will obtain by simple averaging our model quality metrics, under effectively a single layer approximation, that correspond to \ALPHA and \ALPHAHAT.


In deriving these quantities, we will introduce to NN theory a new \SemiEmpirical approach that combines techniques from \STATMECH and \RMT in a novel way.
The \LayerQuality $\Q$ will estimate the contribution that an individual NN layer makes to the overall quality of a trained NN model.
In deriving $\Q$, we have discovered a new \LayerQuality metric, called the \TRACELOG condition,
which indicates the generalizing components of the layer concentrate into a low-rank subspace
(the \emph{\EffectiveCorrelationSpace}, or~\ECS).
Importantly, we have conducted detailed experiments to show 
that the empirical estimates of the \SETOL \TRACELOG condition align remarkably well with predictions from the \HTSR
theory under \Ideal conditions (see Sections~\ref{sxn:empirical-test_acc})
and, then, 
that the key assumptions of our \SETOL theory are valid
(see Sections~\ref{sxn:empirical-effective_corr_space} and \ref{sxn:empirical-trace_log}).
In Section~\ref{sxn:empirical_comp_r_transforms}, we demonstrate how to apply theory directly using explicit calculations of the \RMT layer cumulants.
We next examine how the \HTSR predictions (i.e., the HT PL exponent $\alpha$) behave under non-\Ideal conditions (see Sections~\ref{sxn:empirical-correlation_trap} and \ref{sxn:hysteresis_effect}).
\michael{Does that outline belong here.}
%
In the following, we will outline key conceptual aspects of \SETOL.
In Section~\ref{sxn:setol_overview}, we give an overview of \SETOL;
In Section~\ref{sxn:ideal_learning}, we describe the conditions of \Ideal learning under \SETOL and how they differ from those of \HTSR; and
In Section~\ref{sxn:HT_ESDs} we describe conditions that deviate from this.


\input{sections/310_setol_overview}
\input{sections/320_setol_comparisons}
\input{sections/330_setol_detecting}
