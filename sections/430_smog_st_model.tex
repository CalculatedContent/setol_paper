\subsection{Student-Teacher Perceptron}
\label{sxn:SMOG_main-student_teacher}
%\label{sxn:ST-unified-intro}

In this subsection, we present a unified view of the \emph{\StudentTeacher} (ST) \Perceptron 
model from both a practical and a theoretical perspective.  
From the practical (\emph{operational}) side, imagine a real-world practitioner training a large-scale neural network (NN); \cyan{call this NN the \Teacher.
We wish to assess the Teacher’s \emph{true} accuracy on ground-truth labels in consistently reproducing its own (possibly imperfect) outputs. One can then train one or more similar networks -- called \Student(s) -- to mimic the Teacher’s predictions.}
By comparing Student and Teacher outputs, one can approximate the \red{Teacher’s} generalization performance even without an explicit hold-out set.  
Notably, if the Teacher perfectly interpolates its training data, the Student’s error directly estimates the Teacher’s \emph{true} \GeneralizationAccuracy.
Otherwise, it captures the Teacher’s \emph{Precision} in reproducing its own noisy or biased labels.

From the theoretical side, we seek a succinct, analytic formulation of the ST \AverageGeneralizationError, denoted $\AVGSTGE$.  
We work in the \AnnealedApproximation (AA) -- a simplification often valid when networks are sufficiently large and can nearly interpolate their training data. 
Under this approximation, one obtains closed-form expressions for the \StudentTeacher \emph{Overlap} $(R)$
and thus for the \Teacher’s overall error or accuracy.  
These results lay the foundation for our \emph{\SemiEmpirical} approach, in which we supplement this theoretical form with empirical measurements (e.g., from the trained weight matrices) to account for real-world correlations in the data and the model’s internal structure.  

\cyan{I\textbf{Take note that we deviate from the standard \SMOG ST model in an important way} -- we make no direct assumption that the labels are realizable by a \Teacher NN. \SETOL makes its realizability assumption in a novel way: rather than assume that there exists an unknown \Teacher that \emph{generates} the labels $\Yt$ implicitly
This point is both subtle -- since the derivations will proceed almost identically to those found in~\cite{engel2001statistical,EngelAndVanDenBroeck,SST90,SST92}
-- and also radical, because our derived  quality metrics $\Q$ now
depend directly on empirical measurables of the \Teacher.
\textbf{As far as we know, this is completely new.}
}

\begin{enumerate}[label=4.3.\arabic*]
\item
  \textbf{Operational Setup.}
  In subsection~\ref{sxn:ST_OP_setup} we explain how to set up the \cyan{our formulation} \StudentTeacher
  model in an \emph{operational} manner. 
  In particular, we emphasize the difference between \emph{true accuracy} (vs.\ ground-truth labels)
  and \Precision (vs. the Teacher’s own labels). We also the discuss the difference between
  our \Quality metrics and the \emph{Generalization Gap}.

  \item
    \textbf{Theoretical Student-Teacher Average Generalization Error $(\AVGSTGE)$.}
    In subsection~\ref{sxn:SMOG_main-st_av}  we outline how to derive $\AVGSTGE$ using the AA.  
    We introduce the key expressions that will serve as the starting point for our extended \SemiEmpirical theory in Section \ref{sxn:matgen}.
\end{enumerate}

\input{sections/431_smog_st_model}
\input{sections/432_smog_theory_st_avg}
\clearpage
