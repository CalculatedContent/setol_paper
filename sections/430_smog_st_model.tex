\subsection{Student-Teacher Perceptron}
\label{sxn:SMOG_main-student_teacher}
%\label{sxn:ST-unified-intro}

In this subsection, we present a unified view of the \emph{\StudentTeacher} (ST) \Perceptron 
model from both a practical and a theoretical perspective.  
The common sense modern understanding of Student-Teacher learning typically involves knowledge distillation: a larger, pretrained neural network (the \emph{Teacher}) is used to train a smaller network (the \emph{Student}), which learns to imitate the Teacher’s outputs $\Yt$.
In practical terms, this often helps the smaller Student achieve comparable performance more efficiently~\cite{hu2023TS}.
And it remains an open question as to why this works so well.
In contrast, classical theoretical models from the Statistical Mechanics of Generalization (\SMOG) interpret the Student-Teacher relationship differently. Here, the Teacher is an idealized theoretical model $T$, a Perceptron (vector) $\TVEC$, that explicitly generates (binary) labels $\Yt$ from some idealized data $\xi$, and the Students
$S$ are Perceptrons (vectors) of the same size as the Teacher, $\Vert\SVEC\Vert=\Vert\TVEC\Vert$, selected from a Boltzmann distribution.
The ST theory seeks simple analytic derivations of the generalization errors $\AVGSTGE$ under different simplifying assumptions (AA, high-T, linear activations, etc.),
in order to understand phenomena such as what is now called Double Descent\cite{Vallet1989}, NN storage capacity, learning with noisy labels,  memorization vs. generalization, and other 
fundamental learning phenomena\cite{Opper01,OK96_CHAPT,Eng01,EngelAndVanDenBroeck,SST90,SST92}.
A key result is that $\AVGSTGE$ is a simple analytic function of the ST \emph{Overlap}, $R=\SVEC^{\top}\TVEC$.
Our \SETOL approach is motivated by both these perspectives, and introduces a novel, \SemiEmpirical twist. 
We seek a matrix-generalization of the classical vector ST model.
And while inspired by the ST Perceptron, 
rather than an idealized theoretical Teacher $T$, we take the Teacher as an empirical input—a layer weight matrix, $\TMAT=\WMAT$,
taken from a real model--a model you might train yourself, download, or acquire elsewhere.
We aim to estimate the layer accuracy (or precision),
which we call \LayerQuality $\Q$, using an ensembles of similar Students, $S\sim T$, i.e. $N \times M$ matrices, $\SMAT,\TMAT$.
$\Q$ is also a function of the ST Overlap, now a matrix,
$\OVERLAP=\tfrac{1}{N}\SMAT^{\top}\TMAT$,
but now also critically depends on the ESD of $\TMAT$.
And while our Students are the same size as the Teacher, 
we will find later that the optimal Teacher is actually much smaller than the one we started out with. 

\paragraph{Practical framing.}
Imagine being a real-world practitioner who has trained a large-scale neural network (NN), or downloaded it from the internet, or received it by other means --- an increasingly common scenario.
 We call this NN the \Teacher, because it is the primary empirical input to the theory. The term ``\Teacher'' means that we consider this object to be the original source of supervision -- this trained model replicates the training labels $\Yt$, often to perfection (interpolation) or nearly so.\footnote{If nearly so, then we consider this label noise to be the contribution of the \Teacher, which \SETOL can characterize.}
You also hold a dataset $\ADD$ that approximates the distribution on which $T$ was trained, yet you lack a reliable yardstick for evaluating $T$’s outputs: perhaps $\ADD$ is contaminated with test data, or $T$ is an LLM with highly arbitrary outputs, or you just
don't have access to test data.  In any case, you can escape this bind by applying a \emph{Student–Teacher} protocol.  
You retrain one or more \Students\ $S_{1},S_{2},\dots$ on $D$ using ordinary tools (SGD), that resemble the Teacher $T$
(same size, amount of regularization, etc),
but now slightly varying the training procedures (i.e  initial conditions, optimization hyperparameters, etc.)--
tasking them solely with \emph{imitating} $T$’s predictions. 
Averaging over many such Students, you can obtain an \emph{indirect but quantitative} estimate of $T$’s performance.

\paragraph{Why this works.}  
We view $T$ as an \textit{inevitable} outcome of the training pipeline: if you reran the same recipe many times, you would likely converge on a narrow family of similar models.  
The tighter that distribution, the more \emph{Precise} it is, and—assuming the ground-truth concepts $\Yt$
are at least \emph{nearly} realizable— the closer any single draw (your Teacher) must sit to the truth.  
By comparing Student and Teacher outputs, you can approximate the Teacher’s generalization performance even without an explicit hold-out set.  
Notably, if the Teacher perfectly interpolates its training data, the Student’s error directly estimates the Teacher’s \emph{true} \GeneralizationAccuracy.
Otherwise, it captures the Teacher’s \emph{Precision} in reproducing its own noisy or biased labels.

\paragraph{An extended theory.} We seek a succinct, analytic formulation of the ST \AverageGeneralizationError, denoted $\AVGSTGE$.  
We work in the \AnnealedApproximation (AA) -- a simplification often valid when networks are sufficiently large and can nearly interpolate their training data. 
Under this approximation, one obtains closed-form expressions for the \StudentTeacher \emph{Overlap} $(R)$
and thus for the \Teacher overall error or accuracy.  
Take note also that we do not assume that the labels $\Yt$ are realizable by the \Teacher, but, rather,
take the Teacher as empirical input to the theory.  
This point is both subtle -- since the derivations  proceed closely at times to those found in~\cite{engel2001statistical,EngelAndVanDenBroeck,SST90,SST92}
-- and also radical, because our derived quality metrics $\Q$ now
depend directly on empirical properties of the \Teacher.
These results lay the foundation for our \emph{\SemiEmpirical} approach, in which we supplement this theoretical form with empirical measurements (e.g., from the trained weight matrices) to account for real-world correlations in the data and the model’s internal structure.  
\textbf{As far as we know, this is completely new \SemiEmpirical approach for modeling NNs.}


\begin{enumerate}[label=4.3.\arabic*]
\item
  \textbf{Operational Setup.}
  In subsection~\ref{sxn:ST_OP_setup} we explain how to set up our formulation\StudentTeacher
  model in an \emph{operational} manner. 
  In particular, we emphasize the difference between \emph{true accuracy} (vs.\ ground-truth labels)
  and \Precision (vs. the Teacher’s own labels). We also the discuss the difference between
  our \Quality metrics and the \emph{Generalization Gap}.

  \item
    \textbf{Theoretical Student-Teacher Average Generalization Error $(\AVGSTGE)$.}
    In subsection~\ref{sxn:SMOG_main-st_av}  we outline how to derive $\AVGSTGE$ using the AA.  
    We introduce the key expressions that will serve as the starting point for our extended \SemiEmpirical theory in subsequent subsubsection.
\end{enumerate}

\input{sections/431_smog_st_model}
\input{sections/432_smog_theory_st_avg}
\clearpage
