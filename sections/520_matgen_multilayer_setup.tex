\subsection{\MultiLayer Setup: MLP3}
\label{sxn:matgen_mlp3}

In this section, we describe the matrix generalization of the ST  model of the \LinearPerceptron; and, 
in particular, a matrix-generalized version of the key quantities we derived in Section~\ref{sxn:SMOG_main-st_av}.

\paragraph{A simple model.}

Consider a simple NN with three layers (two hidden and an output), i.e., a three-layer \MultiLayerPerceptron, denoted as the \emph{MLP3} model.
(This is a \emph{very} simple model of a modern NN with hundreds of layers and complex internal structure.)

Ignoring the bias terms, \emph{Without Loss of Generality}, (WLOG), the NN outputs
 $E_{1\mu},E_{2\mu},E_{3\mu}$ for each layer, as defined in \EQN~\ref{eqn:dnn_energy}, are given by:
\begin{align}
\nonumber
  \EVEC_{1\mu} &:= \frac{1}{\sqrt{N_1}} h (\WMAT_1^\top \boldsymbol{\xi}_{\mu}) , \\
\nonumber
  \EVEC_{2\mu} &:= \frac{1}{\sqrt{N_2}} h (\WMAT_2^\top \EVEC_{1\mu})     , \\
\label{eqn:nflow}
             y_{\mu} := \EVEC_{3\mu} &= \frac{1}{\sqrt{N_3}} h (\WMAT_{3}^\top \EVEC_{2\mu})     ,
\end{align}
where $\EVEC$ denotes a vector of energies or NN outputs, and $h$ is a general function or functional, denoting either a non-linear activation or a more complex layer structure, such as a CNN or an RNN.  Note that for binary classification, $\EVEC_3{\mu}=1|-1$ is just a binary number.  
We can consider $h(\cdot)$ to be an (unspecified) activation function.

Let us specify the ST error, or Energy difference, specifically in terms of the L2 or RMSE loss:
\begin{equation}
\label{eqn:DE}
\DETOPNN = \frac{1}{2}\sum_{\mu=1}^{\ND} (\Ys - \Yt)^2.
\end{equation}

We now start to develop the self-overlap \(\eta\) using \(\SMAT - \TMAT\):

\subsubsection{Data-Dependent Multi-Layer ST Self-Overlap
\texorpdfstring{$(\ETA(\SMAT,\TMAT))$}{ETA(S, T)}}

It is convenient to rewrite $\Delta \mathbf{\hat{E}}$ in \EQN~\ref{eqn:DE} as:
\begin{align}
\label{eqn:DE2}
\DETOPNN
   := \frac{1}{2} \Trace{ (\Ys - \Yt)^\top (\Ys - \Yt) }
   = N - \Trace{ (\YsVEC)^{\top} \, \YtVEC  }
   = \ND(1 - \ETAMLPXI)
\end{align}
where the \SelfOverlap $\ETAMLPXI$
is of the same form as the (vector) \LinearPerceptron (in \EQN~\ref{eqn:deriveSTerror}).
Note that $\ETAMLPXI$ depends on the (model) data $\boldsymbol{\xi}$
because we have not evaluated the expected value $\langle \cdots \rangle_{\boldsymbol{\xi}}$ yet.

Using the general expression from \EQN~\ref{eqn:nflow} for the action of the NN on the input data $\boldsymbol{\xi}$,
we can write the formal expression of the ST error for the simple MLP3 model as:
\begin{align}
\label{eqn:overlap1}
\ETAMLPAVG  \\ \nonumber
& =\langle\ETA(\SMAT_1,\SMAT_2,\SMAT_3,\TMAT_1,\TMAT_2,\TMAT_3,\boldsymbol{\xi})\rangle_{\AVGNDXI}  \\ \nonumber
& :=  \frac{1}{\ND ?}\Trace{
    \frac{1}{\sqrt{N_3}} h\left(\SMAT_3^{\top} 
    \frac{1}{\sqrt{N_2}} h\left(\SMAT_2^{\top} 
    \frac{1}{\sqrt{N_1}} h\left(\SMAT_1^{\top} \boldsymbol{\xi} \right)\right)\right)^{\top} 
    \times
    \frac{1}{\sqrt{N_3}} h\left(\TMAT_3^{\top} 
    \frac{1}{\sqrt{N_2}} h\left(\TMAT_2^{\top} 
    \frac{1}{\sqrt{N_1}} h\left(\TMAT_1^{\top} \boldsymbol{\xi} \right)\right)\right)
  }.
\end{align}

So far, we have not used any particular assumption on the form of the NN or the data, 
\michael{But this is the model Gaussian data, not the real data}
\charles{Yeah, I guess thats in the notation, but we have not integrated out the data yet.  We can rephrase this}
other than that the layer structure used to write the explicit expression for the form eventually needed,
$\ETA(\SMAT,\TMAT)$, a single layer \SelfOverlap.
As a next step, we show which assumptions are needed in order to reformulate the setup as
an effectively a single layer linear model for a NN.

\subsubsection{A Single Layer Matrix Model}
Following others in the literature~\cite{SMG2013_TR}, and for simplicity, one can restrict to the simplifying case that the function $h(x)$ is the identity function.
To evaluate \EQN~\ref{eqn:overlap1}, there are three possibilities.
First, we can multiply all the matrices together, and treat a multi-layer NN effectively as a single layer.
Under this assumption, \EQN \ref{eqn:overlap1} simplifies to
%
\begin{align}
\label{eqn:overlap2}
  \ETAMLPAVG &= \frac{1}{\ND}\tfrac{1}{N_3 \, N_2 \, N_1} 
  \Trace{ \boldsymbol{\xi}^\top 
    \SMAT_1 \SMAT_2 \SMAT_3 
    \TMAT_3^{\top} \TMAT_2^{\top} \TMAT_1^{\top} 
    \boldsymbol{\xi} }.
\end{align}
While this is possible, it would not lead to layer-by-layer insights (as \HTSR-based approaches do).
%
Second, we could attempt to expand \EQN~\ref{eqn:overlap2} into inter- and intra-layer terms, 
which we could readily do if the $S$ and $T$ matrices were square and the same shape, and then apply Wick's theorem:
\begin{align}
\ETAMLP\approx\frac{1}{\ND}\prod_{l=1}^{L}\tfrac{1}{N_l}\ETAMATAVG=
\frac{1}{\ND}\prod_{l=1}^{L}\tfrac{1}{N_l}\Trace{ \XI^{\top} \SMAT^{\top}_{l} \TMAT_{l} \XI) } +\text{intra-layer cross terms}.
\end{align}
However, these matrices are not square, and we don't know how to express the intra-layer cross terms.
%
Finally, we can simply assume that the individual layers are statistically independent, in which case we can treat each layer independently.
By ignoring the intra-layer cross-terms, let us write the single-layer self-overlap $\ETAMAT$ as:
\begin{align}
  \label{eqn:eta}
  \ETAMAT =
        \ETAMATAVG\rightarrow \
        \tfrac{1}{\ND}\tfrac{1}{N} \Trace{ \XI^{\top} \SMAT^{\top}_{l} \TMAT_{l} \XI }  .
\end{align}
This third approach is the one we will adopt.
Moving forward, we will drop the layer subscript, $l$, and we will consider a \SETOL as a single-layer theory.


\subsubsection{The Matrix-Generalized ST Overlap 
\texorpdfstring{$(\ETA(\SMAT,\TMAT)$)}{ETA(S, T)}.}
We can now relate the \SelfOverlap for the NN layer $\ETA(\XI)_{l}$ in \EQN~\ref{eqn:eta}
as a matrix-generalized form of the ST \EffectivePotential $\EPSL(R)=1-R$.
\michaeladdressed{We are calling it \EffectivePotential there; not sure if we are going to use that term or the other, consistently throughout.}
Since we can interpret the Trace as an expected value over the model data $\NDXI$, this gives the desired
\begin{align}
  \label{eqn:eta_mat_avg_def}
  \ETA(\SMAT,\TMAT)
  :=\tfrac{1}{\ND ?}\tfrac{1}{N} \Trace{\SMAT^{\top}\TMAT} 
\end{align}
This is the matrix generalized form of \EQN~\ref{eqn:QST_final} for the \LayerQuality.
\nred{The $1/\ND$ will be implicit in the matrices because they are observables}
