\subsection{\MultiLayer Setup: MLP3}
\label{sxn:matgen_mlp3}

In this section, we describe the matrix generalization of the ST  model of the \LinearPerceptron; and, 
in particular, a matrix-generalized version of the key quantities we derived in Section~\ref{sxn:SMOG_main-st_av}.

\paragraph{A simple model.}

Consider a simple NN with three layers (two hidden and an output), i.e., a three-layer \MultiLayerPerceptron, denoted as the \emph{MLP3} model.
(This is a \emph{very} simple model of a modern NN with hundreds of layers and complex internal structure.)

Ignoring the bias terms, \emph{Without Loss of Generality}, (WLOG), the NN outputs
 $E_{1\mu},E_{2\mu},E_{3\mu}$ for each layer, as defined in \EQN~\ref{eqn:dnn_energy}, are given by:
\begin{align}
\nonumber
  \EVEC_{1\mu} &:= \frac{1}{\sqrt{N_1}} h (\WMAT_1^\top \boldsymbol{\xi}_{\mu}) , \\
\nonumber
  \EVEC_{2\mu} &:= \frac{1}{\sqrt{N_2}} h (\WMAT_2^\top \EVEC_{1\mu})     , \\
\label{eqn:nflow}
             y_{\mu} := \EVEC_{3\mu} &= \frac{1}{\sqrt{N_3}} h (\WMAT_{3}^\top \EVEC_{2\mu})     ,
\end{align}
where $\EVEC_{\#\mu}$ denotes a vector of energies or NN outputs for that layer, and $h$ is a general function or functional, denoting either a non-linear activation or a more complex layer structure, such as a CNN or an RNN.  Note that for binary classification, $\EVEC_3{\mu}=1|-1$ is just a binary number.  
We can consider $h(\cdot)$ to be an (unspecified) activation function.

As in \EQN~\ref{eqn:deriveSTerror}, let us specify the ST error, or Energy difference, specifically in terms of the L2 or RMSE loss:
\begin{equation}
\label{eqn:DE}
\DETOPNN = \frac{1}{2}\sum_{\mu=1}^{\ND} (\Ys - \Yt)^2.
\end{equation}

We now start to develop the  matrix generalized form of the self-overlap \(\eta\):

\subsubsection{Data-Dependent Multi-Layer ST Self-Overlap
\texorpdfstring{$(\ETA(\SMAT,\TMAT))$}{ETA(S, T)}}

Following the same approach in Section~\ref{sxn:SMOG_main-st_av}
}, it is convenient to rewrite $\Delta \mathbf{\hat{E}}$ in \EQN~\ref{eqn:DE} as:
\begin{align}
\label{eqn:DE2}
\DETOPNN
   := \frac{1}{2} \Trace{ (\YsVEC - \YtVEC)^\top (\YsVEC - \YtVEC) }
   = \ND - \Trace{ (\YsVEC)^{\top} \, \YtVEC  }
   = \ND - \ETAMLPXI
\end{align}
where the \SelfOverlap $\ETAMLPXI$
is of the same form as the (vector) \LinearPerceptron (in \EQN~\ref{eqn:deriveSTerror}).
Note that $\ETAMLPXI$ depends on the (model) data $\NDXI$
because we have not evaluated the expected value $\langle \cdots \rangle_{\NDXI}$ yet.

Using the general expression from \EQN~\ref{eqn:nflow} for the action of the NN on the input data $\boldsymbol{\xi}$,
we can write the formal expression of the ST error for the simple MLP3 model as:
\begin{align}
\label{eqn:overlap1}
\ETAMLPAVG  \\ \nonumber
& =\langle\ETA(\SMAT_1,\SMAT_2,\SMAT_3,\TMAT_1,\TMAT_2,\TMAT_3,\NDXI)\rangle_{\AVGNDXI}  \\ \nonumber
& :=  \frac{1}{\ND}\Trace{
    \frac{1}{\sqrt{N_3}} h\left(\SMAT_3^{\top} 
    \frac{1}{\sqrt{N_2}} h\left(\SMAT_2^{\top} 
    \frac{1}{\sqrt{N_1}} h\left(\SMAT_1^{\top} \NDXI \right)\right)\right)^{\top} 
    \times
    \frac{1}{\sqrt{N_3}} h\left(\TMAT_3^{\top} 
    \frac{1}{\sqrt{N_2}} h\left(\TMAT_2^{\top} 
    \frac{1}{\sqrt{N_1}} h\left(\TMAT_1^{\top} \NDXI \right)\right)\right)
  }.
\end{align}

So far, we have not used any particular assumption on the form of the NN or the data, 
other than that the layer structure used to write the explicit expression for the form eventually needed,
$\ETA(\SMAT,\TMAT)$, a single layer \SelfOverlap.
As a next step, we show which assumptions are needed in order to reformulate the setup as
an effectively a single layer linear model for a NN.

\subsubsection{A Single Layer Matrix Model}
Following others in the literature~\cite{SMG2013_TR}, and for simplicity, one can restrict to the simplifying case that the function $h(x)$ is the identity function.
To evaluate \EQN~\ref{eqn:overlap1}, there are three possibilities.
First, we can multiply all the matrices together, and treat a multi-layer NN effectively as a single layer.
Under this assumption, \EQN \ref{eqn:overlap1} simplifies to
%
\begin{align}
\label{eqn:overlap2}
  \ETAMLPAVG &= \frac{1}{\ND}\tfrac{1}{N_3 \, N_2 \, N_1} 
  \Trace{ \boldsymbol{\xi}^\top 
    \SMAT_1 \SMAT_2 \SMAT_3 
    \TMAT_3^{\top} \TMAT_2^{\top} \TMAT_1^{\top} 
    \boldsymbol{\xi} }.
\end{align}
While this is possible, it would not lead to layer-by-layer insights (as \HTSR-based approaches do).
%
Second, we could attempt to expand \EQN~\ref{eqn:overlap2} into inter- and intra-layer terms, 
which we could readily do if the $S$ and $T$ matrices were square and the same shape, and then apply Wick's theorem:
\begin{align}
\ETAMLP\approx\frac{1}{\ND}\prod_{l=1}^{L}\tfrac{1}{N_l}\ETAMATAVG=
\frac{1}{\ND}\prod_{l=1}^{L}\tfrac{1}{N_l}\Trace{ (\NDXI)^{\top} \SMAT^{\top}_{l} \TMAT_{l} (\NDXI)) } +\text{intra-layer cross terms}.
\end{align}
However, these matrices are not square, and we don't know how to express the intra-layer cross terms.
%
Finally, we can simply assume that the individual layers are statistically independent, in which case we can treat each layer independently.
By ignoring the intra-layer cross-terms, let us write the single-layer self-overlap $\ETAMAT$ as:
\begin{align}
  \label{eqn:eta}
  \ETAMAT =
        \ETAMATAVG\rightarrow \
        \tfrac{1}{\ND}\tfrac{1}{N} \Trace{ (\NDXI)^{\top} \SMAT^{\top}_{l} \TMAT_{l} (\NDXI) }  .
\end{align}
This third approach is the one we will adopt.
Moving forward, we will drop the layer subscript, $l$, and we will consider a \SETOL as a single-layer theory.


\subsubsection{The Matrix-Generalized ST Overlap 
\texorpdfstring{$(\ETA(\SMAT,\TMAT)$)}{ETA(S, T)}.}
We can now relate the \SelfOverlap for the NN layer $\ETA(\XI)_{l}$ in \EQN~\ref{eqn:eta}
as a matrix-generalized form of the ST \EffectivePotential $\EPSL(R)=1-R$.
\michaeladdressed{We are calling it \EffectivePotential there; not sure if we are going to use that term or the other, consistently throughout.}
Since we can interpret the Trace as an expected value over the model data $\NDXI$, this gives the desired
\begin{align}
  \label{eqn:eta_mat_avg_def}
  \ETA(\SMAT,\TMAT)
  :=\tfrac{1}{N} \Trace{\SMAT^{\top}\TMAT} 
\end{align}
\nred{DO WE need to repeat this ?:  the $1/\ND$ will be implicit in the $\TMAT$ and $\SMAT$ matrices because they represent empirical observables. In other words, the weights are the learned parameters, effectively averaged over the entire real-world training data set $\ADD$}
This is the matrix generalized form of \EQN~\ref{eqn:QST_final} for the \LayerQuality.

