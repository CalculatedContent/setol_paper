\subsection{\SETOL Overview}
\label{sxn:setol_overview}

\charles{Some old text here I need to clean up}

Our \SETOL formulates a parametric expression for the \LayerQuality $\Q$ using a matrix-generalization of the classic \StudentTeacher (ST)
model from the \SMOG theory of the 1990s~\cite{SST92,STS90}, 
\cyan{but with a \SemiEmpirical twist in that the \Teacher is now an actual, trained NN that is input to theory.}
%\red{albeit with a radical departure in terms of the meaning of the \Student and \Teacher, which our theory will require. This expression can be} evaluated using
Recent advances in the evaluation of so-called HCIZ random matrix integrals~\cite{potters_bouchaud_2020,Tanaka2007, Tanaka2008},
allow the final expression for $\Q$ to be written in terms of empirically measured statistical properties of the layer ESD.
We summarize our basic approach here; see Section~\ref{sxn:matgen} for a detailed derivation, and see Section~\ref{sxn:empirical} for a detailed empirical analysis.  \cyan{For reference here and later, Figure~\ref{fig:cetl-flow} displays a flowchart of the conceptual development of \SETOL.}

\red{We may want to align the text below more closely with the flowchart..will review }
% NOVE THIS TO 5 and weave into discussion on what is a semiempirical theory
%\red{We deviate from the standard \SMOG theory in an important way -- we make no direct assumption that the labels are realizable by a \Teacher NN. \SETOL makes its realizability assumption in a novel way: rather than assume that there exists an unknown \Teacher that \emph{generates} the labels, \SETOL assumes that the labels are learnable only if the trained model, (which we refer to as the \Teacher in a slight abuse of terminology,) meets the \TRACELOG condition and has an \ECS, and the \ECS will have an ESD as provided by the \Teacher. The essence of \SETOL is that {\bf we assume that the \Teacher provides the ESD of the weight matrix, \emph{not} the labels.} We treat the \Student as a sample from the Boltzmann distribution of model training, much the same as in \SMOG theory. }
%
%This point is both subtle -- since the derivations will proceed almost identically to those found in~\cite{engel2001statistical,EngelAndVanDenBroeck,SST90,SST92} -- and also radical, because our derived quality metrics are now measures of concentration of the Boltzmann distribution in terms of the trained weight matrix spectral density, which can make far more specific predictions without the need for more labels. This also allows us to discuss label noise in terms of the \Teacher, since we are not bound by the labels themselves.}

\red{As is done in} the \StudentTeacher (ST) model~\cite{SST92}, 
we first formulate the \GeneralizationError $(\AVGE^{ST}_{gen})$ of the linear \Perceptron (in the \emph{\AnnealedApproximation},
and in the \emph{\HighTemperature} limit; see Section ~\ref{sxn:SMOG_main}), and 
we then generalize this to the case of a NN $(\AVGE^{ST}_{gen}\rightarrow\AVGE^{NN}_{gen})$, so that we can analyze the \Quality of each layer.
For the \Perceptron, the \GeneralizationError is an Energy, given as $\AVGE^{ST}_{gen}:=\THRMAVG{1-R}$, where $R$ is the ST vector overlap,
and $\THRMAVG{\cdots}$ is a \emph{\ThermalAverage} (defined in Section~\ref{sxn:mathP}),
a Boltzmann-weighted average.
In this case, the model \Quality, $\Q^{ST}$ is exactly the AA, high-T \AverageGeneralizationAccuracy 
$\Q^{ST}:=1-\AVGE^{ST}_{gen}=\THRMAVG{R}$.
For an MLP or general NN, each layer's Energy is (negatively) associated with a
\LayerQuality $\Q$, which we identify as the average contribution an
individual layer contributes to the overall generalized accuracy,
(i.e $1-\AVGE^{NN}_{gen}$) for a multilayer perceptron (MLP).

\cyan{\textbf{Importantly, we deviate from the traditional approach in that we take the \Teacher as a fixed, empirical input to the theory.}  The Teacher contribution is then formulated in terms of the actual the layer eigenvalues, or Empirical Spectral Density (ESD).  In this way, the theory becomes \SemiEmpirical}


For technical reasons (below), we will seek the
\emph{\LayerQuality (Squared)} $\QT$, which is defined as the \ThermalAverage
of the matrix-generalized overlap ($\Trace{\OVERLAP^{2}}$),
\begin{align}
  \label{eqn:QT}
  \QT :=   \THRMAVGIZ{\Trace{\OVERLAP^{2}}}
\end{align}
where $\OVERLAP^{2}$ can be thought of as a \Hamiltonian for the \QualitySquared  $(\HBARE=\OVERLAP^{\top}\OVERLAP)$.

In \EQN~\ref{eqn:QT}, the so-called \emph{\Teacher} ($T$) is the NN model under consideration,
%where $\TMAT$ and $\SMAT$  are the \Teacher and \Student layer weight matrices, resp,
and 
%$\mathbf{R}$ 
$\mathbf{R}:=\frac{1}{N}{\SMAT^{\TR}\TMAT}$
denotes the ST overlap operator 
between the \Teacher layer weight matrix $\TMAT$ and a similar \emph{\Student} ($S$) layer weight matrix $\SMAT$.
The notation $\THRMAVGIZ{\cdots}$ denotes a \ThermalAverage over
all \Student weight matrices $\SMAT$ that resemble the \Teacher weight matrix $\TMAT$.
By ``resemble'', the \SETOL approach assumes that 
the ESD of $\SMAT$ has the same \emph{limiting} form as $\TMAT$, placing them in the same \HTSR Universality class.
This is made more precise below.

\paragraph{The Overlap $\OVERLAP$, and the Inner and Outer forms of $\AMAT$, $\AMATM$ and $\AMATN$, resp.}
Let us now express the average matrix-matrix overlap $\mathbf{R}$ in squared form using:
\begin{align}
  \label{eqn:R2}
\Trace{\OVERLAP^{2}} &:=\OLAPSQD  \\ \nonumber
&=\frac{1}{N^{2}}\Trace{\TMAT^{\TR}\SMAT\SMAT^{\TR}\TMAT}
=\frac{1}{N}\Trace{\TMAT^{\TR}\AMATN\TMAT}
\end{align}
where $\AMATN$ is called the \emph{Outer} \Student correlation matrix, and is  the $N\times N$ form of the \Student correlation matrix,
$\AMATN:=\frac{1}{N}\mathbf{SS}^{\TR}$.
We will also define the $M\times M$ \emph{Inner} \Student correlation matrix $\AMATM=\frac{1}{N}\SMAT^{\TR}\SMAT$, which is 
used later. Note that $\AMATM$ also has a $\frac 1 N$ factor, so that its non-zero eigenvalues will be the same as $\AMATN$, but it has dimensions $M \times M$. We will denote the Outer $\AMATN$ and Inner $\AMATM$ forms when the context demands it, and will simply use the general form $\AMAT$ when they can be interchanged.

%It is argued that the \Student and \Teacher weight matrices (i.e., $\TMAT, \SMAT$) have
%specific generalizing eigencomponents that, after training,
%concentrate in a low rank subspace, called the \emph{\EffectiveCorrelationSpace} (\ECS)
%(and that this space satisfies the \TRACELOG condition, described below).
%This is both consistent with the empirical observations and necessary to apply the HCIZ integral
%methods, both described below.

\paragraph{The Quality-Squared Generating Function $\IZG$.}
As explained in Section~\ref{sxn:mathP}, this \QualitySquared is more readily obtained as 
the derivative of the \LayerQualitySquared \GeneratingFunction, $\IZG$, defined as
\begin{align}
  \label{eqn:IZG_QT}
  \QT := \dfrac{1}{\beta}\dfrac{\partial }{\partial \ND} \lim_{N\gg 1}\IZG
\end{align}
where $\ND$ is the number of training examples used to train the model.

$\IZG$ is essentially ($\beta$ times) a \emph{negative \FreeEnergy} for the (approximate) \LayerQualitySquared
(see Section~\ref{sxn:SMOG_main}, and the Appendix, Section~\ref{sxn:quality}).
For more details, see Section~\ref{sxn:matgen}, and the Appendix, Section~\ref{sxn:summary_sst92}).

We can write $\IZG$ as an HCIZ Integral, 
\begin{align}
  \label{eqn:QT_dS}
  \IZG  &= \tfrac{1}{N} \ln\int d\mu(\SMAT)\exp\left(\ND \beta N\Trace{\tfrac{1}{N}\TMAT^{\TR}\AMATN\TMAT}\right),
\end{align}
where $n$ is the number of training examples used to train the models.

The \SETOL approach then seeks to express $\IZG$ in~\EQN~\ref{eqn:QT_dS} as an HCIZ integral (and in the \WideLayer \LargeN limit in $N$$)~$\cite{potters_bouchaud_2020,Tanaka2007,Tanaka2008}.
We evaluate this at large-$N$ in $N$, and write
\begin{equation}
  \IZGINF:=\lim_{N\gg 1}\IZG
\end{equation}
The result is effectively expressed in the limit of fixed \emph{layer load} $\ND/N$, analogous to a renormalized mean-field  (i.e., \SemiEmpirical) theory over $N$ interacting (feature) vectors of length $M$.
In other words, in taking the \LargeN in $N$ limit, we assume implicitly that $\IZG$ is in the \ThermodynamicLimit (and at high-T).

With these definitions in place, moving forward, the following key assumptions, which can be tested empirically, must hold:
\begin{itemize}
  \item
  \textbf{The Effective Correlation Space (\ECS) Condition.}
  The generalizing components of the \Student (and \Teacher) layer weight matrices concentrate into a lower rank subspace---the~\ECS---spanned by the
  eigenvectors associated with the (heavy) tail of the layer ESD $\rho_{tail}(\lambda)$, such that the test error can 
  be reproduced with only these components. 
  We write $\AECS$ to denote the projection of the correlation
  matrix $\AECS:=\mathbf{P}_{ecs}\AMAT$, onto this subspace, now with rank $\MECS\ll M$.
  This restricts the measure $d\mu(\AMAT)$ to the~\ECS, $(d\mu(\AMAT)\rightarrow d\mu(\AECS))$.
  This assumption will be empirically examined using real-world \Teacher weight matrices $\TMAT=\WMAT$
  in Section~\ref{sxn:empirical-effective_corr_space}. 
%  Importantly, this lets interpret the \EQN~\ref{eqn:QT_HCIZ}  as acting only in this subspace.
  \item
  \textbf{The \TRACELOG Condition.}
  The Effective Student correlation matrix $\AECS$ 
  satisfies the \ERG condition that $\Trace{\ln\AECS}=\ln\Det{\AECS}=0$,
  so that the change of measure $d\mu(\SMAT) \rightarrow d\mu(\AECS) $ is Volume Preserving.
  This condition is derived explicitly in terms of Inner form,  $\AECSM$, and will also  hold for the Outer form, $\AECSN$
  Practically, this implies that the $\MECS$ eigenvalues $\LambdaECS$
  of the tail of the ESD must satisfy $\sum_{i=1}^{\MECS}\ln\LambdaECS_{i}\approx 0$.
  Experiments will test this assumption explicitly in Section~\ref{sxn:empirical-trace_log}.
\end{itemize}
Remarkably, both conditions hold best empirically when the \HTSR PL quality metric $\alpha\gtrsim 2$ is \Ideal. Motivated from these empirical observations, we have:
\begin{itemize}
  \item
  \textbf{$\IZGINF$ is expressed as an HCIZ integral, at large-$N$.}
  We have
  \begin{align}
  \label{eqn:IZGINF_HCIZ}
  \IZGINF = \lim_{N\gg 1}\tfrac{1}{N}\ln\int d\mu(\AECS)\exp\left(\ND\beta\Trace{\TMAT^{\TR}\AECSN\TMAT}\right) %= \beta\sum_{i=1}^{\MECS}\GNECS
  \end{align}
  where  measure $d\mu(\AECS)$ lets us average over all (Outer) \Student Correlation matrices $\AECSN$ explicitly, which
  lie in the~\ECS space and which ``resemble'' the \Teacher.
By ``resemble'' we mean that they share the same functional form in
  their limiting ESDs,
   $\rho^{\infty}_{\AECS}(\lambda)\sim\rho^{\infty}_{\XECS}(\lambda)$.
  \item
  \textbf{The Layer Quality (Squared) $\QT$ is a~\GEN.}
  The final expression for $\QT$ can be written as the derivative of $\IZGINF$  as
  \begin{align}
    \label{eqn:QT_result}
    \QT = \dfrac{1}{\beta}\dfrac{\partial}{\partial \ND}\IZGINF = \sum_{i=1}^{\MECS}\GNI
  \end{align}
  where $\GNI$ is, albeit imprecisely, a \emph{\GEN}, and is  defined as the integrated \emph{\RTransform} $R(z)$ of the \Teacher
  layer ESD (where $z\in\mathbb{C}$), such that $\GN:= \int_{\LambdaECSmin}^{\lambda}R(z)dz$
  and $\LambdaECSmin$  encapsulates  the~\ECS (and, for some choices of $R(z)$,  selects the desired branch-cut of $R(z)$
  so that it is both analytic and well-behaved) .
\end{itemize}

%\red{[I think this one \EQN~\ref{eqn:QT_result} is the prize of the paper. The opening of Section \ref{sxn:setol} should refer to it, and this should say "See the derivation of \EQN XXX in Section \ref{sxn:comp_rmt}.]}
%\cyan{This IS section 3}
\chris{The flowchart is good enough for me.}

To apply the theory, one must choose an \RTransform $R(z)$ for the \Teacher that models 
the tail of the ESD $\rho^{emp}_{T}(\lambda)$, and that can be
parameterized by some measurable property.
This may include the number of Spikes $\lambda^{spike}$, the fitted PL exponent $\alpha$,
the maximum eigenvalue $\lambda_{max}$, or even the entire tail $\rho^{tail}_{T}(\lambda)$.
This may be a formal expression, a computational procedure, or some combination. 

To integrate $R(z)$, however, and to have a physically meaningful result,
one must ensure that $R(z)$ is both
analytic and single-valued on the domain of interest, namely, the \ECS (and therefore
the (PL) tail of the ESD),  $\Re[z] \ge \LambdaECSmin$.
\footnote{$\Re[z]$ is the Real part of $z$.}
Because the ESD is frequently \HeavyTailed (HT), this
\RTransform $R(z)$ may have a branch-cut, and it is expected that this will occur
at $\Re[z]\approx\LambdaECSmin$, corresponding to a point roughly at or before the start of the \ECS (i.e., at the peak of the ESD on a log scale).
Selecting the branch-cut $R(z)$ provides an additional physical meaning to the \ECS.

To complete the theory, we
will also show that the \HTSR PL \LayerQuality metrics \ALPHA $(\alpha)$ and \ALPHAHAT $(\ALPHAHATEQN)$
can be formally derived directly from the \SETOL \LayerQuality $\Q$ by selecting the appropriate
\RTransform $R(z)$ and making empirically motivated approximations. In Section~\ref{sxn:r_transforms} we provide several possible
models of $R(z)$ and the resulting \LayerQuality $\Q$.

\paragraph{Renormalization Group Effective Hamiltonian}
The formulation of \SETOL closely parallels the construction of an \EffectiveHamiltonian~$\HEFF$
via the \WilsonExactRenormalizationGroup~(\ERG) approach. Consider a \emph{\Bare} Hamiltonian $\HBARE$ for the \LayerQualitySquared,
defined as $\HBARE:= \mathbf{R}^{\TR}\mathbf{R}$.
We can express \EQN~\ref{eqn:QT_dS} in terms of this \Bare Hamiltonian $\HBARE$,
and rewrite \EQN~\ref{eqn:IZGINF_HCIZ} in terms of a \emph{Renormalized} \EffectiveHamiltonian~$\HEFF$
that spans the \EffectiveCorrelationSpace~(\ECS). Formally, we have:
\begin{align}
\label{eqn:RG}
\tfrac{1}{N}\ln \int d\mu(\SMAT)e^{\ND \beta N \operatorname{Tr}[\HBARE]} \;\xrightarrow{ERG}\; \lim_{N\gg 1}\tfrac{1}{N}\ln \int d\mu(\AECS)e^{\ND \beta N \operatorname{Tr}[\HEFF]} 
\end{align}
where the \ERG transformation is defined by the \ScaleInvariant change of measure,
applied in the \WideLayer \LargeN limit in $N$,
and where $\HEFF$ is defined implicitly through the result for $\QT$ (\EQN~\ref{eqn:QT_result}).
The result is, formally, a sum of the integrated \RTransforms $\GNI$.
In a sense, this result resembles (a non-perturbative form of) the Linked Cluster Theorem
in that the log \PartitionFunction is expressed as a sum of integrated matrix-generalized cumulants.  And in analogy with \SemiEmpirical theories of Quantum Chemistry, the \HTSR \ALPHA $(\alpha)$ and \ALPHAHAT $\ALPHAHATEQN)$ enter as renormalized empirical parameters.
Most importantly, the \ScaleInvariant \TRACELOG condition can be verified empirically (See Section~\ref{sxn:empirical}.)
Importantly, in analogy with the Wilson \ExactRenormalizationGroup theory, 
the \HTSR $\alpha=2$ resembles in spirit
an \ERG \emph{Universal Critical Exponent} at a phase boundary being between the \HeavyTailed 
(HT) and the \VeryHeavyTailed (VHT) phase of learning of the \HTSR theory.
This observation strengthens our argument that the \HTSR HT and VHT phases
are analogous to the generalizing and overfitting phases, respectively,
of the classical \SMOG theories of NN learning.


