\subsection{\SETOL Overview}
\label{sxn:setol_overview}

\charles{Some old text here I need to clean up}

Our \SETOL formulates a parametric expression for the \LayerQuality $\Q$ using a matrix-generalization of the classic \StudentTeacher (ST)
model from the \SMOG theory of the 1990s~\cite{SST92,STS90}, evaluated using
recent advances in the evaluation of so-called HCIZ random matrix integrals~\cite{potters_bouchaud_2020,Tanaka2007, Tanaka2008},
such that the final expression for $\Q$ can be written in terms of empirically measured statistical properties of the layer ESD.
We summarize our basic approach here; see Section~\ref{sxn:matgen} for a detailed derivation, and see Section~\ref{sxn:empirical} for a detailed empirical analysis.

Following the \StudentTeacher (ST) model~\cite{SST92}, 
we first formulate the \GeneralizationError $(\AVGE^{ST}_{gen})$ of the linear \Perceptron (in the \emph{\AnnealedApproximation},
and in the \emph{\HighTemperature} limit; see Section ~\ref{sxn:SMOG_main}), and 
we then generalize this to the case of a NN $(\AVGE^{ST}_{gen}\rightarrow\AVGE^{NN}_{gen})$, so that we can analyze the \Quality of each layer.
For the \Perceptron, the \GeneralizationError is an Energy, given as $\AVGE^{ST}_{gen}:=\THRMAVG{1-R}$, where $R$ is the ST vector overlap,
and $\THRMAVG{\cdots}$ is a \emph{\ThermalAverage} (defined in Section~\ref{sxn:mathP}),
a Boltzmann-weighted average.
In this case, the model \Quality, $\Q^{ST}$ is exactly the AA, high-T \AverageGeneralizationAccuracy 
\michaeladdressed{still not sure we are using that term correctly, here and elsewhere}
$\Q^{ST}:=1-\AVGE^{ST}_{gen}=\THRMAVG{R}$.
For an MLP or general NN, each layer Energy is associated with a
\LayerQuality $\Q$, which we identify as the average contribution an
individual layer makes to the overall generalized accuracy,
(i.e $1-\AVGE^{NN}_{gen}$) for a multilayer perceptron (MLP).

For technical reasons (below), we will seek the
\emph{\LayerQuality (Squared)} $\QT$, which is defined as the \ThermalAverage
of the matrix-generalized overlap ($\Trace{\OVERLAP^{2}}$),
\begin{align}
  \label{eqn:QT}
  \QT :=   \THRMAVGIZ{\Trace{\OVERLAP^{2}}}
\end{align}
where $\OVERLAP^{2}$ can be thought of as a \Hamiltonian for the \QualitySquared  $(\HBARE=\OVERLAP^{\top}\OVERLAP)$.

In \EQN~\ref{eqn:QT}, the so-called \emph{\Teacher} ($T$) is the NN model under consideration,
%where $\mathbf{T}$ and $\mathbf{S}$  are the \Teacher and \Student layer weight matrices, resp,
and 
%$\mathbf{R}$ 
$\mathbf{R}:=\frac{1}{N}{\mathbf{S}^{\TR}\mathbf{T}}$
denotes the ST overlap operator 
between the \Teacher layer weight matrix $\TMAT$ and a similar \emph{\Student} ($S$) layer weight matrix $\SMAT$.
The notation $\THRMAVGIZ{\cdots}$ denotes a \ThermalAverage over
all \Student weight matrices $\mathbf{S}$ that resemble the \Teacher weight matrix $\mathbf{T}$.
By ``resemble'', the \SETOL approach assumes that 
the ESD of $\mathbf{S}$ has the same \emph{limiting} form as $\mathbf{T}$, placing them in the same \HTSR Universality class.
This is made more precise below.

\paragraph{The Overlap $\OVERLAP$, and the Inner and Outer forms of $\AMAT$, $\AMATM$ and $\AMATN$, resp.}
Let us now express the average matrix-matrix overlap $\mathbf{R}$ in squared form using:
\begin{align}
  \label{eqn:R2}
\Trace{\OVERLAP^{2}} &:=\OLAPSQD  \\ \nonumber
&=\frac{1}{N^{2}}\Trace{\mathbf{T}^{\TR}\mathbf{SS}^{\TR}\mathbf{T}}
=\frac{1}{N}\Trace{\mathbf{T}^{\TR}\mathbf{A}_N\mathbf{T}}
\end{align}
where $\AMATN$ is called the \emph{Outer} \Student correlation matrix, and is  the $N\times N$ form of the \Student correlation matrix,  
$\mathbf{A}_N:=\frac{1}{N}\mathbf{SS}^{\TR}$.
We will also define the  the \emph{Inner} \Student correlation matrix,  the $M\times M$ matrix, $\AMATM=\frac{1}{N}\mathbf{S}^{\TR}\mathbf{S}$, which is 
used later. Note that $\AMATM$ also has a $\frac 1 N$ factor, so that its non-zero eigenvalues will be the same as $\AMATN$, but has dimensions $M \times M$. We will denote the Outer  $\AMATN$ and Innder $\AMATM$ forms when the context demands it, and will simply use the general form $\AMAT$ when they can be interchanged.

\michaeladdressed{Clarify.}
%It is argued that the \Student and \Teacher weight matrices (i.e., $\TMAT, \SMAT$) have
%specific generalizing eigencomponents that, after training,
%concentrate in a low rank subspace, called the \emph{\EffectiveCorrelationSpace} (\ECS)
%(and that this space satisfies the \TRACELOG condition, described below).
%This is both consistent with the empirical observations and necessary to apply the HCIZ integral
%methods, both described below.
\michaeladdressed{MM TO DO: Dense.  Where is the best place for this par.}

\paragraph{The Quality-Squared Generating Function $\IZG$.}
As explained in Section~\ref{sxn:mathP}, this \QualitySquared is more readily obtained as 
the derivative of the \LayerQualitySquared \GeneratingFunction, $\IZG$, defined as
\begin{align}
  \label{eqn:IZG_QT}
  \QT := \dfrac{1}{\beta}\dfrac{\partial }{\partial \ND} \lim_{N\gg 1}\IZG
\end{align}
\red{where $\ND$ is the number of training examples in the model.}

$\IZG$ is essentially ($\beta$ times) a \emph{negative \FreeEnergy} for the (approximate) \LayerQualitySquared.
(see Section~\ref{sxn:SMOG_main}, and the Appendix, Section~\ref{sxn:quality}).
For more details, see Section~\ref{sxn:matgen}, and the Appendix, Section~\ref{sxn:summary_sst92}).

We can write $\IZG$ as an HCIZ Integral, 
\begin{align}
  \label{eqn:QT_dS}
  \IZG  &= \red{\tfrac{1}{N}} \ln\int d\mu(\mathbf{S})\exp\left(\ND \beta N\Trace{\mathbf{T}^{\TR}\mathbf{A}_N\mathbf{T}}\right),
\end{align}

The \SETOL approach then seeks to express $\IZG$ in~\EQN~\ref{eqn:QT_dS} as an HCIZ integral (and in the large-N limit)~\cite{potters_bouchaud_2020,Tanaka2007,Tanaka2008}.
We evaluate this at large-$N$, and write
\begin{equation}
  \IZGINF:=\lim_{N\gg 1}\IZG
\end{equation}
\nred{The result is effectively expressed in the limit of fixed load $N/\ND$, analogous to a  renormalized mean-field  (i.e., \SemiEmpirical) theory over $N$ interacting feature vectors of length $M$.}
%%%must restrict the matrices to a lower rank subspace, called the~\ECS,
%%%i.e.,  $\mathbf{\tilde{A}}$. 
%%%We also must change the measure from an integral over \Student weight matrices
%%%to an integral over \Student Correlation matrices, i.e., 
%%%\begin{align}
%%%  \label{eqn:change_measure}
%%%  d\mu(\mathbf{S})\rightarrow d\mu(\mathbf{\tilde{A}}) ,
%%%\end{align}
%%%We can define a single measure for either form of \Student correlation matrix, restricted to the~\ECS,
%%%such that 
%%%$d\mu(\mathbf{\tilde{A}})=d\mu(\mathbf{\tilde{A}}_1)=d\mu(\mathbf{\tilde{A}}_2)$.
%%%This will be important as we need the form $\AMAT_1$ to derive the \TRACELOG condition, below, but
%%%we  formulate the  $\IZG$ using the form $\AMAT_2$.
%%%%Also, and importantly, the change in measure from $d\mu(\mathbf{S})$ to $d\mu(\mathbf{\tilder{A}})$
%is needed in order to apply the HCIZ  integral.
With these definitions in place, moving forward, the following key assumptions, which can be tested empirically, must hold:
\begin{itemize}
  \item
  \textbf{The Effective Correlation Space (\ECS) Condition.}
  The generalizing components of the \Student (and \Teacher) layer weight matrices concentrate into a lower rank subspace---the~\ECS---spanned by the
  eigenvectors associated with the (heavy) tail of the layer ESD $\rho_{tail}(\lambda)$, such that the test error can 
  be reproduced with only these components. 
  \michael{MM TO DO: we need to define tail better somewhere, but see also my comment above about presenting the two generalizing subspaces better.}
  We write $\AECS$ to denote the projection of the correlation
  matrix $\AECS:=\mathbf{P}_{ecs}\AMAT$, onto this subspace, now with rank $\MECS\ll M$.
  This restricts the measure $d\mu(\AMAT)$ to the~\ECS, $(d\mu(\AMAT)\rightarrow d\mu(\AECS))$.
  This assumption will empirically examined using real-world \Teacher weight matrices $\TMAT=\WMAT$
  in Section~\ref{sxn:empirical-effective_corr_space}. 
%  Importantly, this lets interpret the \EQN~\ref{eqn:QT_HCIZ}  as acting only in this subspace.
  \item
  \textbf{The \TRACELOG Condition.}
  The Effective Student correlation matrix $\AECS$ 
  satisfies the condition that $\Trace{\ln\AECS}=\ln\Det{\AECS}=0$,
  so that the change of measure $d\mu(\SMAT) \rightarrow d\mu(\AECS) $ is Volume Preserving.
  This condition is derived explicitly in terms of Inner form,  $\AECSM$, and will also  hold for the Outer form, $\AECSN$
  Practically, this implies that the $\MECS$ eigenvalues $\LambdaECS$
  of the tail of the ESD must satisfy $\sum_{i=1}^{\MECS}\ln\LambdaECS_{i}\approx 0$.
  Experiments will test this assumption explicitly in Section~\ref{sxn:empirical-trace_log}.
\end{itemize}
Remarkably, both conditions hold best empirically when the \HTSR PL quality metric $\alpha\gtrsim 2$ is \Ideal. Motivated from these empirical observations, we have:
\begin{itemize}
  \item
  \textbf{$\IZGINF$ is expressed as an HCIZ integral, at large-$N$.}
  We have
  \begin{align}
  \label{eqn:IZGINF_HCIZ}
  \IZGINF = \lim_{N\gg 1}\red{\tfrac{1}{N}}\ln\int d\mu(\AECS)\exp\left(\ND\beta\Trace{\mathbf{T}^{\TR}\AECSN\mathbf{T}}\right) %= \beta\sum_{i=1}^{\MECS}\GNECS
  \end{align}
  where  measure $d\mu(\AECS)$ lets us average over all (Outer) \Student Correlation matrices $\AECSN$ explicitly, which
  lie in the~\ECS space and which ``resemble'' the \Teacher, 
  where by ``resemble'' we mean that they share the same form of the tail of
  their limiting ESDs,
  i.e., $\rho^{\infty}_{\AECS}(\lambda)\sim\rho^{\infty}_{\XECS}(\lambda)$.
  \item
  \textbf{The Layer Quality (Squared) $\QT$ is a~\GEN.}
  The final expression for $\QT$ can be written as the derivative of $\IZGINF$  as
  \begin{align}
    \label{eqn:QT_result}
    \QT = \dfrac{1}{\beta}\dfrac{\partial}{\partial \ND}\IZGINF = \sum_{i=1}^{\MECS}\GNI
  \end{align}
  where $\GNI$ is, albeit imprecisely, a \emph{\GEN}, and is  defined as the integrated \emph{\RTransform} $R(z)$ of the \Teacher
  layer ESD (where $z\in\mathbb{C}$), such that $\GN:= \int_{\LambdaECSmin}^{\lambda}R(z)dz$
  and $\LambdaECSmin$  encapsulates  the~\ECS (and, for some choices of $R(z)$,  selects the desired branch-cut of $R(z)$
  so that it is both analytic and well-behaved) .
\end{itemize}

To apply the theory, one must choose an \RTransform $R(z)$ for	the \Teacher that models 
the tail of the ESD $\rho^{emp}_{T}(\lambda)$, and that can be
parameterized by some measurable property.
This may include the number of Spikes $\lambda^{spike}$, the fitted PL exponent $\alpha$,
the maximum eigenvalue $\lambda_{max}$, or even the entire tail $\rho^{tail}_{T}(\lambda)$.
This may be a formal expression, a computational procedure, or some combination.

To integrate $R(z)$, however, to have a physically meaningful result,
one must ensure that $R(z)$ is both
analytic and single-valued on the domain of interest, namely, the \ECS (and therefore
the (PL) tail of the ESD),  $z \ge \LambdaECSmin$.
Because the ESD is frequently \HeavyTailed (HT), this
\RTransform $R(z)$ may have a branch-cut, and it is expected that this will occur
at $z\approx\LambdaECSmin$, corresponding roughly at or before the start of the \ECS (i.e., at the peak of the ESD on a log scale).
Selecting the branch-cut $R(z)$ provides an additional physical meaning to the \ECS.

To complete the theory, we
will also show that the \HTSR PL \LayerQuality metrics \ALPHA $(\alpha)$  and \ALPHAHAT $(\ALPHAHATEQN)$
can be formally derived directly from the \SETOL \LayerQuality $\Q$ by selecting the appropriate
\RTransform $R(z)$ and making empirically motivated approximations. In Section~\ref{sxn:r_transforms} we provide several possible
models of $R(z)$ and the resulting \LayerQuality $\Q$.

\paragraph{Renormalization Group Effective Hamiltonian}
The formulation of SETOL closely parallels the construction of an \EffectiveHamiltonian~$\HEFF$
via the \WilsonExactRenormalizationGroup~(\ERG) approach. Consider a \emph{\Bare} Hamiltonian $\HBARE$ for the \LayerQualitySquared,
defined as $\HBARE:= \mathbf{R}^{\TR}\mathbf{R}$.
We can express Eqn.\ref{eqn:QT_dS} in terms of this \Bare Hamiltonian $\HBARE$,
and rewrite Eqn.\ref{eqn:IZGINF_HCIZ} in terms of an \emph{Renormalized} \EffectiveHamiltonian~$\HEFF$
that spans the \EffectiveCorrelationSpace~(\ECS). Formally, we have:
\begin{align}
\label{eqn:RG}
\red{\tfrac{1}{N}}\ln \int d\mu(\SMAT)e^{\ND \beta N \operatorname{Tr}[\HBARE]} \;\xrightarrow{ERG}\; \lim_{N\gg 1}\red{\tfrac{1}{N}}\ln \int d\mu(\AECS)e^{\ND \beta N \operatorname{Tr}[\HEFF]} 
\end{align}
where the \ERG transformation is defined by the \ScaleInvariant change of measure ,
applied in a \largeN expansion,
and where $\HEFF$ is defined implicitly through result for $\QT$ (Eqn.~\ref{eqn:QT_result}).
The result is, formally, a sum of the integrated \RTransforms $\GNI$.
In a sense, this result resembles (a non-perturbative form of) the Linked Cluster Theorem
in that the log \PartitionFunction is expressed as a sum of integrated matrix-generalized cumulants.  And in analogy with \SemiEmpirical theories of Quantum Chemistry, the \HTSR \ALPHA $(\alpha)$ and \ALPHAHAT $\ALPHAHATEQN)$ enter as renormalized empirical parameters.
Most importantly, the \ScaleInvariant \TRACELOG condition can be verified empirically (See Section~\ref{sxn:empirical}.)
Importantly, in analogy with the \WilsonExactRenormalizatonGroup theory, 
the \HTSR $\alpha=2$ resembles in spirit
an \ERG \emph{Universal Critical Exponent} at a phase boundary being between the \HeavyTailed 
(HT) and the \VeryHeavyTailed (VHT) phase of learning of the \HTSR theory.
This observation strengthens our argument that the \HTSR HT and VHT phases
are analogous to the generalizing and overfit phases, respectively,
of the classical \SMOG theories of NN learning.


