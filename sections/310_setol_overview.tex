\subsection{\SETOL Overview}
\label{sxn:setol_overview}

\charles{Some old text here I need to clean up}

Our \SETOL formulates a parametric expression for the \LayerQuality $\Q$ using a matrix-generalization of the classic \StudentTeacher (ST)
model from the \SMOG theory of the 1990s~\cite{SST92,STS90}, evaluated using
recent advances in the evaluation of so-called HCIZ random matrix integrals~\cite{potters_bouchaud_2020,Tanaka2007, Tanaka2008},
such that the final expression for $\Q$ can be written in terms of empirically measured statistical properties of the layer ESD.
We summarize our basic approach here; see Section~\ref{sxn:matgen} for a detailed derivation, and see Section~\ref{sxn:empirical} for a detailed empirical analysis.

Following the \StudentTeacher (ST) model~\cite{SST92}, 
we first formulate the \GeneralizationError $(\AVGE^{ST}_{gen})$ of the linear \Perceptron (in the \emph{\AnnealedApproximation},
and in the \emph{\HighTemperature} limit; see Section ~\ref{sxn:SMOG_main}), and 
we then generalize this to the case of a NN $(\AVGE^{ST}_{gen}\rightarrow\AVGE^{NN}_{gen})$, so that we can analyze the \Quality of each layer.
For the \Perceptron, the \GeneralizationError is an Energy, given as $\AVGE^{ST}_{gen}:=\THRMAVG{1-R}$, where $R$ is the ST vector overlap,
and $\THRMAVG{\cdots}$ is a \emph{\ThermalAverage} (defined in Section~\ref{sxn:mathP}),
a Boltzmann-weighted average.
In this case, the model \Quality, $\Q^{ST}$ is exactly the AA, high-T \AverageGeneralizationAccuracy 
\michaeladdressed{still not sure we are using that term correctly, here and elsewhere}
$\Q^{ST}:=1-\AVGE^{ST}_{gen}=\THRMAVG{R}$.
For an MLP or general NN, each layer Energy is associated with a
\LayerQuality $\Q$, which we identify as the average contribution an
individual layer makes to the overall generalized accuracy,
(i.e $1-\AVGE^{NN}_{gen}$) for a multilayer perceptron (MLP).

For technical reasons (below), we will seek the
\emph{\LayerQuality (Squared)} $\QT$, which is defined as the \ThermalAverage
of the matrix-generalized overlap ($\Trace{\OVERLAP^{2}}$),
\begin{align}
  \label{eqn:QT}
  \QT :=   \THRMAVGIZ{\Trace{\OVERLAP^{2}}}
\end{align}
where $\OVERLAP^{2}$ can be thought of as a \Hamiltonian for the \QualitySquared  $(\HBARE=\OVERLAP^{\top}\OVERLAP)$.

In \EQN~\ref{eqn:QT}, the so-called \emph{\Teacher} ($T$) is the NN model under consideration,
%where $\mathbf{T}$ and $\mathbf{S}$  are the \Teacher and \Student layer weight matrices, resp,
and 
%$\mathbf{R}$ 
$\mathbf{R}:=\frac{1}{N}{\mathbf{S}^{\TR}\mathbf{T}}$
denotes the ST overlap operator 
between the \Teacher layer weight matrix $\TMAT$ and a similar \emph{\Student} ($S$) layer weight matrix $\SMAT$.
The notation $\THRMAVGIZ{\cdots}$ denotes a \ThermalAverage over
all \Student weight matrices $\mathbf{S}$ that resemble the \Teacher weight matrix $\mathbf{T}$.
By ``resemble'', the \SETOL approach assumes that 
%$\mathbf{S}$ lies in the same \HTSR Universality class as the $\mathbf{T}$, and, even more, that 
the ESD of $\mathbf{S}$ has the same \emph{limiting} form as $\mathbf{T}$, placing them in the same \HTSR Universality class.
This is made more precise below.

Let us now express the average matrix-matrix overlap $\mathbf{R}$ in squared form using:
\begin{align}
  \label{eqn:R2}
\Trace{\OVERLAP^{2}} &:=\OLAPSQD  \\ \nonumber
&=\frac{1}{N^{2}}\Trace{\mathbf{T}^{\TR}\mathbf{SS}^{\TR}\mathbf{T}}
=\frac{1}{N}\Trace{\mathbf{T}^{\TR}\mathbf{A}_2\mathbf{T}}
\end{align}
where $\AMAT_{2}$ is the $N\times N$ form of the \Student correlation matrix,
$\mathbf{A}_2:=\frac{1}{N}\mathbf{SS}^{\TR}$.
We will also define the $M\times M$ matrix, $\AMAT_{1}=\frac{1}{N}\mathbf{S}^{\TR}\mathbf{S}$,
used later.

\michaeladdressed{Clarify.}
%It is argued that the \Student and \Teacher weight matrices (i.e., $\TMAT, \SMAT$) have
%specific generalizing eigencomponents that, after training,
%concentrate in a low rank subspace, called the \emph{\EffectiveCorrelationSpace} (\ECS)
%(and that this space satisfies the \TRACELOG condition, described below).
%This is both consistent with the empirical observations and necessary to apply the HCIZ integral
%methods, both described below.
\michaeladdressed{MM TO DO: Dense.  Where is the best place for this par.}

As explained in Section~\ref{sxn:mathP}, this \QualitySquared is more readily obtained as 
the derivative of the \LayerQualitySquared \GeneratingFunction, $\IZG$, defined as
\begin{align}
  \label{eqn:IZG_QT}
  \QT := \dfrac{1}{\beta}\dfrac{\partial }{\partial N} \lim_{N\gg 1}\IZG
\end{align}
$\IZG$ is essentially ($\beta$ times) a \emph{\FreeEnergy} for the (approximate) \LayerQualitySquared.
(see Section~\ref{sxn:SMOG_main}, and the Appendix, Section~\ref{sxn:quality}).
For more details, see Section~\ref{sxn:matgen}, and the Appendix, Section~\ref{sxn:summary_sst92}).

We can write $\IZG$ as an HCIZ Integral, 
\begin{align}
  \label{eqn:QT_dS}
  \IZG  &=  \ln\int d\mu(\mathbf{S})\exp\left(N\beta\Trace{\mathbf{T}^{\TR}\mathbf{A}_2\mathbf{T}}\right),
\end{align}



The \SETOL approach then seeks to express $\IZG$ in~\EQN~\ref{eqn:QT_dS} as an HCIZ integral (and in the large-N limit)~\cite{potters_bouchaud_2020,Tanaka2007,Tanaka2008}.
We evaluate this at large-$N$, and write
\begin{equation}
  \IZGINF:=\lim_{N\gg 1}\IZG
\end{equation}
%%%must restrict the matrices to a lower rank subspace, called the~\ECS,
%%%i.e.,  $\mathbf{\tilde{A}}$. 
%%%We also must change the measure from an integral over \Student weight matrices
%%%to an integral over \Student Correlation matrices, i.e., 
%%%\begin{align}
%%%  \label{eqn:change_measure}
%%%  d\mu(\mathbf{S})\rightarrow d\mu(\mathbf{\tilde{A}}) ,
%%%\end{align}
%%%We can define a single measure for either form of \Student correlation matrix, restricted to the~\ECS,
%%%such that 
%%%$d\mu(\mathbf{\tilde{A}})=d\mu(\mathbf{\tilde{A}}_1)=d\mu(\mathbf{\tilde{A}}_2)$.
%%%This will be important as we need the form $\AMAT_1$ to derive the \TRACELOG condition, below, but
%%%we  formulate the  $\IZG$ using the form $\AMAT_2$.
%%%%Also, and importantly, the change in measure from $d\mu(\mathbf{S})$ to $d\mu(\mathbf{\tilder{A}})$
%is needed in order to apply the HCIZ  integral.
\michaeladdressed{I feel like expositionally it may be good to highlight this in the intro also and to have a bullet highlighting the rule from \HTSR for selecting the generalizing subspace; that will make it easier in the next subsection to highlight how the two connect.}
\charles{Lets discuss.  The intro has some of this. and a lot more}
With these definitions in place, moving forward, the following key assumptions, which can be tested empirically, must hold:
\begin{itemize}
  \item
  \textbf{The Effective Correlation Space (\ECS) Condition.}
  The generalizing components of the \Student (and \Teacher) layer weight matrices concentrate into a lower rank subspace---the~\ECS---spanned by the
  eigenvectors associated with the (heavy) tail of the layer ESD $\rho_{tail}(\lambda)$, such that the test error can 
  be reproduced with only these components. 
  \michael{MM TO DO: we need to define tail better somewhere, but see also my comment above about presenting the two generalizing subspaces better.}
  We write $\AECS$ to denote the projection of the correlation
  matrix $\AECS:=\mathbf{P}_{ecs}\AMAT$, onto this subspace, now with rank $\MECS\ll M$.
  This restricts the measure $d\mu(\AMAT)$ to the~\ECS, $(d\mu(\AMAT)\rightarrow d\mu(\AECS))$.
  This assumption will empirically examined using real-world \Teacher weight matrices $\TMAT=\WMAT$
  in Section~\ref{sxn:empirical-effective_corr_space}. 
%  Importantly, this lets interpret the \EQN~\ref{eqn:QT_HCIZ}  as acting only in this subspace.
  \item
  \textbf{The \TRACELOG Condition.}
  The \Student correlation matrix $\AECS_1$ (when properly normalized)
  satisfies the condition that $\Trace{\ln\AECS_1}=\ln\Det{\AECS_1}=0$,
  so that the change of measure $d\mu(\SMAT) \rightarrow d\mu(\AECS) $ is Volume Preserving.
  This condition is derived explicitly in terms of $\AECS_1$, and therefore will hold for $\AECS_2$
  (and the Teacher Correlation matrix, $\XECS=\frac{1}{N}\TMAT^{\TR}\TMAT$).
  Practically, this implies that the $\MECS$ eigenvalues $\LambdaECS$
  of the tail of the ESD must satisfy $\sum_{i=1}^{\MECS}\ln\LambdaECS_{i}\approx 0$.
  Experiments will test this assumption explicitly in Section~\ref{sxn:empirical-trace_log}.
\end{itemize}
Remarkably, both conditions hold best empirically when the \HTSR PL quality metric $\alpha\gtrsim 2$ is \Ideal. Motivated from these empirical observations, we have:
\begin{itemize}
  \item
  \textbf{$\IZGINF$ is expressed as an HCIZ integral, at large-$N$.}
  We have
  \begin{align}
  \label{eqn:IZGINF_HCIZ}
  \IZGINF = \lim_{N\gg 1}\ln\int d\mu(\AECS)\exp\left(\beta\Trace{\mathbf{T}^{\TR}\AECS_2\mathbf{T}}\right) %= \beta\sum_{i=1}^{\MECS}\GNECS
  \end{align}
  where  measure $d\mu(\AECS)$ lets us average over all \Student Correlation matrices $\AECS_{2}$ which
  lie in the~\ECS space and which ``resemble'' the \Teacher, 
  where by ``resemble'' we mean that they share the same form of the tail of
  their limiting ESDs,
  i.e., $\rho^{\infty}_{\AECS}(\lambda)\sim\rho^{\infty}_{\XECS}(\lambda)$.
  \item
  \textbf{The Layer Quality (Squared) $\QT$ is a~\GEN.}
  \michael{We need to figure out what to call that, I think norm isn't right, is it a generating function.}
  \charles{OK go ahead.}
  The final expression for $\QT$ can be written as the derivative of $\IZGINF$  as
  \begin{align}
    \label{eqn:QT_result}
    \QT = \dfrac{1}{\beta}\dfrac{\partial}{\partial N}\IZGINF = \sum_{i=1}^{\MECS}\GNI
  \end{align}
  where $\GNI$ is a \emph{\GEN}, and is  defined as the integrated \emph{\RTransform} $R(z)$ of the \Teacher
  layer ESD (where $z\in\mathbb{C}$), such that $\GN:= \int_{\LambdaECSmin}^{\lambda}R(z)dz$
  and $\LambdaECSmin$  encapsulates  the~\ECS (and selects the desired branch-cut of $R(z)$
  so that it is both single-valued and analytic).
\end{itemize}


\michael{MM TO DO: Somewhere we need to distinguish between \SemiEmpirical in general and our specific \SETOL approach.}
%\charles{TRY THIS: These two key, testable assumptions allow \SETOL to move beyond the \HTSR \Phenomenology to derive a more rigorous expression of model \GeneralizationError. It does so by leveraging spectral properties of weight matrices that permit deeper characterization of ways in which weight \emph{matrices} can be similar -- or \ATypical -- which is something parameter \emph{vectors} cannot.}
%\michael{No, I understand that.  What I am asking is: how do we want to describe the difference between \SemiEmpirical and \SETOL?  I think \SETOL is our particular model and setup; but \SemiEmpirical is a more general approach (meaning, in particular, it may not need a bolded name here).  }
%\charles{OK, well we have a section (140) where we discuss nuclear physics; what about there ?}

To apply the theory, one must choose an \RTransform $R(z)$ for	the \Teacher that models 
the tail of the ESD $\rho^{emp}_{T}(\lambda)$, and that can be
parameterized by some measurable property.
This may include the number of Spikes $\lambda^{spike}$, the fitted PL exponent $\alpha$,
the maximum eigenvalue $\lambda_{max}$, or even the entire tail $\rho^{tail}_{T}(\lambda)$.
This may be a formal expression, a computational procedure, or some combination.

To integrate $R(x)$, however, to have a physicaly meaningful result,
one must ensure that $R(z)$ is both
analytic and single-valued on domain of interest, namely, the \ECS (and therefore
the (PL) tail of the ESD),  $z \ge \LambdaECSmin$.
Because the ESD is frequently \HeavyTailed (HT), this
\RTransform $R(z)$ may have a branch-cut, and it is expected that this will occur
at $z\le \LambdaECSmin$, corresponding roughly at or before the start of the \ECS.
In a sense, selecting the branch-cut $R(z)$ forces one to define the \ECS.

To complete the theopry, we
will also show that the \HTSR PL \LayerQuality metrics \ALPHA $(\alpha)$  and \ALPHAHAT $(\ALPHAHATEQN)$
can be formally derived directly from the \SETOL \LayerQuality $\Q$ by selecting the appropriate
\RTransform $R(z)$. In Section~\ref{sxn:r_transforms} we provide several possible
model $R(z)$ and the resulting \LayerQuality $\Q$.
\michaeladdressed{Huh?}

\paragraph{Renormalization Group Effective Hamiltonian}
The formulation of SETOL closely parallels the construction of an \EffectiveHamiltonian~$\HEFF$
via the \WilsonExactRenormalizationGroup~(RG) approach. Consider a \emph{\Bare} Hamiltonian $\HBARE$ for the \LayerQualitySquared,
defined as $\HBARE:= \mathbf{R}^{\TR}\mathbf{R}$.
We can express Eqn.\ref{eqn:QT_dS} in terms of this \Bare Hamiltonian $\HBARE$,
and rewrite Eqn.\ref{eqn:IZGINF_HCIZ} in terms of an \emph{\Renormalized} \EffectiveHamiltonian~$\HEFF$
that spans the \EffectiveCorrelationSpace~(\ECS). Formally, we have:
\begin{align}
\label{eqn:RG}
\ln \int d\mu(\SMAT)e^{N\beta \operatorname{Tr}[\HBARE]} \;\xrightarrow{RG}\; \lim_{N\gg 1}\ln \int d\mu(\AECS)e^{N\beta \operatorname{Tr}[\HEFF]} 
\end{align}
where the RG transformation is defined by the \ScaleInvariant \TRACELOG condition,
applied at large-$N$,
and where $\HEFF$ is defined implicitly through result for $\QT$ (Eqn.~\ref{eqn:QT_result}).
The result is, formally, a sum of the integrated \RTransforms $\GNI$.
In a sense, this result resembles (a non-perturbative form of) the Linked Cluster Theorem
in that the log \PartitionFunction is expressed as a sum of integrated matrix-generalized cumulants and/or self-energy.  And in analogy with \SemiEmpirical theories of Quantum Chemistry, the \HTSR \ALPHA $(\alpha)$ and \ALPHAHAT $\ALPHAHATEQN)$ enter as renormalized empirical parameters.
Most importantly, the \ScaleInvariant \TRACELOG condition can be verified empirically (See Section~\ref{sxn:empirical}.  
Importantly, in analogy with the Wilson RG theory, 
the \HTSR $\alpha=2$ resembles in spirit
an RG \emph{Universal Critical Exponent} at a phase boundary being between the \HeavyTailed 
(HT) and the \VeryHeavyTailed (VHT) phase of learning of the \HTSR theory.
This observation strengthens our argument that the \HTSR HT and VHT phases
are analogous to the generalizing and overfit phases, resp.,
of the classical \SMOG theories of NN learning.


