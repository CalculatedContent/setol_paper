\subsection{Heavy-Tailed Self-Regularization (\HTSR)}

\HTSR theory is an approach that combines ideas from \STATMECH~with those of \emph{\HeavyTailed} \emph{\RandomMatrixTheory} (\RMT),
providing eigenvalue-based quality metrics that correlate with model quality (i.e., out-of-sample performance).
\HTSR theory posits that well-trained models have extracted subtle correlations from the training data, and that these correlations manifest themselves in the \SHAPE and \SCALE of the eigenvalues of the layer weight matrices $\mathbf{W}$. 
In particular, if one computes the empirical distribution of the eigenvalues, $\lambda_i$, of an individual  $N \times M$ weight matrix, $\mathbf{W}$, then this density, $\rho^{emp}(\lambda)$, which is an ESD, is \HeavyTailed (HT) and can be well-fit to a \emph{\PowerLaw} (PL), i.e., $\rho(\lambda)\sim\lambda^{-\alpha}$, with exponent $\alpha$.
\HTSR~theory provides a \emph{\Phenomenology} for qualitatively-distinct phases of learning~\cite{MM18_TR_JMLRversion}.
It can, however, also be used to define \emph{Layer-level Quality metrics} and \emph{Model-level Quality metrics}: e.g., the \ALPHA~$(\alpha)$ and \ALPHAHAT~$(\ALPHAHATEQN)$ PL metrics, described below.

Not needing any training data, \HTSR~theory has many practical uses.
It can be directly applicable to large, open-source models where the training and test data may not be available.
Model quality metrics can be used, e.g., to predict trends in the quality of SOTA models in computer vision (CV)~\cite{MM20a_trends_NatComm} and natural language processing (NLP)~\cite{YTHx22_TR,YTHx23_KDD}, both during and after training, and without needing access to the model test or training data.
Layer quality metrics can be used to diagnose potential internal problems in a given model, or (say) to accelerate training by providing optimal layer-wise learning rates \cite{NEURIPS2023_CHM} or pruning rations \cite{alphapruning_NEURIPS2024}.
%
Most notably, the \HTSR theory provides \emph{Universal} \LayerQuality metrics encapsulated in what appears to be a critical exponent, $\alpha=2$, that is empirically associated with optimal or \IdealLearning. Moreover, as argued below, the
value $\alpha=2$ appears to define a phase boundary between a generalization and overfitting, analogous
to the phase boundaries seen in \STATMECH theories of NN learning. 

These results both motivate the search for a first principles understanding of the \HTSR theory, 
and suggest a path for developing a practical predictive theory of Deep Learning.
For this, however, we need to go beyond the \Phenomenology provided by \HTSR theory, to relate it to some sort of (at least semi-rigorous/semi-empirical) derivations based on the \STATMECH theory of learning, and drawing
upon previous success (in Quantum Chemistry) in developing a first principles \SemiEmpirical theory. 

