%\subsection{Traditional Approaches to the \SMOG} 
\subsection{\STATMECH: the \SMOG approach and the \SETOL approach} 

\label{sxn:trad_smog}

In this subsection, we review the basic \STATMECH setup necessary to understand \SMOG theory as well as \SETOL.
This theory was developed in the 1980s and 1990s~\cite{SST90,SST92,Gar85,Gar88,engel2001statistical}. 


\begin{table}[t] %[h!]
\centering
%\begin{center}
\renewcommand{\arraystretch}{1.15} % Increases row height
\begin{tabular}{c|c}
  \textbf{Statistical Physics} & \textbf{Neural Network Learning}                      \\ \hline
  Gaussian field variables     & Gaussian i.i.d data  $\NDXI\in\mathcal{D}$            \\ \hline
  State Configuration          & Trained / Learned weights $\WVEC$                     \\ \hline
  State Energy Difference      & Training and Generalization Errors  $\AVGTE, \AVGGE$  \\ \hline
  Temperature                  & Amount of stochasticity present during training $T$       \\ \hline
  \AnnealedApproximation       & Average over the data first                          \\ \hline
  \ThermalAverage              & Expectation w.r.t. the distribution of trained models \\ \hline
  \FreeEnergy                  & Generating function for the error(s) $F$             \\ \hline
\end{tabular}
%\end{center}
\caption{Mapping between states and energies of a physical system and parameters of the learning process of a neural network.}
\label{table:statMech_to_NNs}
\end{table}


\begin{table}[t] %[h!]
\centering
%\begin{center}
\renewcommand{\arraystretch}{1.15} % Increases row height
\begin{tabular}{c|c}
  \textbf{SETOL Terminology} & \textbf{Explanation}                      \\ \hline
  \ModelQuality $\Q$          & \makecell{Generalization accuracy, \\in the AA and at high-T }      \\ \hline
  \LayerQuality $\Q$          & \makecell{Layer contribution to the accuracy, \\in the AA and at high-T}       \\ \hline
  \LayerQualitySquared $\QT$ &  \LayerQuality squared, used for technical reasons \\ \hline
  \Quality \GeneratingFunction $\Gamma_{\Q}, \Gamma_{\QT}$   & Generating function for quality    \\ \hline
  \AnnealedHamiltonian $H^{an}$                & \makecell{Energy function, \\for errors or accuracies}             \\ \hline
  \EffectiveHamiltonian $H^{eff}$     & \makecell{Exact energy function, \\but restriced to a low-rank subspace}      \\ \hline
\end{tabular}
%\end{center}
\caption{Additional terminology introduced for the \SETOL.  
  Notice that the \Quality \GeneratingFunction $\Gamma$ is simply one minus the \FreeEnergy, $\Gamma:=1-F$,
but it introduced because sign convention for the \FreeEnergy is always decreasing with the error.
  In contrast, we define the Hamiltonian in terms of the model error or accuracy, depending on the context.
  \michael{AWK.}
  \michaelB{Do we ref this table anywhere.}
  }
\label{table:SETOL_terminology}
\end{table}


\paragraph{Traditional \SMOG theory.}
In traditional \SMOG theory, one maps the learning process of a NN to the states and energies of a physical system.
The mapping is given in Table~\ref{table:statMech_to_NNs}.
\SMOG theory models the SGD training of a \emph{\Perceptron}, on the data, $\NDX$, to learn the optimal weights, $\WVEC$, as a Langevin process.%
\footnote{Typically, we have no guarantee of the true equilibrium in a high‚Äêdim nonconvex landscape; so,
when the \emph{\ThermodynamicLimit} exists,
the  Langevin process converges or relaxes to the thermodynamic equilibrium.}
The power of the \STATMECH approach comes from the fact that the core concept of \ThermalAverages corresponds to taking the expectation of a given quantity only \emph{over the set of trained models}, as opposed to uniformly over all possible models (or, in a worst-case sense, over all possible models in a model class).
This capability is particularly compelling in light of the \STATMECH capacity to characterize disordered 
systems with complex non-convex energy landscape (which can even be \emph{\Glassy}, characterized by a
highly non-convex landscape~\cite{SST92, STS90, engel2001statistical},
and recognized classically by a slowing down of the training dynamics~\cite{gutfreund1985spin}).
Thus, concepts such as training and \GeneralizationError arise naturally from integrals that are familiar to \STATMECH; 
and theoretical quantities such as Load, Temperature, and \FreeEnergy also map onto useful and relevant concepts~\cite{MM17_TR}. 
The \FreeEnergy is of particular interest because it can be used as a generating function to obtain the desired \GeneralizationError and/or accuracy.
We wish to understand how to compute the associated thermodynamic quantities such as the expected value of the
various forms of the \AverageGeneralizationError $(\AVGGE)$, \PartitionFunction $(Z)$, and the
\FreeEnergy $(F)$ and other \GeneratingFunctions~$(\Gamma)$.


\charles{TODO: discuss mean field theory, saddle points, replicas, etc.} 
\michael{Certainly not~here.}
\charles{A good introduction discusses the themes in a section}
\michaeladdressed{@charles: Can we just cange ``Generalization Accuracy'' to ``Model Quality'' and ``Model Accuracy'' since I think that will help to minimize confusion.  We should define what precisely is Model Quality and new notions.}

\paragraph{The \StudentTeacher model.}
In the basic student-teacher model, one considers, e.g., the classification of the elements of some input space $X$.
There is a target rule, $T$, which is one particular mapping of the input space into the set of classes, and a hypothesis space, $\mathcal{F}$, which consists of the available mappings $f$ used to approximate $T$.
Given this setup, the basic problem is the following: on the basis of the classification determined by $T$ for the elements of a subset $\mathcal{X} \subset X$ (the ``training set''), select an element of $\mathcal{F}$ and evaluate how well that element approximates $T$ on the complete input space $X$.
One can think of the target rule $T$ as being a \emph{teacher}, and the goal of the \emph{student} is to approximate the teacher as well as possible.
One popular metric, the generalization error, is the probability of disagreement between the student/hypothesis and teacher/target on a randomly chosen subset of $X$; and 
one popular analysis approach is the probably approximately correct (PAC) framework~\cite{Val84}, where the problem of deciding, on the basis of a small training set, which hypothesis will perform well on the complete input is closely related to the statistical problem of convergence of frequencies to probabilities~\cite{Vapnik98}.
A different analysis approach is based on statistical mechanics (\STATMECH)~\cite{SST92,WRB93,Kea95,DKST96,EB01_BOOK}.

Our approach is closer to \STATMECH; but due to the \SemiEmpirical nature of \SETOL, there are important differences from the usual \SMOG approach.
First, we will seek to estimatie the  \AverageGeneralizationAccuracy (with a metric we call the ST \ModelQuality, which we can denote $\Q^{ST}$), rather than the more traditional generalization error.
Second, we will do so for a \emph{fixed} \Teacher Perceptron $T$ by averaging over an ensemble of \Student $S$ \Perceptrons, in (what is \SMOG is called) the \AnnealedApproximation (AA) and at \HighTemperature (high-T), rather than averaging over all teacher vectors $\TVEC$~\cite{SST92,engel2001statistical}.
Third, rather than modeling students and the teacher as vectors, we will model them as matrices.
Thus, we will generalize $\Q^{ST}$ to an
arbitrary layer in \MultiLayerPerceptron, giving a \LayerQuality (i.e., $\Q^{ST}\rightarrow\Q$).
Because of this, we present both a pedagogic derivation of $\Q^{ST}$: for a general NN in Section~\ref{sxn:mathP}; and for the ST model specifically in Appendix~\ref{sxn:summary_sst92} and (using a more heuristic approach) in Section~\ref{sxn:SMOG_main-st_av} (assuming the AA and high-T at all times).


\paragraph{Towards a \SemiEmpirical Theory.}
In the \SETOL approach to \STATMECH, we want a matrix generalization of the ST \ModelQuality, $\Q^{ST}$, for a single \LayerQuality
$\Q\sim\Q^{NN}_{L}$ in an arbitrary \MultiLayerPerceptron (MLP).
This matrix generalization is a relatively straightforward extension of the classical (i.e., for a vector \Teacher) \SMOG ST \ModelQuality (but our \SETOL approach will use it in a conceptually new way).
%
In our matrix generalization, the \Teacher vector $\TVEC$ is replaced by a \Teacher matrix $\TMAT$ (i.e., $\TVEC\rightarrow\TMAT$); 
and, in our \SETOL approach, $\TMAT$ will be an actual (pre-)trained NN weight matrix (i.e., $\TMAT=\WMAT$). 
Importantly, this matrix $\WMAT$ is neither a Gaussian Random Matrix, nor is it obtained from Gaussian i.i.d training data.
As such, for our \SETOL theory, we seek an expression that can be parameterized by the \Teacher, and in particular by the ESD of the \Teacher.
This is what makes the basic method \SemiEmpirical: even though we do not know the form of the \Teacher,
we make a modeling assumption that the  \Teacher has the same limiting spectral distribution as the \Student, 
and hence the same PL fit parameter $\alpha$. 
This is all done with the understanding that later we will augment
(and hopefully ``correct'') our mathematical formulations with 
phenomenological parameters fit from experimental data.
To make the \SemiEmpirical method a \SemiEmpirical \emph{Theory}, 
we not only seek to parameterize our model; but 
we also try to understand 
how to derive \HTSR empirical metrics, such as $\ALPHA$ and $\ALPHAHAT$,
how they arise from this formalism, 
how they are related to the correlations in the data, and 
why they are transferable and exhibit \Universality.
This gives what we call a \SemiEmpirical \emph{Theory}.
