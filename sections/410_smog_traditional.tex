\subsection{Traditional Approaches to the \SMOG} 
\label{sxn:trad_smog}

In this subsection, we review the basic \STATMECH setup necessary to understand the \SMOG theory.
This theory was developed in the 1980s and 1990s, and it is described in~\cite{SST90,SST92,Gar85,Gar88,engel2001statistical} and elsewhere. 
The \SMOG theory models the SGD training of a \emph{\Perceptron}, 
\michaeladdressed{@charles: that macro is plural, it should be simgular with an apostrophe, or have an apostrophe}
on data $\NDX$, to learn the optimal weights $\WVEC$,
as a Langevin process, % and which is treated
Typically in modern ML, we have no guarantee of true equilibrium in a high‚Äêdim nonconvex landscape, so
when the \emph{\ThermodynamicLimit} exists,
it converges or relaxes to a thermodynamic equilibrium.
We wish to understand how to compute the associated thermodynamic quantities such as the expected value of the
various forms of the \AverageGeneralizationError $(\AVGGE)$, \PartitionFunction $(Z)$, and the
\FreeEnergy $(F)$ and other \GeneratingFunctions~$(\Gamma)$.

\begin{table}[t] %[h!]
\centering
%\begin{center}
\renewcommand{\arraystretch}{1.15} % Increases row height
\begin{tabular}{c|c}
  \textbf{Statistical Physics} & \textbf{Neural Network Learning}                      \\ \hline
  Gaussian field variables     & Gaussian i.i.d data  $\NDXI\in\mathcal{D}$            \\ \hline
  State Configuration          & Trained / Learned weights $\WVEC$                     \\ \hline
  State Energy Difference      & Training and Generalization Errors  $\AVGTE, \AVGGE$  \\ \hline
  Temperature                  & Amount of stochasticity present during training $T$       \\ \hline
  \AnnealedApproximation       & Average over the data first                          \\ \hline
  \ThermalAverage              & Expectation w.r.t. the distribution of trained models \\ \hline
  \FreeEnergy                  & Generating function for the error(s) $F$             \\ \hline
\end{tabular}
%\end{center}
\caption{Mapping between states and energies of a physical system and parameters of the learning process of a neural network.}
\label{table:statMech_to_NNs}
\end{table}


\begin{table}[t] %[h!]
\centering
%\begin{center}
\renewcommand{\arraystretch}{1.15} % Increases row height
\begin{tabular}{c|c}
  \textbf{SETOL Terminology} & \textbf{Explanation}                      \\ \hline
  \ModelQuality $\Q$          & Generalization accuracy, in the AA and at high-T       \\ \hline
  \LayerQuality $\Q$          & Layer contribution to the accuracy, in the AA and at high-T       \\ \hline
  \GeneratingFunction $\Gamma_{\Q}, \Gamma_{\QT}$   & Generating function for quality    \\ \hline
  \QualitySquared $\QT$ &  \LayerQuality squared, used for technical reasons \\ \hline
  \Quality \GeneratingFunction $\Gamma_{\Q}, \Gamma_{\QT}$   & Generating function for quality    \\ \hline
  \AnnealedHamiltonian $H^{an}$                & Energy function. For errors or accuracies             \\ \hline
  \EffectiveHamiltonian $H^{eff}$     & Exact energy function, but restriced to a low-rank subspace      \\ \hline
\end{tabular}
%\end{center}
\caption{Additional terminology introduced for the SETOL.  
  Notice that the \Quality \GeneratingFunction $\Gamma$ is simply one minus the \FreeEnergy, $\Gamma:=1-F$,
but it introduced because sign convention for the \FreeEnergy is always decreasing with the error.
  In contrast, we define the Hamiltonian in terms of the model error or accuracy, depending on the context.}
\label{table:SETOL_terminology}
\end{table}


\paragraph{Traditional \SMOG theory.}
In traditional \SMOG theory, one maps the learning process of a NN to the states and energies of a physical system.
The mapping is given in Table~\ref{table:statMech_to_NNs}.
%%
%%\noindent
The power of the \STATMECH approach comes from the fact that the core concept of \ThermalAverages equates to taking 
the expectation of a given quantity only \emph{over the set of trained models}, as opposed to 
%MM%the uniform space of parameter vectors. 
uniformly over all possible models (or in a worst-case sense, over all possible models in a model class).
%%\michael{@charles: confirm that change is okay.}
%%\cformike{Not sure.  The \ThermalAverage refers to the average over all possible equilibirum solutions,
%%  so in this sense is like taking the \ThermodynamicLimit.  
%%  Importantly,  The \GeneralizationError is obtained by taking a derivative w/r.t. the N dependence.  Notice that unlike VC theory,
%%  the N dependence is retained in the \ThermodynamicLimit because of how the limit is taken}
This capability is particularly compelling in light of the \STATMECH capacity to characterize disordered 
systems with complex non-convex energy landscape (which can even be \emph{\Glassy}, characterized by a
highly non-convex landscape~\cite{SST92, STS90, engel2001statistical},
and recognized classically by a slowing down of the training dynamics~\cite{gutfreund1985spin}).
Thus, concepts such as training and \GeneralizationError arise naturally from integrals that are familiar to \STATMECH; 
and theoretical quantities such as Load, Temperature, and \FreeEnergy also map onto useful and relevant concepts~\cite{MM17_TR}. 
The \FreeEnergy is of particular interest, as it can be used as a generating function to obtain the desired \GeneralizationError and/or accuracy.

\charles{TODO: discuss mean field theory, saddle points, replicas, etc.} 
\michael{Certainly not~here.}
\charles{A good introduction discusses the themes in a section}
\michaeladdressed{@charles: Can we just cange ``Generalization Accuracy'' to ``Model Quality'' and ``Model Accuracy'' since I think that will help to minimize confusion.  We should define what precisely is Model Quality and new notions.}

\paragraph{The \StudentTeacher model.}
%To express these integrals, the \SMOG approach models both Students $S$ and Teachers $T$ as \emph{weight vectors},
We seek to compute and/or estimate the  \AverageGeneralizationAccuracy for a \emph{fixed} \Teacher Perceptron $T$
by averaging over an ensemble of \Student $S$ \Perceptrons, in the \AnnealedApproximation (AA), and at
\HighTemperature (high-T); we call this ST \ModelQuality, and denote it $\Q^{ST}$.
We will then, later, in Section~\ref{sxn:matgen}, we generalize $\Q^{ST}$ to an
arbitrary layer in \MultiLayerPerceptron, giving a \LayerQuality, i.e., $\Q^{ST}\rightarrow\Q$.
This formulation of the ST problem is different than the classic approach  because one
usually does not fix the \Teacher but, instead,
averages over all \Teacher vectors $\TVEC$~\cite{SST92,engel2001statistical}.
This one of many ways that distinguishes the current approach from previous work.
Because of this, we present both a pedagogic derivation of $\Q^{ST}$
(for a general NN in Section~\ref{sxn:mathP}, and for the ST model specifically
in the Appendix, Section~\ref{sxn:summary_sst92}),
and we provide a more heuristic approach in Section~\ref{sxn:SMOG_main-st_av}, assuming
the AA and high-T at all times.

\michaeladdressed{MM: Remove this par, after making sure all the text is incorporated elsewhere.}
\paragraph{SubSection Roadmap}
Briefly, in the following subsection,
we start by defining an arbitrary NN model, with weights $(\WVEC)$
Then, we explain the difference between using real-world $(\XVEC)$ and random $(\XI)$ data
This lets us define an energy error function, $\DETOP$,
the error the NN makes on the data.
We then explain how to take different kinds of \emph{\Thermodynamic} averages of the data,
including \emph{Sample} and \emph{\ThermalAverages} and the implications,
and the difference between computing errors and accuracies.
Next, we define the \emph{FreeEnergy} $(F)$ for the error(s), and the \emph{GeneratingFuncton}  $(\Gamma)$
for the accuracy and/or quality.
From here, we explain the \emph{AnnealedApproximation} (AA) and
how to define the  \emph{\AnnealedHamiltonian}, $\GAN$, a crucial expression
that will be the starting point later for our matrix model.
In the AA, $\GAN$ simplies to  $\HANHT=\EPSLw$, where $\EPSLw$ is an \EffectivePotential
that depends only on the weights $\WVEC$.
Likewise, we can define the \SelfOverlap, $\ETAw:=1-\EPSLw$, which is useful for
obtaining the \Quality.
We show how to obtain the \emph{Average Training and Generalization Errors} $\AVGTE$, $\AVGGE$
using the \STATMECH formalism, which defines them in terms of partial derivatives of the \FreeEnergy $(F)$.
Doing this, we show that in the AA and at high-T they are equivalent,
$[\AVGSTTE]^{an,hT}=[\AVGSTGE]^{an,hT}$,
and can both be expressed as a \ThermalAverage over all Students, as a function 
of the \Teacher, as $[\AVGSTGE]^{an,hT}=\THRMAVG{\GANHTR}=\THRMAVG{\EPSL(R)}$.
Note that these averages are obtained by using the \FreeEnergy as a \GeneratingFunction.
We then explain how to obtain the \ModelQuality  as  partial derivatives of a
\emph{\GeneratingFunction} $(\Gamma_{\Q})$.
We then discuss the more advanced techniques, the
\emph{Large-$N$ Approxmation} and the \emph{SaddlePointApproximation} (SPA),
which will be used extensively later.
Finally, we introduce  HCIZ integrals, which will be necessary to evaluate the matrix-generalized
form of $\Gamma_{\Q}$ to obtain the final result.
%\footnote{This assumes that agreement of \Student and \Teacher predictions will follow from overlap of weight vectors --- as it 
%surely does in the \emph{\LinearPerceptron} case, and does with high probability under Lipschitz 
%smoothness~\cite{neyshabur18_TR}. 
%\michaeladdressed{@charles: Im not sure what this is saying.}\charles{YOU WROTE THIS!}
%}
\michael{@charles: What precisely is Accuracy versus Quality?}
\charles{@michael: We have discusses this multiple times. }
%We also degfine  write an expression for the Average \GeneralizationAccuracy,
%$1-\AVGSTGE$, which we call the \ModelQuality.
\michaeladdressed{This last sentence is an exaple of one where the phrasing is a little unclear.  I think we can mention \ModelQuality, but it's not clear if this is from SST or something we introduce, etc.  So, it's unclear later when we use it without a clear definition, etc., in the vector or matrix case.  Probably it's best just to either remove a sentence like that here (if SST did not have it, since here we are describing the past), or have a footnote saying that we will use the \GeneralizationAccuracy they used, but in a different way by considering one-minus it, and then have an explicit pointer to where we define it.}



\paragraph{Towards a \SemiEmpirical Theory.}
In the \SETOL approach to \STATMECH, we want a matrix generalization of the ST \ModelQuality, $\Q^{ST}$, for a single \LayerQuality
$\Q\sim\Q^{NN}_{L}$ in an arbitrary \MultiLayerPerceptron (MLP).
This matrix generalization is a relatively straightforward extension of the classical (i.e., for a vector \Teacher) \SMOG ST \ModelQuality (but our \SETOL approach will use it in a conceptually new way).
%
In our matrix generalization, the \Teacher vector $\TVEC$ is replaced by a \Teacher matrix $\TMAT$ (i.e., $\TVEC\rightarrow\TMAT$); 
and, in our \SETOL approach, $\TMAT$ will be an actual (pre-)trained NN weight matrix (i.e., $\TMAT=\WMAT$). 
Importantly, this matrix $\WMAT$ is neither a Gaussian Random Matrix, nor is it obtained from Gaussian i.i.d training data.
As such, for our \SETOL theory, we seek an expression that can be parameterized by the \Teacher, and in particular by the ESD of the \Teacher.
This is what makes the basic method \SemiEmpirical: even though we do not know the form of the \Teacher,
we make a modeling assumption that the  \Teacher has the same limiting spectral distribution as the \Student, 
and hence the same PL fit parameter $\alpha$. 
This is all done with the understanding that later we will augment
(and hopefully ``correct'') our mathematical formulations with 
phenomenological parameters fit from experimental data.
To make the \SemiEmpirical method a \SemiEmpirical \emph{Theory}, 
we not only seek to parameterize our model; but 
we also try to understand 
how to derive \HTSR empirical metrics, such as $\ALPHA$ and $\ALPHAHAT$,
how they arise from this formalism, 
how they are related to the correlatons in the data, and 
why they are transferable and exhibit \Universality.
This gives what we call a \SemiEmpirical \emph{Theory}.
