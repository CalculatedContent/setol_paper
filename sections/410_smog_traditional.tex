%\subsection{Traditional Approaches to the \SMOG} 
\subsection{\STATMECH: the \SMOG approach and the \SETOL approach} 

\label{sxn:trad_smog}

In this subsection, we review the basic \STATMECH setup necessary to understand \SMOG theory as well as \SETOL.
This theory was developed in the 1980s and 1990s~\cite{SST90,SST92,Gardner_1985,Gardner_1988,engel2001statistical,EB01_BOOK}. 


\begin{table}[t] %[h!]
\centering
%\begin{center}
\renewcommand{\arraystretch}{1.15} % Increases row height
\begin{tabular}{c|c}
  \textbf{Statistical Physics} & \textbf{Neural Network Learning}                      \\ \hline
  Gaussian field variables     & Gaussian i.i.d (idealized) data  $\NDXI\in\mathcal{D}$            \\ \hline
  State Configuration          & Trained / Learned weights $\WVEC$                     \\ \hline
  State Energy Difference      & Training and Generalization Errors  $\AVGTE, \AVGGE$  \\ \hline
  Temperature                  & Amount of regularization present during training $T$       \\ \hline
  \AnnealedApproximation       & Average over data $\NDXI$ first, then weights $\WVEC$.                          \\ \hline
  \ThermalAverage              & Expectation w.r.t. the distribution of trained models \\ \hline
  \FreeEnergy                  & Generating function for the error(s) $F$             \\ \hline
\end{tabular}
%\end{center}
\caption{Mapping between states and energies of a physical system and parameters of the learning process of a neural network.}
\label{table:statMech_to_NNs}
\end{table}


\begin{table}[t] %[h!]
\centering
\renewcommand{\arraystretch}{1.15} % Increases row height
\begin{tabular}{c|c}
  \textbf{\SETOL Terminology} & \textbf{Explanation}                      \\ \hline
  \ModelQuality $\Q$          & \makecell{Generalization accuracy, \\in the AA and at high-T }      \\ \hline
  \LayerQuality $\Q$          & \makecell{Layer contribution to the accuracy, \\in the AA and at high-T}       \\ \hline
  \LayerQualitySquared $\QT$ &  \LayerQuality squared, used for technical reasons \\ \hline
  \Quality \GeneratingFunction $\Gamma_{\Q}, \Gamma_{\QT}$   & Generating function for \Quality    \\ \hline
  \AnnealedHamiltonian $H^{an}$                & \makecell{Energy function, \\for errors or accuracies}             \\ \hline
  \EffectiveHamiltonian $H^{eff}$     & \makecell{Exact energy function, \\but restricted to a low-rank subspace}      \\ \hline
\end{tabular}
\caption{Additional terminology introduced for the \SETOL.  
  Notice that the \Quality \GeneratingFunction $\Gamma$ is simply one minus the \FreeEnergy, $\Gamma:=1-F$,
but it is introduced because sign convention for the \FreeEnergy is always decreasing with the error.
  In contrast, we define the Hamiltonian in terms of the model error or accuracy, depending on the context.
}
\label{table:SETOL_terminology}
\end{table}


\paragraph{Traditional \SMOG theory.}
In traditional \SMOG theory, one maps the learning process of a NN to the states and energies of a physical system.
The mapping is given in Table~\ref{table:statMech_to_NNs}.
\SMOG theory models the SGD training of a \emph{\Perceptron} on the data, $\NDX$, to learn the optimal weights, $\WVEC$, as a Langevin process.%
\footnote{Typically, we have no guarantee of the true equilibrium in a high‚Äêdim nonconvex landscape; so, when the \emph{\ThermodynamicLimit} exists,
the Langevin process converges or relaxes to the thermodynamic equilibrium.} The power of the \STATMECH approach comes from the fact that the core concept of \ThermalAverages corresponds to taking the expectation of a given quantity only \emph{over the set of trained models}, as opposed to uniformly over all possible models (or in a worst-case sense, over all possible models in a model class).
This capability is particularly compelling in light of the \STATMECH capacity to characterize disordered 
systems with complex non-convex energy landscape (which can even be \emph{\Glassy}, characterized by a
highly non-convex landscape~\cite{SST92, STS90, engel2001statistical},
and recognized classically by a slowing down of the training dynamics~\cite{gutfreund1985spin}).
Thus, concepts such as training and \GeneralizationError arise naturally from integrals that are familiar to \STATMECH; 
and theoretical quantities such as Load, Temperature, and \FreeEnergy also map onto useful and relevant concepts~\cite{MM17_TR}. 
The \FreeEnergy is of particular interest because it can be used as a generating function to obtain the desired \GeneralizationError and/or accuracy.
We wish to understand how to compute the associated thermodynamic quantities such as the expected value of the
various forms of the \AverageGeneralizationError $(\AVGGE)$, \PartitionFunction $(Z)$, and the
\FreeEnergy $(F)$ and other \GeneratingFunctions~$(\Gamma)$.

\paragraph{The \StudentTeacher model.}
We seek to compute and/or estimate the \AverageGeneralizationAccuracy for a \emph{fixed} \Teacher Perceptron $T$
by averaging over an ensemble of \Student $S$ \Perceptrons, in the \AnnealedApproximation (AA), and at
\HighTemperature (high-T); we call this ST \ModelQuality, and denote it $\Q^{ST}$.
In Section~\ref{sxn:matgen}, we generalize $\Q^{ST}$ to an
arbitrary layer in a \MultiLayerPerceptron, giving a \LayerQuality, i.e., $\Q^{ST}\rightarrow\Q$.
This formulation of the ST problem differs from the classic approach~\cite{SST92,engel2001statistical} in that
we treat the \Teacher as input to the theory rather than a random vector $\TVEC$
and that \Teacher provides the trained weights $\WVEC$ explicitly as opposed
to the labels $\Yt$.
%because one
%usually does not fix the \Teacher but, instead,
%averages over all \Teacher vectors $\TVEC$~\cite{SST92,engel2001statistical}.
In the simpler \Perceptron formulation, there are only a few degrees of freedom (i.e., the ST overlap $R$, the feature load, the inverse Temperature $\beta$, and the number of training examples $\ND$), whereas in the matrix generalization,
the structure of the empirical \Teacher weight matrix  $\TMAT=\WMAT$ provides $N\times M$ additional degrees of freedom,
allowing for a much richer theoretical analysis.
%\red{[This is wrong -- \cite{SST92} and ~\cite{EngelAndVanDenBroeck} both treat the Teacher and a single fixed vector, and the average is either a Boltzmann average over students or a quenched average over datasets or both. See \EQN 2.4 in~\cite{EngelAndVanDenBroeck} or \EQN 4.5 in~\cite{SST92}.]}
%\red{This formulation of the \StudentTeacher problem is different from the classic approach because while we assume that there is a fixed \Teacher, we only assume that the \Teacher provides the ESD of the trained weight matrix $\WMAT$, and not the labels.}
This is one of many ways that distinguishes the current approach from previous work.
Because of this, we present both a pedagogic derivation of $\Q^{ST}$
(for a general NN in Section~\ref{sxn:mathP}, and for the ST model specifically
in the Appendix, Section~\ref{sxn:summary_sst92}), whereas in
Section~\ref{sxn:SMOG_main-st_av} we provide a simple derivation, assuming
the \AnnealedApproximation and \HighTemperature at all times.

\paragraph{Towards a \SemiEmpirical Theory.}
In the \SETOL approach to \STATMECH, we want a matrix generalization of the \StudentTeacher \ModelQuality, $\Q^{ST}$, for a single \LayerQuality
$\Q\sim\Q^{NN}_{L}$ in an arbitrary \MultiLayerPerceptron (MLP).
This matrix generalization is a relatively straightforward extension of the classical (i.e., for a vector \Teacher) \SMOG ST \ModelQuality (but our \SETOL approach will use it in a conceptually new way).

In our matrix generalization, the \Teacher vector $\TVEC$ is replaced by a \Teacher matrix $\TMAT$ (i.e., $\TVEC\rightarrow\TMAT$); 
and, in our \SETOL approach, $\TMAT$ represents actual (pre-)trained NN weight matrix (i.e., $\TMAT\simeq\WMAT$) that has been trained on real-world, strongly correlated data $\ADD$.
As such, for our \SETOL theory, we seek an expression that can be parameterized by the \Teacher, and in particular by the ESD of the \Teacher.
This is what makes the basic method \SemiEmpirical: 
we make a modeling assumption that the \Teacher has the same limiting spectral distribution as the \Student, 
and hence the same PL fit parameter $\alpha$. 
This is all done with the understanding that later we will augment
(and hopefully ``correct'') our mathematical formulations with 
phenomenological parameters fit from experimental data.
To make the \SemiEmpirical method a \SemiEmpirical \emph{Theory}, 
we not only seek to parameterize our model; but we also try to understand 
how to derive \HTSR empirical metrics, such as $\ALPHA$ and $\ALPHAHAT$,
how they arise from this formalism, 
how they are related to the correlations in the data, and 
why they are transferable and exhibit \Universality.
Importantly, we do not just seek a method with adjustable parameters, but rather formulate the theory using
techniques similar to those to explain the origins of Quantum Chemisty \SemiEmpirical methods, resulting in formal
expressions that resemble a renormalized Self-Energy
and/or \EffectiveHamiltonian from such approaches
~\cite{Martin1994,Martin1996_CPL,MartinFreed1996, Brandow1965, freed1977, Freed1983, PhysRevLett.69.800}.
This gives what we call a \SemiEmpirical \emph{Theory}.
