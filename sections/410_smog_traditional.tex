%\subsection{Traditional Approaches to the \SMOG} 
\subsection{\STATMECH: the \SMOG approach and the \SETOL approach} 

\label{sxn:trad_smog}

In this subsection, we review the basic \STATMECH setup necessary to understand \SMOG theory as well as \SETOL.
This theory was developed in the 1980s and 1990s~\cite{SST90,SST92,Gar85,Gar88,engel2001statistical}. 


\begin{table}[t] %[h!]
\centering
%\begin{center}
\renewcommand{\arraystretch}{1.15} % Increases row height
\begin{tabular}{c|c}
  \textbf{Statistical Physics} & \textbf{Neural Network Learning}                      \\ \hline
  Gaussian field variables     & Gaussian i.i.d data  $\NDXI\in\mathcal{D}$            \\ \hline
  State Configuration          & Trained / Learned weights $\WVEC$                     \\ \hline
  State Energy Difference      & Training and Generalization Errors  $\AVGTE, \AVGGE$  \\ \hline
  Temperature                  & Amount of stochasticity present during training $T$       \\ \hline
  \AnnealedApproximation       & Average over the data first                          \\ \hline
  \ThermalAverage              & Expectation w.r.t. the distribution of trained models \\ \hline
  \FreeEnergy                  & Generating function for the error(s) $F$             \\ \hline
\end{tabular}
%\end{center}
\caption{Mapping between states and energies of a physical system and parameters of the learning process of a neural network.}
\label{table:statMech_to_NNs}
\end{table}


\begin{table}[t] %[h!]
\centering
%\begin{center}
\renewcommand{\arraystretch}{1.15} % Increases row height
\begin{tabular}{c|c}
  \textbf{SETOL Terminology} & \textbf{Explanation}                      \\ \hline
  \ModelQuality $\Q$          & \makecell{Generalization accuracy, \\in the AA and at high-T }      \\ \hline
  \LayerQuality $\Q$          & \makecell{Layer contribution to the accuracy, \\in the AA and at high-T}       \\ \hline
  \LayerQualitySquared $\QT$ &  \LayerQuality squared, used for technical reasons \\ \hline
  \Quality \GeneratingFunction $\Gamma_{\Q}, \Gamma_{\QT}$   & Generating function for quality    \\ \hline
  \AnnealedHamiltonian $H^{an}$                & \makecell{Energy function, \\for errors or accuracies}             \\ \hline
  \EffectiveHamiltonian $H^{eff}$     & \makecell{Exact energy function, \\but restricted to a low-rank subspace}      \\ \hline
\end{tabular}
%\end{center}
\caption{Additional terminology introduced for the \SETOL.  
  Notice that the \Quality \GeneratingFunction $\Gamma$ is simply one minus the \FreeEnergy, $\Gamma:=1-F$,
but it introduced because sign convention for the \FreeEnergy is always decreasing with the error.
  In contrast, we define the Hamiltonian in terms of the model error or accuracy, depending on the context.
  }
\label{table:SETOL_terminology}
\end{table}


\paragraph{Traditional \SMOG theory.}
In traditional \SMOG theory, one maps the learning process of a NN to the states and energies of a physical system.
The mapping is given in Table~\ref{table:statMech_to_NNs}.
\SMOG theory models the SGD training of a \emph{\Perceptron}, on the data, $\NDX$, to learn the optimal weights, $\WVEC$, as a Langevin process.%
\footnote{Typically, we have no guarantee of the true equilibrium in a high‚Äêdim nonconvex landscape; so,
when the \emph{\ThermodynamicLimit} exists,
the  Langevin process converges or relaxes to the thermodynamic equilibrium.}
The power of the \STATMECH approach comes from the fact that the core concept of \ThermalAverages corresponds to taking the expectation of a given quantity only \emph{over the set of trained models}, as opposed to uniformly over all possible models (or, in a worst-case sense, over all possible models in a model class).
This capability is particularly compelling in light of the \STATMECH capacity to characterize disordered 
systems with complex non-convex energy landscape (which can even be \emph{\Glassy}, characterized by a
highly non-convex landscape~\cite{SST92, STS90, engel2001statistical},
and recognized classically by a slowing down of the training dynamics~\cite{gutfreund1985spin}).
Thus, concepts such as training and \GeneralizationError arise naturally from integrals that are familiar to \STATMECH; 
and theoretical quantities such as Load, Temperature, and \FreeEnergy also map onto useful and relevant concepts~\cite{MM17_TR}. 
The \FreeEnergy is of particular interest because it can be used as a generating function to obtain the desired \GeneralizationError and/or accuracy.
We wish to understand how to compute the associated thermodynamic quantities such as the expected value of the
various forms of the \AverageGeneralizationError $(\AVGGE)$, \PartitionFunction $(Z)$, and the
\FreeEnergy $(F)$ and other \GeneratingFunctions~$(\Gamma)$.


\charles{TODO: discuss mean field theory, saddle points, replicas, etc.} 
\michael{Certainly not~here.}
\charles{A good introduction discusses the themes in a section}
\michaeladdressed{@charles: Can we just cange ``Generalization Accuracy'' to ``Model Quality'' and ``Model Accuracy'' since I think that will help to minimize confusion.  We should define what precisely is Model Quality and new notions.}

\paragraph{The \StudentTeacher model.}
\charles{@michael?  Removing for now ; we should discuss :
XXX. PUT TWO OR THREE SENTENCES ON THE BASIC STUDENT SETUP; START WITH THE FOLLOWING AND REFINE.
For simplicity, let's consider the classification of the elements of some input space $X$ into one of two classes, $\{0,1\}$.
There is a \emph{target rule}, $T$, which is one particular mapping of the input space into the set of classes, as well as a \emph{hypothesis space}, $\mathcal{F}$, which consists  of the available mappings $f$ used to approximate the target $T$.
The set $\mathcal{F}$ could, e.g., consist of NNs with a given structure.
Given this setup, the problem of learning from examples is the following: on the basis of the classification determined by the target rule $T$ for the elements of a subset $\mathcal{X} \subset X$, which is the \emph{training set}, select an element of $\mathcal{F}$ and evaluate how well that element approximates $T$ on the complete input space $X$.
One can think of the target rule $T$ as being a \emph{teacher}, and the goal of the \emph{student} is to approximate the teacher as well as possible.
The \emph{generalization error} $\varepsilon$ is the probability of disagreement between the student/hypothesis and teacher/target on a randomly chosen subset of $X$.
XXX. OLD STUFF VECTORS.
XXX. NEW STUFF MATRICES.}

%To express these integrals, the \SMOG approach models both Students $S$ and Teachers $T$ as \emph{weight vectors},
We seek to compute and/or estimate the  \AverageGeneralizationAccuracy for a \emph{fixed} \Teacher Perceptron $T$
by averaging over an ensemble of \Student $S$ \Perceptrons, in the \AnnealedApproximation (AA), and at
\HighTemperature (high-T); we call this ST \ModelQuality, and denote it $\Q^{ST}$.
We will then, later, in Section~\ref{sxn:matgen}, we generalize $\Q^{ST}$ to an
arbitrary layer in \MultiLayerPerceptron, giving a \LayerQuality, i.e., $\Q^{ST}\rightarrow\Q$.
This formulation of the ST problem is different than the classic approach  because one
usually does not fix the \Teacher but, instead,
averages over all \Teacher vectors $\TVEC$~\cite{SST92,engel2001statistical}.
This is one of many ways that distinguishes the current approach from previous work.
Because of this, we present both a pedagogic derivation of $\Q^{ST}$
(for a general NN in Section~\ref{sxn:mathP}, and for the ST model specifically
in the Appendix, Section~\ref{sxn:summary_sst92}),
and we provide a more heuristic approach in Section~\ref{sxn:SMOG_main-st_av}, assuming
the AA and high-T at all times.


\paragraph{Towards a \SemiEmpirical Theory.}
In the \SETOL approach to \STATMECH, we want a matrix generalization of the ST \ModelQuality, $\Q^{ST}$, for a single \LayerQuality
$\Q\sim\Q^{NN}_{L}$ in an arbitrary \MultiLayerPerceptron (MLP).
This matrix generalization is a relatively straightforward extension of the classical (i.e., for a vector \Teacher) \SMOG ST \ModelQuality (but our \SETOL approach will use it in a conceptually new way).
%
In our matrix generalization, the \Teacher vector $\TVEC$ is replaced by a \Teacher matrix $\TMAT$ (i.e., $\TVEC\rightarrow\TMAT$); 
and, in our \SETOL approach, $\TMAT$ will be an actual (pre-)trained NN weight matrix (i.e., $\TMAT=\WMAT$). 
Importantly, this matrix $\WMAT$ is neither a Gaussian Random Matrix, nor is it obtained from Gaussian i.i.d training data.
As such, for our \SETOL theory, we seek an expression that can be parameterized by the \Teacher, and in particular by the ESD of the \Teacher.
This is what makes the basic method \SemiEmpirical: even though we do not know the form of the \Teacher,
we make a modeling assumption that the  \Teacher has the same limiting spectral distribution as the \Student, 
and hence the same PL fit parameter $\alpha$. 
This is all done with the understanding that later we will augment
(and hopefully ``correct'') our mathematical formulations with 
phenomenological parameters fit from experimental data.
To make the \SemiEmpirical method a \SemiEmpirical \emph{Theory}, 
we not only seek to parameterize our model; but 
we also try to understand 
how to derive \HTSR empirical metrics, such as $\ALPHA$ and $\ALPHAHAT$,
how they arise from this formalism, 
how they are related to the correlations in the data, and 
why they are transferable and exhibit \Universality.
This gives what we call a \SemiEmpirical \emph{Theory}.
