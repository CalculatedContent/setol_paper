\subsubsection{Theoretical Student-Teacher Average Generalization Error $(\AVGSTGE)$, }
\label{sxn:SMOG_main-st_av}

Here, we seek a simple, formal expression for the
\StudentTeacher Average \GeneralizationError, $\AVGSTGE$,
that can be used as the starting point for our extended \SemiEmpirical theory.

%\input{data_mapping}

\paragraph{The Data Model.}

To develop a \SemiEmpirical theory of the \Teacher \GeneralizationError, $\TGE^{T}$, 
instead of training and evaluating a NN model using real data $(\DX)$,
we seek a simple, analytical expression with parameters that can be fit to empirical measurements.
So in addition to using a model for our NN, we must specify a model for the data.
In a real NN, the data $\DX$ is correlated, and, in fact, very strongly correlated;
and this is reflected in the layer weight matrices.
However, to be tractable, our starting theoretical expressions use uncorrelated (i.i.d) data.
Formally, we must replace the correlated data 
with some uncorrelated, random model of the data, i.e., $\XVEC\rightarrow\XI$.
As described in Figure~\ref{fig:data_mapping},
our \DataModel is a standard Gaussian $N(0,1)$ model for the input data
\begin{align}
\DATA\rightarrow\XImu,\;\;\XImu\in N(0,1) ,
\label{eqn:mwm_replace_1}
\end{align}
where $N(0,1)$ denotes a Gaussian distribution with zero mean and unit variance,
\and $\XImu$ is normalized such that $\Vert\XImu\Vert=\tfrac{1}{M}$ for all $N$ data vectors.
\michaeladdressed{@charles: I just added a label there, in case you are keeping track of labels.}

We make this distinction between Actual and \ModelData to emphasize that,
later, we will use our so-called \SemiEmpirical procedure to
account for the real correlations in the actual data phenomenologically
by taking some analytical parameter of the theory and fitting it to the real world observations,
here, on the ESD of the NN weight matrices.


\paragraph{The ST Error Model and the Annealed Potential $\EPSLSTx$.}
We now model \Teacher error $\AVGGE^{T}$ with the
\emph{\AverageSTGeneralizationError} $\AVGSTGE$, which is obtained
\michaeladdressed{Is that quite true; doesn't the former involve an extra average, so they are different; see also the comment above on being pedagogically confusing.}
 by \emph{first} computing the ST error function
 %$\Delta\mathbf{E}_{\mathcal{L}}(\SVEC,\TVEC, \XI)$
$\DETOPSTL$
over the set of \emph{all} possible $N$ input examples $\XI$.  Define the data-dependent ST test error function
--or Energy difference--as 
\begin{align}
\label{eqn:DE_L}
%\Delta\mathbf{E}_{\mathcal{L}}(\SVEC,\TVEC, \XI):=\sum_{\mu=1}^{N}\mathcal{L}[E_{NN}(\SVEC,\XI_{\mu}),E_{NN}(\TVEC,\XI_{\mu})]  .
\DETOPSTL:=\sum_{\mu=1}^{N}\mathcal{L}[E_{NN}(\SVEC,\XI_{\mu}),E_{NN}(\TVEC,\XI_{\mu})]  .
\end{align}
where $\mathcal{L}(\SVEC,\TVEC, \XI)$ is simply the $\ell_2$ loss.  This measures the error
between the \Student and the \Teacher; it is zero when their predictions are identical,
$(\Ys=\Yt)$ when $(\XI=\XImu)$, and is nonzero otherwise.

%%5Let us write the Average St \GeneralizationError, $\AVGSTGE$ (formally) as in \EQN~\ref{eqn:finalEgen} as
%%5\begin{align}
%%5\label{eqn:AVGSTGE0}
%%5\AVGSTGE:= & \left\langle\THRMAVG{\Delta\mathbf{E}_{\mathcal{L}}(\SVEC,\TVEC, \XI)}\right\rangle_{\AVGNDXI} 
%%5\end{align}
%%5where we first average over the model data $\NDXI$  and then take a Thermal
%%5average over the \Student weight vectors $\SVEC$.
%%5
%%5We will reduce $\AVGSTGE$ now using the \AnnealedApproximation (AA).
%%5We define two kinds of data-averaged ST errors; the first is used to
%%5define the data-averaged ST \TrainingError, and the second
%%5defines the data-averaged ST test error.
%%5If take the average over the training examples $\XItrain$, we can write
%%5\begin{align}
%%5\label{eqn:ST_train_error}
%%5\langle\Delta\mathbf{E}_{\mathcal{L}}(\SVEC,\TVEC,\XI)\rangle_{\AVGNDXItrain} := \dfrac{1}{\Ntrain}\sum_{\mu=1}^{\Ntrain}\Delta\mathbf{E}_{\mathcal{L}}(\SVEC,\TVEC,\XI_{\mu})
%%5\end{align}
%%5and use this to define the \emph{Model \TrainingError} $\AVGSTTE$.
%%5We dont consider this here,  but it is important for the classic approach;
%%5for a longer discussion, see Section~\ref{sxn:summary_sst92}.
%%5
We aim to derive a simple expression for the  \AverageSTGeneralizationError, $\AVGSTGE$, and to do this, 
we define the  \EffectivePotential for the data-averaged ST \GeneralizationError $\EPSL(\SVEC,\TVEC)$, as in \EQN~\ref{eqn:epsl}, as:
\begin{align}
\label{eqn:STerror}
%\EPSLSTx = \langle\Delta\mathbf{E}_{\mathcal{L}}(\SVEC,\TVEC,\XI)\rangle_{\AVGNDXI}:=\frac{1}{N}\int d\mu(\NDXI)\Delta\mathbf{E}_{\mathcal{L}}(\SVEC,\TVEC,\XI)  ,
\EPSLSTx = \langle\DETOPSTL\rangle_{\AVGNDXI}:=\frac{1}{N}\int d\mu(\NDXI)\DETOPSTL
\end{align}
The measure $d\mu(\NDXI)$ will end up being a Gaussian measure over $N$ samples
(see Appendix~\ref{sxn:summary_sst92}), and the intent is to evaluate it
in the large-$N$ limit, thereby sampling all possible inputs in the model space, $\XI\in\MDD$.

As in \EQN~\ref{eqn:finalEgen} (Section~\ref{sxn:mathP}), by applying the AA, we can rewrite the \AverageSTGeneralizationError, $\AVGSTGE$:
first, a simple average over all the possible inputs $\XI$; and, 
second, then as a Thermal average over all Students $S$, in the AA, and at high-T 
\begin{align}
\label{eqn:MGE}
\AVGSTGE:=\THRMAVG{\EPSLSTx} .
\end{align}
(Recall that in this regime $\AVGSTGE=\AVGGE^{an,hT}$.)


In the classic \STATMECH approach, the average $\THRMAVG{\cdots}$ is
a \ThermalAverage in the canonical ensemble with $\beta$ fixed,
as explained in Section~\ref{sxn:mathP}.  Here, we will do something similar, as the \Student
average $\THRMAVG{\cdots}$ will be computed from the associated
generating function $\IZG$ for the matrix-generalized case  (i.e., an HCIZ integral defined over all students,
and in the large-$N$ limit).)

\michael{MM TO DO: The content of this par is good, it just probably needs to be moved.}
Recall that above, the empirical estimate for $\AVGEMPGE$ depended on a
specific instantiation of the model for the training data $\DATAtrain$,
i.e  $\AVGSTGE$ is \Quenched to the training data.
For that reason, for the final result, we needed to take a second,
quenched average over all possible data sets.
Here, we do not need to consider this and always work in the \AnnealedApproximation(AA).
This is because we incorporate
the specific effects of the real-world training data $(\NDX)$ after we derive our formal expressions
by fitting the model parameters to empirical data.
The final expression for $\AVGSTGE$, derived below,
will be generalized to $\AVGNNGE$, matrix-generalization of  the classic \STATMECH formula
for the \LinearPerceptron, in the \Annealed and High-T approximations.
(see Appendix~\ref{sxn:summary_sst92}). 


%%\subsubsection{The Effective Potential as a function of the overlap $(\EPSL(R))$}
\paragraph{The Annealed Potential as a function of the overlap $(\EPSL(R))$.}

%%In this subsection, we show that the  data-averaged ST test error function (for the $\mathcal{L}=\ell_2$ loss) is:
%%\begin{align}
%%  \EPSL(R)=1-R,\;\;\mathcal{L}=\ell_2
%%\end{align}
%%
We want an expression for the data average of the ST test error, from \EQN~\ref{eqn:STerror}, generalized from \Perceptron vectors to NN layer weight matrices.
\michael{MM TO DO: reword that sentence, since it is a bit confusing, since we are vectors here, and I moved the matrix stuff below to the next section.}
For the \Perceptron, one obtains different expressions for the ST error function, depending on 
the type of activation function $h(x)$ in \EQN~\ref{eqn:ENN};
The simplest are the Linear and Boolean \Perceptrons, and
for both (and with $\ell_2$ loss),
 $\EPSL(\SVEC,\TVEC)$ is simply a function of the ST overlap $R$~\cite{SST92}.
This gives $\EPSLSTx\rightarrow\EPSL(R)$, where
\begin{align}
  \label{eqn:Rdef}
R=\frac{1}{N}\SVEC^{\top}\TVEC=\frac{1}{N}\sum_{i=1}^{M}s_{i}t_{i},
\end{align}
which is simply the dot product between the $M$-dimensional \Student $\SVEC$ and \Teacher $\TVEC$ weight vectors, and normalized by the number of training instances $N$.
For a \LinearPerceptron~\cite{SST92},% 
\footnote{In the classic approach for the ST model, the theory examined different expressions $\EPSL(R)$.
For example, one can consider the  Boolean \Perceptron~\cite{SST92,Ros62}, with activation function $h(x)=\mbox{sgn}(x)$, 
i.e., the Heaviside step function. Then, the error is
$
\EPSL(R)=1 - \dfrac{1}{\pi}\arccos(R).
$
In both cases, perfect learning occurs when $R=1$~\cite{SST92}.
}
with activation function $h(x)=x$,  the error function is
\begin{align}
\EPSL(R)=1-R  .
\label{eqn:LinearPerceptronError}
\end{align}



\input{sections/overlap_figure}
%\caption{Comparison of 2D and 3D representations of matrix overlap $R$. (a) 2D visualization of vector overlap $R = \cos(\theta) $.
%  (b) 3D visualization of matrix overlap $R = \Trace{\frac{1}{N^2} \SMAT^\top \TMAT $ with solid angle $R$.}



%%\subsubsection{Derivation of the  ST error $(\EPSL(R))$ for the Linear Perceptron}
\paragraph{Derivation of the  ST error $(\EPSL(R))$ for the Linear Perceptron.}
%\nred{WARNING: there may be some mistakes here}
To derive \EQN~\ref{eqn:LinearPerceptronError},
define the data-dependent ST error (\EQN~\ref{eqn:DE_L}) in terms of an $\ell_2$ loss function
%\begin{align}
%\nonumber
%\Delta \mathbf{E}_{\ell_2}(\SVEC,\TVEC,\XI) = & \frac{1}{2} (\Ys - \Yt)^T(\Ys - \Yt) 
%   = 1 - (\YsVEC)^{\top}(\YtVEC) \\ 
%\label{eqn:deriveSTerror}
%   =&  1 - \eta(\XI),
%\end{align}

\begin{align}
\nonumber
\DETOPSTLL= & \frac{1}{2} (\YsVEC - \YtVEC)^{\top} (\YsVEC - \YtVEC) \\
\nonumber
=& \frac{1}{2} \big[(\YsVEC)^{\top} \YsVEC - 2 (\YsVEC)^{\top} \YtVEC + (\YtVEC)^{\top} \YtVEC \big] \\
\nonumber
=& N - (\YsVEC)^{\top} (\YtVEC) \\
\label{eqn:deriveSTerror}
=& N(1 - \ETA(\SVEC,\TVEC)),
\end{align}
where we call $\ETA(\SVEC,\TVEC,\XI):=\tfrac{1}{N}\mathbf{y}_{S}^{\top}\mathbf{y}_{T}$
the \emph{data-dependent \SelfOverlap}.
The expression $\ETA(\SVEC,\TVEC,\XI)$ is analogous to the ST overlap $R$, but before the data has been integrated out.
The \SelfOverlap $\ETA(\SVEC,\TVEC,\XI)$ will appear later in \EQN~\ref{eqn:eta2} (in Section~\ref{sxn:matgen}), 
when formulating the matrix-generalized overlap operator~$\OVERLAP$.

Using the $E_{NN}$ Energy generating or output function (\EQN~\ref{eqn:ENN}), we can replace the label vectors $\YtVEC,\YsVEC$ as
\begin{align}
\YsVEC=\SVEC^{\top}\XI,\;\;
\YtVEC=\TVEC^{\top}\XI  ,
\end{align}
which gives
\begin{align}
  \label{eqn:eta_vec_xi_def}
\ETA(\SVEC,\TVEC,\XI) = \tfrac{1}{N}(\SVEC^{\top}\XI)^{\top} (\TVEC^{\top}\XI) = \tfrac{1}{N}\XI^{\top} (\SVEC^{\top} \TVEC )\XI   .
\end{align}
or, more simply,
\begin{align}
  \label{eqn:eta_vec_avg_def}
\ETA(\SVEC,\TVEC) = \langle\ETA(\SVEC,\TVEC,\XI)\rangle_{\AVGNDXI}=\tfrac{1}{N}(\SVEC^{\top} \TVEC )
\end{align}

We want to evaluate this as an \EffectivePotential $\EPSL(R)$ for the data-averaged ST test error, as in \EQN~\ref{eqn:STerror}.
To do this, we need to compute the average or \ExpectedValue over all possible input data $\XI$,
an $N-$dimensional Gaussian vector (i.e., $d\mu(\XI)=\mathcal{D}\NDXI P(\NDXI)$).
%
%Using $\mathcal{N}$ as the normalization for the average, as yet unspecified, we obtain 
\begin{align}
 \langle\DETOPSTLL\rangle_{\AVGNDXI}  
   = & \int d\mu(\NDXI)(1-\ETA(\XI)) \\ \nonumber
   = & \int d\mu(\NDXI)(1-\XI^{\top}\tfrac{1}{N}\SVEC^{\top}\TVEC\XI) \\ \label{eqn:XI_ST} 
   = & \int d\mu(\NDXI)-\int d\mu(\NDXI)\XI^{\top}\tfrac{1}{N}\SVEC^{\top}\TVEC\XI)  \\ \nonumber
   = & 1 - \int d\mu(\NDXI)\XI^{\top}\tfrac{1}{N}\SVEC^{\top}\TVEC\XI \\ \nonumber
   = & 1 - \int d\mu(\NDXI)\XI^{\top} R\XI \\ \nonumber
   = &1 - R\int d\mu(\NDXI)\XI^{\top}\XI,
\end{align}
where this holds because the elements of $\XI$ are i.i.d.
Since $d\mu(\NDXI)$ is a measure over a (multi-variate) Gaussian distribution,
if we normalize the data vectors $\XI$ to $\frac{1}{\sqrt{M}}$ (See Section~\ref{app:st-gen-err-annealed-ham}), then the second term on the R.H.S. is unity and we recover (i.e.,\EQN~\ref{eqn:epsl})
\begin{align}
\label{eqn:e0}
\EPSL(R)=\langle\DETOPSTLL\rangle_{\AVGNDXI} =  1 - R .
\end{align}
%%%
%%% CHM: what is this ?
%%\begin{align}
%%\frac{1}{N}\langle\Delta \mathbf{E}_{\ell_2}(\SVEC,\TVEC,\XI)\rangle_{\XI}  
%%= &\frac{1}{\mathcal{N}}\int d\mu(\XI)(1-\ETA(\XI)) \\ \nonumber
%%= &\frac{1}{\mathcal{N}}\int d\mu(\XI)(1-\XI^T\SVEC\TVEC^{\top}\XI) 
%%\end{align}
%%where $\mathcal{N}$ is the normalization for the average, as yet unspecified. Expanding, this gives
%%\begin{align}
%%\label{eqn:XI_ST}
%%\frac{1}{N}\langle\Delta \mathbf{E}_{\ell_2}(\SVEC,\TVEC,\XI)\rangle_{\XI} 
%%= & \frac{N}{\mathcal{N}} - \frac{1}{\mathcal{N}}\int d\mu(\XI)\XI^T\SVEC\TVEC^{\top}\XI \\ \nonumber
%%= & \frac{N}{\mathcal{N}} - \frac{1}{\mathcal{N}}\int d\mu(\XI)N\XI^T R\XI \\ \nonumber
%%= & \frac{N}{\mathcal{N}} - R\frac{N}{\mathcal{N}}\int d\mu(\XI)\XI^T\XI,
%%\end{align}
%%where the third line holds because the elements of $\XI$ are i.i.d.
%%Since $d\mu(\XI)$ is a measure over a Gaussian distribution, if we normalize the data vectors $\XI$ to $\frac{1}{\sqrt{N}}$, 
%%then $\mathcal{N}=N$, and we recover 
%%\begin{align}
%%\label{eqn:e0}
%%\EPSL(R)=\frac{1}{N}\langle\Delta \mathbf{E}_{\ell_2}(\SVEC,\TVEC,\XI)\rangle_{\AVGNDXI} =  1 - R .
%%\end{align}

\michaeladdressed{We have several types of subscripts on the BraKet average: $\NDXI$, $\XI$, and $\NDXIn$.  Im not sure if there are typos, or if things are overloaded. The difference isnt clear to me yet.}

In traditional \STATMECH (e.g., \cite{SST92}), one is interested in how the \emph{\TotalModelGeneralizationError} $\TGE(R)$ depends on $R$.
With these simple error functions, \EQN~\ref{eqn:MGE} reduces to a function over $R$,
and the \AverageSTGeneralizationError $\STGE(R)$ is then obtained by averaging over the Students 
\begin{align}
\label{eqn:AVGSTGE_R}
\AVGSTGE(R)=\THRMAVG{\EPSL(R)}=
\THRMAVG{1-\langle\ETA(\SVEC,\TVEC,\XI)\rangle_{\AVGNDXI}}=
\THRMAVG{1-\ETA(\SVEC,\TVEC)}=
\THRMAVG{1-\tfrac{1}{N}\SVEC^{\top}\TVEC}=
\THRMAVG{(1-R)}  ,
\end{align}
where $\THRMAVG{\cdots}$ is a \ThermalAverage over the \Student weight vector $\SVEC$.

The \ModelQuality for the ST \Perceptron, $\Q^{ST}$
is just the \AverageGeneralizationAccuracy, so we can write
\begin{align}
\label{eqn:QST_final}
\Q^{ST} := 1 - \AVGSTGE(R) 
       = \THRMAVG{1 - \EPSL(R)} 
       = \THRMAVG{\langle\ETA(\SVEC, \TVEC, \XI)\rangle_{\AVGNDXI}} 
       = \THRMAVG{\ETA(\SVEC, \TVEC)} 
       = \THRMAVG{\tfrac{1}{N}\SVEC^{\top}\TVEC} 
       = \THRMAVG{R}.
\end{align}
\EQN~\ref{eqn:QST_final} is the starting point for deriving a \SEMIEMP theory for the \WW quality metrics (\ALPHA,\ALPHAHAT);
see Section~\ref{sxn:matgen_mlp3}.
To generalize this expression, we will start with the \SelfOverlap $\ETA(\SMAT,\TMAT,\XI)$ for a
\MultiLayerPerceptron (MLP3) in Section~\ref{sxn:matgen}.

Before doing this, however, we note that 
we can obtain this expression for $\STGE$ by defining the
\AnnealedHamiltonian $\HANHT(R)$, at high-Temperature, as in Section~\ref{sxn:mathP}, \EQN~\ref{eqn:Gan_highT_final}.
Indeed, it is really $\HANHT(R)=\EPSL(R)$ that we must generalize to the matrix case, which we do (using a technique
similar to a Replica calculation, but still in the AA).
For more details, see Appendix~\ref{sxn:summary_sst92}.
(In particular, doing this allows us to define the normalization needed later for the \TRACELOG condition).
\michaeladdressed{MM TO DO: still to work on this par.}

\input{sections/mapping_table}
