\subsection{What is a Semi-Empirical Theory?}

Historically, one of the most well known \emph{\SemiEmpirical} methods comes from \NuclearPhysics.
The \SemiEmpirical Mass Formula, dating back to 1935, is based on the heuristic Liquid Drop Model of the nucleus,
and it was used to predict experimentally observed binding energies of nucleons. 
This model describes nuclear fission, and it was central to its development of the atomic bomb:
\begin{quote}
  Prior to WWII, \NuclearPhysics was a phenomenological science, which relied upon experimental data and descriptive
  models~\cite{Negele05}.
\end{quote}
In the Post-war era, the epistemological nature of nuclear theory changed,
as it saw the development of \SemiEmpirical shell models of the nucleus.
These models were formulated with rigor (in the physics sense)
but also relied on heuristic assumptions and experimental data for accurate predictions.
They captured the structure of atomic nuclei
and could accurately describe various nuclear properties~\cite{Ivanenko1932, GoeppertMayer1949, Jensen1949}.
The shell models, analogous to the electronic shell structure of atoms,
represented a shift toward a more rigorous understanding of nuclear phenomena.

About this time, \RMT~itself was also introduced by \emph{\Wigner}~\cite{Wigner55}
to model the statistical patterns of the nuclear energy spectra of strongly interacting heavy nuclei.
These patterns were universal, independent of the specific nucleus,
suggesting that a probabilistic approach would be fruitful.
In the following decades, \RMT saw many advances, including the development of
the Marchenko-Pastur model~\cite{MarchenkoPastur1967},
and numerous other applications in physics~\cite{Guhr1998}.
By the 1990s, \RMT~was further expanded when \emph{Zee} introduced the \emph{Blue Function},
and reinterpreted the \emph{R-transform} as a self-energy within the
framework of many-body / quantum field theory (QFT)~\cite{Zee1996}.
Also, so-called \HCIZtext integrals, integrals over random matrices,
were being used both to model disordered electronic spectra~\cite{SchultenRMT},
and, later, the behavior of spin glass models~\cite{Bouchaud1998,Cherrier_2003}.

Returning to the 1950s, and prior to the development of highly accurate, modern,
computational \emph{ab initio} theories of \QuantumChemistry, %\cite{ChemistryNobel1988},
Theoretical chemists introduced the \SemiEmpirical PPP method
for
conjugated polyenes~\cite{PariserParr53}.
The PPP model recasts the electronic structure problem as an \emph{\EffectiveHamiltonian}
for the $\pi$-electrons.
The PPP model resembles the later developed tight-binding model of condensed matter physics\cite{Hubbard1963}.
For many year,s this and related \SemiEmpirical methods
worked remarkably well, even better than the existing \emph{ab initio}
theories~\cite{Dewar1975,Ridley1973,Stewart1990,Warshel1976,THIEL2005559}.
Most importantly, these methods could be \emph{fit}
on a broad set of empirical molecular data, and then applied to molecules not in the original training set.

Around the same time, \emph{Löwdin} first formalized the concept of the \EffectiveHamiltonian,
which allowed the reduction of complex many-body problems to simplified
\emph{Effective Potentials} that still captured the essential physics.
Then, in the late 1960s, Brandow 
developed an \EffectiveHamiltonian theory of
nuclear structure, 
leveraging the \emph{Linked Cluster Theorem} (LCT) (see \cite{Hubbard1959}) and quantum mechanical many-body theory
to describe the highly correlated effective interactions in a reduced model space.
\footnote{Note also that the LCT shows that the log partition function $(i.e., \ln Z)$ can be expressed
a sum of connected diagrams, which is very similar to our result below, which expresses the
log partition function here as a sum of matrix cumulants from RMT.}

Like modern NNs, these \SemiEmpirical methods of \QuantumChemistry
worked well beyond their apparent range of validity,
generalizing very well to out-of-distribution (OOD) data. This led to the
search for a \SemiEmpirical \emph{Theory} to explain the
remarkable performance of these phenomenological methods.
Building on Brandow’s many-body formalism, Freed and collaborators
\cite{freed1977, Freed1983}
developed an \emph{ab initio} \EffectiveHamiltonian
Theory of \SemiEmpirical methods to explain the remarkable success of the \SemiEmpirical methods.
Specifically, the values of the PPP empirical parameters could be directly computed by way of
effective interactions, including both renormalized self-energies and higher-order terms.
 Somewhat later, in the 1990s,
 Martin et. al.~\cite{MartinFreed1996, Martin1996, Martin1996_CPL, Martin1998}
 extended and applied this \EffectiveHamiltonian theory
 and demonstrated the \Universality of the \SemiEmpirical PPP parameters numerically.
 Indeed, it is this \Universality that enabled the `for a time' inexplicable
 OOD performance of these methods.
 Crucially, this decades-long line of work established a comprehensive
 analytic and numerical \emph{Theory} of \SemiEmpirical methods.
 That is, a framework that confirmed the empirically observed \Universality
 provided theoretical justification for this,
 and enabled systematic improvements of the methods using numerical techniques.
  %bridging the gap between computationally expensive ab initio approaches and purely empirical techniques. This dual capability of providing explanatory power and enabling refinements made their contributions foundational to the evolution of computational \QuantumChemistry.

 Finally, it is important to mention the \EffectiveHamiltonian approach provided by the Wilson \emph{\RenormalizationGroup}
 (RG)~\cite{NobelPrizeRG,PhysRevLett.69.800}.
 The RG approach provides a powerful framework for studying strongly correlated systems across different scales,
 enabling the construction of an \EffectiveHamiltonian by \emph{integrating out} weakly-correlated degrees of freedom in a \ScaleInvariant way.
 It is particularly suited for critical points and phase boundaries --
 such as the phase boundary between generalization and memorization in spin glass models of neural networks --
 and, importantly, predicts the existence of Universal Power Law (PL) exponents .
For \SETOL, we take what amounts to a single step of the \emph{\ExactRenormalizationGroup} (\ERG), leading to an new empirical metric
that defines the 'Ideal' NN layer.

 
 \paragraph{Relevance to Deep Learning}
 In this sense, \SemiEmpirical theories of \NuclearPhysics and \QuantumChemistry,
 (as well as the \RenormalizationGroup approach), seem particularly appropriate
 for Deep Learning.
  DNN models are complex black boxes that defy statistical descriptions in that
  they are commonly pre-trained on a large set of data; and than applied to new data sets in new domains via transfer learning.
  Most recently, the inexplicable success of transfer-learning is seen
in the GPT (Generative Pre-Trained Transformer) models~\cite{Radford2018},
and motivated early work by Jumper et. al. on protein folding~\cite{JKS16_TR}.

In contrast, these \SemiEmpirical approaches differ from more
recently developed theoretical approaches to deep learning, which are typically based on SLT, rather than \STATMECH~\cite{Roberts2021}.
In particular, there have recently appeared several theories of deep learning, formulated using ideas from \RMT.
However, regarding realistic models, it has been explicitly stated that
``These networks are however too complex in general for developing a rigorous theoretical analysis on the spectral behavior''~\cite{LBNx17_TR}.
Even in recent work applying \RMT~to NNs, it has been noted
``\emph{that we make no claim about trained weights, only random weights}''~\cite{Yang2021}.
The weight matrices of a trained NN, however, are clearly \emph{not} simply random matrices---since they encode the specific correlations from the training data.
