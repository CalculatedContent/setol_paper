Deep Neural Networks (DNNS) have emerged as one of the most powerful modeling methods in science, engineering, and a wide range of other applications, including computer vision (CV) and natural language processing (NLP).
Despite this, there is no working theory which can explain \emph{why deep learning works}.
Here, we provide a theory that shows how to compute the generalization accuracy, or quality, of a (pre-)trained DNN simply by computing the empirical properties of the layer weight matrices, fitting the tail of the empirical spectral density (ESD) to a Power Law (PL), and plugging the PL exponent $(\alpha)$ into our resulting theoretical expression for the test error.
Using this, we derive the \WW~ \ALPHAHAT metric, which has been shown to correlate well with the test accuracies of the (pre-)trained DNNs across hundreds of models.
Our Semi-Empirical theory provides practical guidance for engineers designing complex, modern DNNs.  




