
\subsubsection{HCIZ Integrals}
\charles{How much to put here vs later, or has been earlier?}

Our goal is to generalize the Linear Student-Teacher Percepton, in the AA, and at high-T,
from Percepton vectors to NN matrices, and for that we need to generalize the thermal average over the
$m$-dimensional Perceptron weight vector $(\WVEC=\red{\SVEC})$
to an integral over NN Student $N \times M$ weight matrices $\mathbf{S}$;
the resulting Partition Function and Free Energy will be expressed with what is called an HCIZ integral.

Specifically, we  generalize the thermal average over data-averaged accuracy, which is
$1-\epsilon(R)=R$, giving $\THRMAVG{\epsilon(R)}$, 
where $R=\tfrac{1}{N}\SVEC^{T}\TVEC$  is the vector overlap between
Student $\SVEC$ and Teacher $\TVEC$ weight vectors.
To generalize the ST Perceptron to NN layers, the overlap $R$ will become
 $\mathbf{R}=\tfrac{1}{N}\SMAT^{T}{\TMAT}$, the ST overlap matrix or operator
between Student and Teacher layer weight matrices $\SMAT, \TMAT$.

Then, for technical reasons which will become obvious later,
we will approximate the thermal average over $R$ as
\begin{align}
  \THRMAVG{R}\approx\sqrt{\THRMAVG{R^{2}}}
\end{align}
Because $\mathbf{R}$ is now a matrix, the thermal average 
$\THRMAVG{R^{2}}$  will become an  HCIZ integral 
over the matrix generalized squared overlap 
\begin{align}
R^{2}\rightarrow \OLAPSQD,\;\;  \THRMAVG{R^{2}}\rightarrow\HCIZAVG{\OLAPSQD}
\end{align}
which is a Student random $(N \times M)$ layer weight matrices $\mathbf{S}$,
and evaluated over measure $d\mu(\mathbf{S})$, which is now a measure over
all suitable random matrices.

We will not, however, evaluate $\HCIZAVG{\OLAPSQD}$ directly using the measure
$d\mu(\mathbf{S})$, but, instead,
following Eqn~\ref{eqn:}, the \emph{Average Model Generalization Error} $\AVGNNGE$
will be obtained by taking the derivative of the matrix-generalized
Partition Function $\ZQ$.  This requires two critical steps.

\nred{probably we can cut this}
\paragraph{The Effective Correlation Space and the \TRACELOG Condition}
First, we must change the measure from all Student layer weight matrices
to all possible Heavy/Fat Tailed Student \emph{Correlation Matrices}
$d\mu(\mathbf{S})\rightarrow d\mu(\mathbf{A})$
where
$\mathbf{A}:=\tfrac{1}{N}\mathbf{S}^{T}\mathbf{S}$ (which is an $M \times M$ matrix)
using what we call the \TRACELOG condition.
This is accomplished with an SPA, and leads to the condition on $\mathbf{A}$.
that it only spans an \emph{Effective Correlation Space} (ECS), and is denoted by
$\AECS$  The ECS is defined by the eigencomponents corresponding to the
$\MECS$ largest eigenvalues $\LambdaECS_{i}$
where $MECS$ is chosen such that this set of eigenvalues satisfies
the following condition:
\begin{align}
  \ln[\det \AECS]=\Trace{\ln \sum_{i=1}^{\MECS}\LambdaECS_{i}}=0
\end{align}
Importantly, the \TRACELOG condition can be verified empirically on the layers
of real-world, large scale DNNs.

\paragraph{The HCIZ Integral}
We then effectively change the measure $d\mu(\mathbf{S})\rightarrow d\mu(\AECS)$,
where  $\AECS:=\tfrac{1}{N}\mathbf{S}\mathbf{S}^{T}$  (which is now an $N \times N$ matrix)
is restricted to the  to the \emph{Effective Correlation Space} (ECS) defined by the \TRACELOG condition.
\footnote{
We argue we can do this since both forms of $\AECS$ have the same eigenvalues (upto the null space),
 and when $\mathbf{A}$, and also therefore the measure $d\mu(\mathbf{A})$, is restricted to the ECS.
 This can be seen by looking at the Vandermonde Determinant~\cite{Vandermonde},
  which is effectively the same for both forms. See the Appendix, Section~\ref{sxn:TraceLogDerivation}.
 }
This lets us cast $\ZQ$ in the form of an HCIZ integral given by
\begin{align}
  \label{eqn:ZQ}
  \ZQ:= \int d\mu(\AECS) \exp\left(\beta Tr[\mathbf{T}\AECS\mathbf{T}^{T}]\right)
\end{align}
where $\OLAPSQD=\Trace{\mathbf{T}\AECS\mathbf{T}^{T}}$,
and the $\LambdaECS_{i}$ are the eigenvalues of the Teacher correlation matrix, $\XECS=\frac{1}{N}\TMAT\TMAT^{T}$,
and both $\AECS$ and $\XECS$  spans the low-rank ECS.

We then can define Generating Function (for the Quality squared $\QT$)
as the average of the log Parition Function
\begin{align}
  \label{eqn:IZG}
  \IZG:=  \dfrac{1}{N} \ln \ZQ
\end{align}

This integral can be evaluated in the AA and in the large-$N$ limit
using a technique developed by Takana~\cite{Tanaka},
\begin{align}
  \label{eqn:IZGINF}
  \IZGINF:= \lim_{N\rightarrow\infty} \IZG \approx \beta N \sum_{i=1}G(\LambdaECS_{i})
\end{align}
which is essentially a Saddle Point Approximation (SPA).
The final form the $\ZQ$ is given in the form of an SPA, where $G(\LambdaECS)$ is a kind of \emph{Genralized Norm}
which depends only empirical spectral properties of the Teacher layer weight matrix $\TMAT$
(i.e the layer eigenvalues $\LambdaECS_{i}$ in the ECS and/or the matrix moments)

The final result for the \emph{Average Layer Quality Squared}, $\QT$, is then given by the appropriate
derivative of the Generating Function $\IZG$
\begin{align}
  \label{eqn:Q2}
  \QT:= \frac{1}{\beta}\frac{\partial}{\partial N}\IZGINF = \sum_{i=1}G(\LambdaECS_{i})
\end{align}









