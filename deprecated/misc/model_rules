
In Section~\ref{empirical}, we will show that 

the tail of the layer ESD $\rho^{tail}(\lambda)$ contains the dominant generalizing components of the model.  Specifically, we can estimate the test accuracy of a DNN by evaluating the training accuracy reasonably well  by using the actual training data, and using a low-rank approximation (i.e. TruncatedSVD)  of it's weight matrices.  

Below, we define different \emph{Model Selection Rules} (MSR), which determine which eigenvectors to keep in the low-rank approximation, based on the associated eigenvalues $(\lambda)$.  These rules include:
\begin{enumerate}

\item  SVD20: keeping the largest $20\%$ of all $\lambda$.
\item SVDALPHA: keeping those in the tail  $\lambda\ge\lambda_{min}$, ads defined by the empirical PL fit.
\item LOGDETX: keeping just enough of the $\lambda$ such that $\log\det\mathbf{X}\simeq 1$. 
\end{enumerate}
All of these choices work to some extent, however, the MSR SVD20 works best on average.
Notably, we find that the MSR for SVDALPHA and the LOGDETX are nearly identical when the PL fit yields $\alpha\simeq 2$, and that  corresponds, roughly, on average, to around $20\%$ of the largest eigenvalues.  

\nred{add bullet points}


%
Using these empirical observations, we now construct a our \emph{Semi-Empirical} theory for data-free the \WW~\ALPHAHAT generalization capacity metric.
%
%
