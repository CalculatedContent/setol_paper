\subsubsection{Setup and Outline}
\label{sxn:tanaka_setup}

\noindent
To evaluate \ref{eqn:hciz2},
we %%MM%% In setting up the problem for this paper, we 
want to integrate over all \Student Correlation matrices $\mathbf{\AMAT}$
that ``resemble the \Teacher Correlation matrix $\mathbf{X}$.  
To %%MM%% So, first, we need to 
formalize this idea,
we need to %%MM%% which requires that we 
define the measure over ``all desired $\mathbf{\AMAT}$, $d\mu(\mathbf{\AMAT})$, 
in 
terms of the actual $M$ eigenvalues, $\left\{ \lambda_{i} \right\}_{i=1}^{M}$, of the \Teacher.


\paragraph{Randomness assumption.}
For real weights $\mathbf{W}$,  we assume an \emph{orthogonally invariant} ensemble,
$d\mu(\mathbf W)=d\mu(\mathbf U\mathbf W\mathbf U^{\top})$ for all $\mathbf U\in O(M)$,
mirroring the isotropic Gaussian initialisation widely used in neural networks.
Crucially, Tanaka’s large-$N$ analysis shows the resulting HCIZ exponent depends only on the eigenvalue spectrum, so the final integrated–$R$ expression should remain applicable even when full rotational invariance is later broken in training.

%%\paragraph{Representing $d\mu(\AMAT)$ with $d\mu(\WMAT)$ using the source matrix $\mathbf{D}$.}
\paragraph{Using a source matrix $\mathbf{D}$ to represent $d\mu(\AMAT)$ with $d\mu(\WMAT)$.}

We consider all matrices $\mathbf{\AMAT}$ with the same limiting spectral density, $\rho_{\AMAT}^{\infty}(\lambda)$,
as the limiting (\emph{empirical}) ESD of the \Teacher.
That is, we want $\rho_{\AMAT}^{\infty}(\lambda)=\rho^{\infty}_{\WMAT}(\lambda)$, where $\TMAT=\WMAT$.
%\nred{Comment on the nature of the randomness required to be invariant under unitary transformations.}
Of course, there are infinitely many weight matrices $\mathbf{W}$ with the same $M$ eigenvalues, $\left\{ \lambda_{i} \right\}_{i=1}^{M}$, as the \Teacher.
Let us specify these matrices with the measure $d\mu(\mathbf{W})$.
Doing this lets us then write the measure $d\mu(\mathbf{\AMAT})$ in terms of $d\mu(\mathbf{W})$ as:
\begin{equation}
\label{eqn:dmuA}
d\mu(\mathbf{\AMAT}) 
   := e^{- \frac{\beta}{2} Tr[\mathbf{W}\mathbf{D}\mathbf{W}^{\top}]} d\mu(\mathbf{W})  ,
\end{equation}
where $\mathbf{D}$ is some $M \times M$ matrix, called the \SourceMatrix, to be specified below,
and the $\tfrac{1}{2}$ here as well.
Indeed, the key idea here will be to define $\mathbf{D}$ in such a way as to obtain the desired final result.
Notice also that we have added a $\beta$ term; this will be factored out later.
%Notice that we  define $\mathbf{\AMAT}$ through this change of measure, and 
%$\mathbf{\AMAT}$ should be an $M \times M$ matrix (i.e. $\mathbf{\AMAT}:=\tfrac{1}{N}\mathbf{S}^{T}\mathbf{S}$),
%but we could also let $\mathbf{\AMAT}$ be an $N \times N$ matrix, (i.e $\mathbf{\AMAT}:=\tfrac{1}{N}\mathbf{S}\mathbf{S}^{T}$),
%simply with $N-M$ zero eigenvalues.
%\paragraph{Unitary‐invariant randomness.}
%To ensure the ensemble contains \emph{no information beyond the fixed spectrum} $\{\lambda_i\}$, impose invariance under every orthogonal rotation:
%\begin{equation}
%  d\mu(\mathbf{W}) \;=\; d\mu\!\bigl(\mathbf{U}\mathbf{W}\mathbf{U}^{\top}\bigr),
%  \qquad
 % \forall\,\mathbf{U}\in O(M).
%\end{equation}
%Haar‐averaging over $\mathbf{U}$ renders all eigenvector choices equally likely, so the HCIZ integral collapses to a pure eigenvalue problem; this isotropy drives the saddle‐point/large‐deviation steps in Eqns.~\ref{eqn:ZD_step1}–\ref{eqn:ZD_step2}.


We can now represent the partition function
$\ZD$, by inserting \EQN~\ref{eqn:dmuA} into \EQN~\ref{eqn:hciz_def}.
$\ZD$ is now defined as an integral over all
possible (\Teacher) weight matrices $\mathbf{W}$
\begin{equation}
  \label{eqn:ZD0}
    \ZD=\int d\mathbf{W}\exp[\frac{\beta}{2}\left
    (\Trace{\mathbf{W}^{\top}\AMAT_{2}\mathbf{W}}-\Trace{\mathbf{W}\mathbf{D}\mathbf{W}^{\top}}  
    \right)]  ,
\end{equation}
%where the matrix $\mathbf{D}$ is the called the \SourceMatrix.
\michaeladdressed{@charles: What is this a ``modification of? I get it if it is just a regularized objective, with LAgrange parameters to be determined; but are we going to use it as a generating function or partition function of some effective physical system? Maybe a sentence or two saying why doing this modification makes sense. This is probably just going to be a vector of prices or lagrange parameters that control the constraint satisfaction?}
\michaeladdressed{Also, $Z(D)$ versus $\ZD$?}
Observe that this integral only converges when all the eigenvalues of $\DMAT$, 
$\left\{ \DeltaMu \right\}_{\mu=1}^{M}$, 
are larger than the maximum eigenvalue of $\mathbf{\AMAT}$, i.e., when $\DeltaMu >\lambda_{max}$, for $\mu\in[1,M]$
(although below this will become $\beta\DeltaMu >\lambda_{max}$).
Later, we will place $\mathbf{D}$ in diagonal form, and we will obtain an explicit expression for its $M$ eigenvalues in terms of the $M$ non-zero eigenvalues of $\mathbf{X}$.
The eigenvalues of $\mathbf{D}$ will turn out to Lagrange Multipliers, needed later.


\paragraph{The Saddle Point Approximation (SPA) and the Large Deviation Principle (LDP).}

To evaluate the large-$N$ case of $\IZG$ (see \ref{eqn:hciz_def2},~\ref{eqn:hciz_def3}), 
we assume that the distribution of possible \Teacher correlation matrices,
$\mu(\mathbf{X})$, satisfies a \emph{Large Deviation Principle (LDP)}.
A LDP applies to probability distributions that take an exponential form,
such that $\mu(\mathbf{X})=e^{-N I(\mathbf{X})}d\mu(\mathbf{X})$,
where  $I(\mathbf{X})$ is Entropy or Rate function $I(\mathbf{X})$.
\michael{REF.}
\michael{Maybe write out the general expression, citing that ref, so that it is clear how \ref{eqn:EZD} follows from it, given our setup.}
%\charles{(((
%Is this a resonable assumption ?
%While we model the ESD of $\mathbf{X}$ as a \PowerLaw (PL), it is really
%finite-size distrubtion, best modeled by a Truncated \PowerLaw (TPL),
%either cut off at the largest eigenvalue $\lambda^{max}$
%and/or with an exponential decay~\cite{YTHx22_TR}.
%)))}
In applying a LDP, we effectively restrict measure of student correlation matrices $\mathbf{\AMAT}$
to those most similar to the empirically observed \Teacher correlation matrix $\mathbf{X}$.
%
We expect the measure over all \Teacher correlation matrices
follows an LDP because the ESD is far from Gaussian,
the dominant generalizing components reside in the tail of the ESD,
and at finite-size the tail decays at worst as an exponentially
Truncated \PowerLaw (TPL).

%\nmove{ MOVE THIS BELOW:
%Using the LDP (and following similar approaches in spin glass theory \cite{PP95}),
%below we will show that we can write the expected value of $\ZD$ 
%in terms of $d\mu(\mathbf{X})$ now (as opposed to $d\mu(\mathbf{A})$)
%and in the large-$N$ approximation, as
%\begin{equation}
%  \label{eqn:EZD}
%  \EZDA=
%  \int\exp\left(\beta N Tr[\GNORM(\mathbf{X})]-NI(\mathbf{X})+o(N)\right)d\mu(\mathbf{X})
%\end{equation}
%where $I(\mathbf{X})$ is the \RateFunction, defined below.
%}
%\michael{@charles: Which previous equation from this section does this follow from?}
%\charles{None.  We defined it beliow}


\paragraph{Two steps to evaluate $\Expected[\AMAT]{\ZD}$ in the large-$N$ approximation.}
The goal is to start with \EQN~\ref{eqn:ZD0} and obtain two separate, equivalent
relations, Eqns.~\ref{eqn:ZD_step1} and ~\ref{eqn:ZD_step2}:
%this proceeds in the following two steps:
\begin{enumerate}
   \item
   \textbf{Obtaining an integral transform of $\rho^{\infty}_{\AMAT}(\lambda)$.}
   First, we expand and reduce \EQN~\ref{eqn:ZD0} and evaluate the expected value of
   $\EZDA=\EZDATWO$ in the large-$N$ limit by expressing the $\rho_{\AMAT}(\lambda)$
   for the $N\times N$ matrix $\AMAT=\mathbf{A}_{2}=\tfrac{1}{N}\SMAT\SMAT^{\top}$
   in the continuum representation, i.e., as ]
   $\rho^{emp}_{\AMAT}(\lambda)\rightarrow \rho^{\infty}_{\AMAT}(\lambda)$, to obtain:
   \begin{equation}
      \label{eqn:ZD_step1}
      \lim_{N\gg 1}\dfrac{1}{N}
      \ln\EZDATWO =M\ln(\dfrac{2\pi}{\beta})-\sum_{\mu=1}^{M}\int \ln(\delta_{\mu}-\lambda)\rho^{\infty}_{\AMAT}(\lambda)d\lambda  .
   \end{equation}
   This gives us an $\EZDATWO$ in terms of an integral transform $\rho^{\infty}_{\AMAT}(\lambda)$, which we can model.\footnote{This integral of $\rho^{\infty}_{\AMAT}(\lambda)$  is related to the \emph{Shannon Transform}, an integral transform from information theory that is useful when analyzing the mutual information or the capacity of a communication channel~\cite{Tanaka2007}. }
   \michaeladdressed{I dont think thats obvious, is it? I thought the point was that is is convex/concave when $\delta_{\mu} >\lambda_{max}$, in which case it is a Laplace(?) transformation (which we can then invert)? Or is this something else?}
   \item
   \textbf{Forming the \SaddlePointApproximation (SPA).}
   We evaluate \EQN~\ref{eqn:ZD0} as the expected value of $\EZDA=\EZDAONE$
   for the $M \times M$ matrix $\AMAT=\mathbf{A}_{1}=\tfrac{1}{N}\SMAT^{\top}\SMAT$
   (but explicitly in terms of $d\mu(\mathbf{X})$).
   Then, taking in the large-$N$ approximation using the SPA,
    (and which can be done implicitly using the LDP), we obtain
   \begin{equation}  
  \label{eqn:ZD1} 
  \lim_{N \gg 1} \EZDAONE\simeq\int  \exp\left(\beta N\Trace{\GFANCY}\right)d\mu(\mathbf{X}) \approx \exp(\beta N\GMAX)
\end{equation}
  where  $\GFANCY$ depends on $\GNORM(\mathbf{X})$, and $\GMAX=\sup_{\XMAT}\GFANCY$.
  We can then write
   \begin{equation}
      \label{eqn:ZD_step2}
      \lim_{N \gg 1}\dfrac{1}{N}\ln\EZDAONE \approx \beta\GMAX  ,
   \end{equation}

 \item
 \textbf{Finding the Inverse Legendre Transform.}
  To do this, we now equate
  \begin{equation}
  \lim_{N \gg 1}\frac{1}{N}\ln\EZDAONE=  \lim_{N \gg 1}\frac{1}{N}\ln\EZDATWO
  \end{equation}
 Then, we can form the
 %with a suitable choice for the source matrix $\mathbf{D}$, we can form the
 inverse Legendre transform  which we will let us relate $\GNORM(\lambda)$ in \EQN~\ref{eqn:hciz} to the integrated \RTransform of $\rho^{\infty}_{\AMAT}(\lambda)$.
\end{enumerate}

\noindent

(See~\ref{sxn:tanaka_end}.)


