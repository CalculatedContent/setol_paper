

\subsection{MLP3 Model Details}
\label{sxn:appendix_MLP3details}
The empirical MLP3 Model implements the assumptions described in Section~\ref{sxn:matgen} used the following 
procedures:

A three-layer \MultiLayerPerceptron was trained for classification on the MNIST dataset\cite{MNIST1998}. The first Fully 
Connected (FC) hidden layer has 300 units, the second FC hidden layer has 100 units, and the third FC layer has ten 
units for classification, matching the ten digit classes of MNIST. Input images are grayscale, and were rescaled to the 
$[0, 1]$ range. Following the keras\cite{keras2015} defaults, the weights were initialized using the Glorot 
Normal\cite{GloBen10} method, and the biases were initialized to $0$. Each model was trained using Categorical Cross 
Entropy as the loss function. The loss function was {\em summed} over each mini-batch, which is the default behavior for 
Keras, rather than being {\em averaged}, which is the default for pytorch\cite{pytorch2019}. 

Optimization was carried out by either Stochastic Gradient Descent (SGD) 
without momentum, or the Adam algorithm \cite{kingma2014_TR}. The \LearningRate (LR) was set to 0.01 for SGD, and 0.001 
for Adam. The LR was held constant, i.e., there was no decay schedule. Each algorithm proceeded epoch by epoch until the 
value of the loss function did not decrease by more than 0.0001 for three consecutive epochs. At each epoch, 
the \WW~ tool was used to compute metrics for each layer. Loss values reported are the average loss per labeled example, 
and not the summed loss over each minibatch. Training loss is averaged over all batches in the epoch, whereas test loss 
is evaluated once at the end of the epoch.

In some experiments, only one layer was trained, while the others were left frozen. In other experiments all layers were 
trained. Models were trained using a series of mini-batch sizes ranging from $1$ to $32$. For each separate training 
run, the models were re-initialized to the same starting random weights, all random seeds were reset, and deterministic 
computations were used to train the models.

Separate notebooks are provided for keras and pytorch implementations of the experiments.


