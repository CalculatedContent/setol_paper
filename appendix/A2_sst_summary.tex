\subsection{Summary of the \StatisticalMechanicsOfGeneralization (\SMOG)}
\label{sxn:summary_sst92}


In this section, we derive the Annealed Hamiltonian for two variants of the ST model, in the high-T limit:
in Appendix~\ref{app:st-gen-err-annealed-ham}, we derive an expression for $\GANR$ for the ST \Perceptron model, when the students and teachers are modeled as $N$-vectors $\WVEC$ (as in~\cite{SST92}); and
in Appendix~\ref{sxn:appendix_Gan}, we derive an expression for $\GANMAT$ for Matrix-Generalized case, i.e., when the students and teachers are modeled as $N \times M$ matrices $\WMAT$ (as our \SETOL requires).
From these, we will obtain expressions for the Average ST \ModelGeneralizationError $\AVGSTGE$ and the Average NN \ModelGeneralizationError $\AVGNNGE$, as well as for the corresponding data-averaged errors.
Although the functional form for these quantities will be the same for the vector case and the matrix case, there are several important differences in the derivation of $\GANMAT$, most notably having to do with a normalization for the weight matrix. 

\michael{@charles: in the interests of being pedantic, we are using $\GANR$ and $\GANMAT$ in at least some places when I think we mean $\GANHT$ and $\GANMATHT$.  I have not made this change consistently below, since I'm not sure how you use them elsewhere, but we should change that everywhere, one way or another , consistently.}
\charles{@michael: I have tried to be very careful about this.  If there are typos, please point them out}


%%\subsubsection{Average Student-Teacher Model Generalization Error $(\AVGSTGE)$ and the Annealed Hamiltonian $(\GANR)$}
\subsubsection{Annealed Hamiltonian $\GANR$ when Student and Teachers are Vectors}
\label{app:st-gen-err-annealed-ham}

In this section, we derive an expression for the Annealed Hamiltonian $\GANR$, 
in the AA and the high-T approximation,
when student and teachers are are modeled as $N$-vectors.
%
From this, we obtain an expression for the data-averaged ST error $\EPSL(R)$, 
which is the same as the expression given in \EQN~\ref{eqn:e0}.


\michael{MM TO DO: Deal with this comment, then remove: This is SST, i.e., for the vector case, correct?  And the point of this section is to derive \EQN~\ref{eqn:Gan4} below and \EQN~\ref{eqn:e0} / \EQN~\ref{eqn:AVGSTGE_R} in the main text, i.e., that $\epsilon = \epsilon(R)$?  }
\charles{@michael: ???}

The procedure starts by computing the associated quenched average of
the \FreeEnergy, defined for the model error as
\begin{align}
\nonumber
\langle-\beta F \rangle_{\AVGNDXI} 
   :=& \langle \ln Z \rangle_{\AVGNDXI} \\ 
\nonumber
   =&\left\langle \int d\mu(\SVEC)e^{-\beta\DETOPSTLL} \right\rangle_{\AVGNDXI}  \\ 
\label{eqn:qaF}
   =& \frac{1}{N}\int d\mu(\NDXI)\ln\int d\mu(\SVEC)e^{-\beta\DETOPSTLL}   ,
\end{align}
where $d\mu(\NDXI):=\prod_{i=1}^{N}d\XI_{i} P(\NDXI)$ (see \EQN~\ref{eqn:pndx}) and
where the data-dependent ST error, $\DETOPSTLL$, is defined in \EQN~\ref{eqn:DE_L},
with an $\mathcal{L}=\ell_2$ loss.
\charles{Do we need a $1/N$ to ensure the free energy is extensive ? I don't think we do because at the end we want to evaluate the
  generalization accuracy, so we need to take the partial w/r.t. $N$. But we need to check this and section 4.2 carefully.
Of course, with the HCIZ integral, we have to be also be careful since we move the $1/N$ from the L.H.S. to the R.H.S of Tanaka just for this reason}
%
If we apply the AA (see \EQN~\ref{eqn:AA}) to \EQN~\ref{eqn:qaF}, then we obtain
\begin{align}
\label{eqn:qaFan}
\langle-\beta F \rangle_{\AVGNDXI} \simeq 
%%   \ln \langle Z \rangle_{\AVGNDXI} = 
   \ln \frac{1}{N}\int d\mu(\NDXI) \int d\mu(\SVEC) e^{-\beta\DETOPSTLL}   .
\end{align}
%
Notice that we have interchanged the logarithm $(\ln)$ and
and the data average (the “disorder average”) 
$\langle\cdots\rangle_{\AVGNDXI}$ over the data; 
this is the essence of the AA, as it lets the disorder fluctuate rather than forcing the system to be quenched to the data.
%
We will now switch the order of integration in \EQN~\ref{eqn:qaFan}, giving
\begin{align}
\label{eqn:qaFan3}
\langle-\beta F \rangle_{\AVGNDXI} \simeq 
%%   \ln \langle Z \rangle_{\AVGNDXI} = 
   \ln \int d\mu(\SVEC)\frac{1}{N}\int d\mu(\NDXI)e^{-\beta\DETOPSTLL)}   .
\end{align}


We now recall the definition of the \AnnealedHamiltonian, $\GANR$ (see \EQN~\ref{eqn:Gan_def} in Section~\ref{sxn:mathP}, which is analogous to \EQN~(2.31) of \cite{SST92}):
\begin{align}
\label{eqn:Gan0}
\beta\GANR = \beta\HAN(\beta,\SVEC,\TVEC):=-\frac{1}{N}\ln \int d\mu(\NDXI)e^{-\beta\DETOPSTLL}  .
\end{align}
where we have denoted the Hamiltonian as $HAN(\beta,\SVEC,\TVEC)$ to indicated the explicit dependence on $\beta$,
and we have added $\beta$ to the R.H.S. to because the L.H.S. is unitless.
%
Using this definition, we can express the \Annealed \PartitionFunction, $Z^{an}_{\ND}$, in the AA in terms of the \Annealed Hamiltonian $\GANR$
\michael{@charles: There the macro is \Annealed and elsewhere it is \AnnealedHamiltonian; does it matter here or elsewhere.}
(as in \EQN~\ref{eqn:Zan_def} in Section~\ref{sxn:mathP}, and as in \EQN~(2.31) of \cite{SST92}:
\begin{align}
 \label{eqn:Zan}
Z^{an}_{\ND}:=\ND\langle Z\rangle_{\AVGNDXI}=\int d\mu(\SVEC)e^{-\ND\beta\HAN(\beta,\SVEC.\TVEC)}  .
\end{align}

\michael{Here, we have a subscript $N$ and in the main text I think we have a subscript $n$ for the same thing. Is it different, or is it a typo.}
\charles{@michael: I used $n$ for the intro 4.1/4.2, and large $N$ for the specific cases here and above.  I thought it would be clearer so things dont get mixed up. Not sure about $\ND$. }
Following Section~\ref{sxn:mathP_errors}, we can write the \emph{Average Model \TrainingError} $\AVGSTTE$, in the AA,
in terms of \Annealed \PartitionFunction, $Z^{an}_{\ND}$:
\begin{align}
 \label{eqn:AVGSTTE_AA}
\AVGSTTE:= \frac{1}{\red{\ND}}\dfrac{\partial }{\partial \beta} Z^{an}_{\ND}
\end{align}

%Given $\GANR$ in \EQN~\ref{eqn:Gan0}, and in analogy with \EQN~\ref{eqn:avgte} in Section~\ref{sxn:mathP}, 
%we can obtain the expected value of the data-averaged ST \TrainingError in \EQN~\ref{eqn:ST_train_error} 
%\michael{which eqn precisely}
%\charles{@michael: need to go back and check this since I changed $H\rightarrow\beta H$ everywhere}
%by taking the derivative of $\GANR$ w.r.t $\beta$:
%
%%%\nred{check delta E}
%%%\charles{Need to review this more carefully}
%%%\begin{align}
%%%  \label{eqn:Ga1}
%%%\red{\Delta E(\SVEC,\TVEC)_{train}}:=\langle  \mathbf{E}_{\ell_2}(\SVEC,\TVEC,\XI) \rangle_{\XItrain} 
%%% =\dfrac{\partial \HAN}{\partial \beta}  .
%%%\end{align}
%%%%
This now lets us write the Average Model \TrainingError $\AVGSTTE$ in the AA in terms of the \AnnealedHamiltonian $\GANR$:
  \begin{align}
  \nonumber
  \AVGSTTE
   =& \dfrac{1}{Z^{an}_{\ND}}\int d\mu(\SVEC)\dfrac{\partial \red{\beta}\HAN}{\partial \beta} e^{-\ND\beta\HAN(\beta,\SVEC,\TVEC)} \\ 
  \label{eqn:EtM2}
   =& \dfrac{1}{Z^{an}_{\ND}}\int d\mu(\SVEC)\langle  \mathbf{E}_{\ell_2}(\SVEC,\TVEC,\XI) \rangle_{\XItrain} e^{-\ND\beta\HAN(\beta,\SVEC,\TVEC)}  ,
  \end{align}
  where $\langle \mathbf{E}_{\ell_2}(\SVEC,\TVEC,\XI) \rangle_{\XItrain}$
  is the specific ST error in the AA for the chosen set of training data $\XItrain$, and is given by
  \nred{CLEAN THIS UP}
 \begin{align}
  \red{ \langle  \mathbf{E}_{\ell_2}(\SVEC,\TVEC,\XI) \rangle_{\XItrain}:==\dfrac{\partial \beta\HAN}{\partial \beta}.}
 \end{align}
 which can be seen from
 \begin{align}
  \label{eqn:partial_Gan}
  \dfrac{\partial \beta\HAN}{\partial \beta}
  &=  \dfrac{\partial }{\partial \beta} \left(-\frac{1}{\ND}\ln \int d\mu(\NDXIn)e^{-\beta\DETOPXILL}\right) \\ \nonumber
  &=  -\frac{1}{\ND}\dfrac{\partial }{\partial \beta} \frac{1}{\ND}\ln \int d\mu(\NDXIn)e^{-\beta\DETOPXILL} \\ \nonumber
  &=  -\frac{1}{\ND} \left( \int d\mu(\NDXIn)e^{-\beta\DETOPXILL}\right)^{-1}\dfrac{\partial }{\partial \beta} \int d\mu(\NDXIn)e^{-\beta\DETOPXILL} \\ \nonumber
    &=  -\frac{1}{\ND} \left(\int d\mu(\NDXIn)e^{-\beta\DETOPXILL}\right)^{-1} \int d\mu(\NDXIn)(-\DETOPXI)e^{-\beta\DETOPXILL}
% &=  \frac{1}{\ND} \langle\DETOPXI\rangle_{\AVGNDXIn}
 \end{align}
This is analagous to defining the average error $\DETOPXI$ as a \ThermalAverage, but as one over the data $\NDXIn$ instead of the weights.

We can also write the \ModelGeneralizationError $\AVGSTGE$  as \BoltzmannWeightedAverage
of $\epsilon(R)$, weighted by $\HAN(\beta,S;T)$ (as in (2.30) in \cite{SST92}), as:
\begin{align}
\label{eqn:EgCanonical}
\AVGSTGE=\dfrac{1}{Z^{an}_{\ND}}\int d\mu(\SVEC)\epsilon(R)e^{-\ND\beta\HAN(\beta,\SVEC,\TVEC)} ,
\end{align}
where $\epsilon(R)=\epsilon(\SVEC)$ is the average ST error, for a fixed \Teacher T,
averaged over \emph{all} possible data inputs, i.e., not just over the specific training data.
(Note that we have dropped the subscript $train$ on $\XI$ since it is clear from the context.)


%Notice that $\GANR$ is like a \emph{data-averaged ST error}, $\epsilon(\SVEC,\TVEC)$, (\EQN (\ref{eqn:STerror}),)
%but now expressed as the logarithm of the (un-normalized) Boltzmann-weighted the data $\XImu$.
%\michael{Charles, I rewrote that sentence, since it was confusing, would you make sure I didnt mess it up.}
%\michael{Is \cite{SST92} the ref for that?}
%%\michael{Is this integral over $d\mu(\NDXI)$ the same as $\sum_{\mu=1}^{N_aD}$ above, or the latter is the empirical version of the former, or are they different?


%  Notice that for the model \GeneralizationError $\MGE$, we evaluate the data-averaged ST error $\epsilon(R)$ directly,
%  hereas for the   model \TrainingError $\MTE$,
%  we have to evaluate $\GANR$ first, which is like an (un-normalized) thermal average over the ST error.
%  (check this; maybe confusing ?)


In the high-T (small $\beta$) limit, the two model errors become formally equivalent
(i.e., $\AVGSTTE=\AVGSTGE$ as $T\rightarrow\infty$).
To show this, consider the \AnnealedHamiltonian $\GANR$, for the \LinearPerceptron with the $\ell_2$ loss. 
As shown in \EQN~(C6) of~\cite{SST92}, this takes a simple analytic form--in the large-$N$ limit--in terms of the ST overlap $R$:
\begin{align}
\label{eqn:Gan2}
\GANR = \dfrac{1}{2}\ln\left[{1+2\beta(1-R)}\right]  .
\end{align}
\EQN~\ref{eqn:Gan2} holds in the AA, but not in the High-T limit.
%%
%%We would like a simple expression for $\langle \mathbf{E}_{\ell_2}(\SVEC,\TVEC,\XI \rangle_{\AVGNDXI}$, so we can use the full form of $\GANR$.
%%\michael{@charles, clarify what is the point of that sentence in the logical flow.  Can't we just remove it, since what we want is \EQN~\ref{eqn:Gan4} below, and to get that it is sufficient to use the following approximation.}
%%However, 
If we evaluate $\dfrac{\partial \HAN}{\partial \beta}$ in the High-T (small $\beta$) limit, 
then we can use the approximation $(\ln[1+x]\simeq x+\cdots)$ to obtain the High-T approximation:
\begin{align}
\label{eqn:Gan3}
\beta\GANR \simeq 
\beta\GANHTR:=\beta(1-R),\;\;\;\;\beta\;\text{small}  .
\end{align}
where we now see that $\GANHTR$ no longer explicitly depends on $\beta$.
%
By \EQN~\ref{eqn:Gan_highT_final}, this gives 
\begin{align}
\label{eqn:Gan4}
\EPSL(R) =
\langle  \mathbf{E}_{\ell_2}(\SVEC,\TVEC,\XI \rangle_{\AVGNDXI} \simeq 1-R\;\;\text{as}\;N\rightarrow\infty  ,
\end{align}
which we recognize as the same as the data-averaged ST error $\epsilon(R)$ in \EQN~\ref{eqn:e0}.
\michael{@charles: also, there is a $1/N$ in \EQN~\ref{eqn:e0}; does that belong here.}


\subsubsection{Annealed Hamiltonian $\GANMAT$ for the Single Layer Matrix-Generalized ST Error}
\label{sxn:appendix_Gan}

In this section, we derive an expression for our matrix generalization of the \AnnealedHamiltonian of the \LinearPerceptron,
in the AA and the high-T approximation, when student and teachers are are modeled as $N \times M$ matrices $\WMAT$,
i.e., $\GANR\rightarrow\GANMAT$.
See \EQN~\ref{xxx}, which has the same form as \EQN~\ref{eqn:Gan0} for the vector case.
\michael{MM TO DO: Something is not right about that, since that is pointing to the definition and not the final expression.}
\charles{@michael: lets disucss we need to close this out}
%
From this, we obtain an expression for the data-averaged ST error $\EPSL(R)$, again when the student and teachers are are modeled as matrices.
There is a subtle normalization issue here, about which we need to be careful.
However, when we normalize appropriately, we will obtain an expression
for ``data-averaged ST error'' (i.e., \EffectivePotential) $\EPSL(R)$ that is of the same form as we obtained in the vector case (as given in \EQN~\ref{eqn:Gan4} and \EQN~\ref{eqn:e0}).
%
The difference will be that in the vector case we take $R=\tfrac{1}{N}\SVEC^{\top}\TVEC$, while in the matrix case we take $R=\tfrac{1}{N}\SMAT^{\top}\TMAT$.

We will need to evaluate an average over the $N$ random $M$-dimensional training data vectors $\XI$,
which are i.i.d Gaussian with $0$ mean and $\sigma^{2}$ variance:
\begin{equation}
  \label{eqn:XInorm}
  \Vert \XI \Vert^2 :=\frac{1}{N} \sum_{\mu=1}^{N} \XI_\mu \XI_\mu^{\top} = \sigma^{2}\IM ,
\end{equation}
where $\XI_{\mu}$ is a vector of length $M$, and $\IM$ is an $M \times M$ identity matrix.
\michaeladdressed{Use $I$ instead of $\IM$ here, and use somethin like $\chi$ for that.}
\charles{@michael: now using $\IM$ ; can change to macro later low priority}
%
The expected value of the squared norm is:
\begin{equation}
\mathbb{E}[\Vert \XI_{\mu} \Vert^2] = M \sigma^2 .
\end{equation}
If we let $\sigma^{2}\sim\tfrac{1}{M}$, 
\michaeladdressed{@charles: why is that a sim, not an equal sign.}
\charles{@michael: might be $\sigma^{2}\sim\tfrac{2}{M}$ I dont recall}
then $\mathbb{E}[\Vert \XI_{\mu} \Vert^2]=1$, i.e., the data vectors can be normalized to $1$.
%
Let the probability distribution over the $N$ data vectors~be
\begin{align}
\nonumber
  P(\NDXI) &= \prod_{\mu=1}^{N} \left( \frac{1}{\sqrt{(2 \pi \sigma^2)^M}} \right) e^{-\frac{\|\XI_{\mu}\|^2}{2 \sigma^2}} \\ 
\nonumber
  &= \left( \frac{1}{\sqrt{(2 \pi \sigma^2)^M}} \right)^{N} \exp\left[-\frac{M}{2}\sum_{\mu=1}^{N}\|\XI_{\mu}\|^2\right] \\ 
  \label{eqn:pndx_vec}
  &= \XINORM \exp\left[-\frac{M}{2}\sum_{\mu=1}^{N}\|\XI_{\mu}\|^2\right] ,
\end{align}
where $M=N_f$ is  the number of features in the data, where the normalization $\XINORM$ is
\begin{align}
\label{eqn:xinorm}
\XINORM 
:=\left( \frac{1}{\sqrt{(2 \pi \sigma^2)^M}} \right)^{N}
 =\left( \frac{M}{2\pi} \right)^{MN/2} .
\end{align}
\michaeladdressed{Is that factor $P(\NDXI)$ due to us working with matrices, i.e., it does not appear with vectors.}
\charles{@michael: see main text section 4, its there}

\paragraph{The Total Data Sample Error $(\DETOPST)$ and the Matrix Normalization}
First, let us express the matrix-generalized \TotalDataSampleError, $\DETOPST$, for a single layer,
in operator form (for each training example)
\begin{align}
 \label{eqn:DETOPST}
 \frac{1}{N}\DETOPST  := \Trace{\IM - \frac{1}{N}\SMAT^{\top}\TMAT} = M - \Trace{\OVERLAP}
\end{align}
where $\IM$ is a diagonal matrix of dimension $M$.
Notice that $\DETOPST$ scales as $N\times M$, the total number of parameters in the system.
Also, importantly, in this representation, the normalization on becomes
\begin{align}
 \label{eqn:matNorm}
\frac{1}{N}\Trace{\SMAT^{\top}\TMAT} = \Trace{\OVERLAP} = M
\end{align}
so that when all $M$ elements overlap, then the error is zero.
%                                                                                                                                                                
We can define the data-dependent form (i.e., in the basis of the data $\XI$) as
\begin{align}
\nonumber
\DETOPNN
   :=& \sum_{\mu=1}^{N} (\XI^{\mu})^{\top} \left( \IM - \frac{1}{N} \SMAT^{\top}\TMAT \right) \XI^{\mu} \\
\label{eqn:DETOPNN}
    =&  \sum_{\mu=1}^{N} \sum_{i,j=1}^{M} \XI_i^{\mu} \left( \delta_{ij} - \frac{1}{N} [\SMAT^{\top}\TMAT]_{ij} \right) \XI_j^{\mu}  .
\end{align}

\paragraph{The \AnnealedHamiltonian (per-parameter, $\HANPP)$}
The definition of the \AnnealedHamiltonian, $\GANMAT$, for the matrix-generalized case must be extended to account for the $M$ parameters per training example, and is now given as
\begin{align}
 \label{eqn:hanpp}
  \GANMAT:=M\HANPP
\end{align}

where the \AnnealedHamiltonian per-parameter, $\HANPP$, is obtained from \EQN~\ref{eqn:Gan0} as
\begin{align}
\label{eqn:Gan_lnI}
\beta\HANPP
   &:=-\frac{1}{N}\ln   \int\mathcal{D}\NDXI \, e^{-\beta \DETOPST} P(\NDXI) \\
\nonumber
   &=-\frac{1}{N}\ln \IH ,
\end{align}
where
\begin{align}
\label{eqn:InsideGan}
\IH := \int\mathcal{D}\NDXI \, e^{-\beta \DETOPST} P(\NDXI) .
\end{align}
That is, $\HANPP$ represents the Energy or Error each of the $M$ parameters contributes
(integrated over the $N$ training examples $\NDXI$).

The goal will be to derive the high-Temperature \AnnealedHamiltonian, $\GANMATHT$, which is now
\begin{align}
 \label{eqn:hanpp}
  \GANMATHT:=M\HANPPHT
\end{align}

Note that if we were to evaluate the trace of $\HANPPHT$, then we could also define
a matrx-generalized \EffectivePotential,
\begin{align}
  \label{eqn:EPSL_mat}
  \EPSL(\OVERLAP):=\red{\cancel{M}}\Trace{\HANPPHT},
\end{align}
which would be like a mean-field potential, now summed over all the $M$ parameters.

%%We now evaluate the integral over $d\mu(\NDXI)$ in \EQN~\ref{eqn:Gan0}. 

To evaluate the integral, notice that $\IH$ is really just an average over i.i.d. data, and so it is just product over $N$ independent terms
\begin{align}
\IH := \int\mathcal{D}\NDXI  e^{-\beta \DETOPST} P(\NDXI)  \rightarrow\left[\int\mathcal{D}\XI \;[\cdots]\; \right]^{N} ,
\end{align}
as in \EQN~\ref{eqn:I_4} below.
\michael{I don't know what ``as in'' means.  We should not be forward pointing in a derivation.  If we mean that we will use this below, let's have a separate sentence that makes that explicit.}
\charles{@michael: fix it}
Moreover, when taking $\ln \IH$, the $N$ term pulls down and become a prefactor
\begin{align}
-\ln \IH = -\ln\left[\int\mathcal{D}\XI \;[\cdots]\; \right]^{N}= -N\ln\left[\int\mathcal{D}\XI \;[\cdots]\; \right] .
\end{align}
Thus, as with the vector case, $\GANMAT$ is like a mean-field average over the data $\XI$, indepedent of the sample size $N$.
Also, since the final result must scale as $N\times M$, the integral should scale as $M$, i.e.,
$\left[\int\mathcal{D}\XI \;[\cdots]\; \right]\sim M$.


If we substitute $\DETOPNN$, \EQN~\ref{eqn:DETOPNN}, into the integral $\IH$, \EQN~\ref{eqn:InsideGan}, then we obtain
\begin{align}
\label{eqn:I_1} 
\IH 
  & =  \int \mathcal{D}\NDXI \, \exp \left( -\beta\sum_{\mu=1}^{N} (\XI^{\mu})^{\top} \left(\IM-\frac{1}{N} \SMAT^{\top}\TMAT \right) \XI^{\mu} \right) P(\NDXI)  \\
\nonumber
%%  \IH
  & =  \int \mathcal{D}\NDXI \, \exp \left( -\beta\sum_{\mu=1}^{N} (\XI^{\mu})^{\top} \left(\IM-\frac{1}{N} \SMAT^{\top}\TMAT \right) \XI^{\mu} \right) \NORM \exp\left( - \sum_{\mu=1}^{N} \frac{\|\XI^{\mu}\|^2}{2 \sigma^2} \right)  \\
    \nonumber
    %%  \IH
  &= \red{\NORM} \int \mathcal{D}\NDXI \, \exp \left(
    -\beta\sum_{\mu=1}^{N} (\XI^{\mu})^{\top} (\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT) (\XI^{\mu}) 
    - \sum_{\mu=1}^{N} \frac{\|\XI^{\mu}\|^2}{2 \sigma^2} \right) \\ 
\nonumber
  &= \red{\NORM} \int \mathcal{D}\NDXI \, \exp \left(
    -\frac{1}{2\sigma^2}\sum_{\mu=1}^{N}2\beta\sigma^{2} (\XI^{\mu})^{\top} (\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT) (\XI^{\mu}) 
    +  \Vert\XI^{\mu}\Vert^{2} \right) \\ 
\label{eqn:I_3} 
  &= \red{\NORM} \int \mathcal{D}\NDXI \, \exp \left(
    -\frac{1}{2\sigma^2}      
      \sum_{\mu=1}^{N}
          (\XI^{\mu})^{\top}[
      2\beta\sigma^{2} (\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT)+\IM] (\XI^{\mu})\right)  .
\end{align}
%
By combining the exponents, we obtain
\begin{align}
\nonumber
\IH
  &=  \red{\NORM}\int \mathcal{D}\NDXI 
  \exp\left[
    -\frac{1}{2\sigma^2}\sum_{\mu=1}^{N}(\XI^{\mu})^{\top}
    \left(\mathbf{M}
    \right)
    \XI^{\mu}
    \right ]\\ 
\label{eqn:I_4} 
  &=  \red{\NORM}\int \mathcal{D}\XI  
 \exp\left[
    -\frac{1}{2\sigma^2}(\XI)^{\top}
    \left(\mathbf{M}
    \right)
    \XI
    \right]^{N}   ,
\end{align}
where $\mathbf{M}=2\beta\sigma^{2}(\IM - \tfrac{1}{N}\SMAT^{\top}\TMAT)+\IM$ is an $M \times M$ matrix.

We now use the familiar property of multi-variant Gaussian integrals,
\begin{align}
\label{eqn:det_M}
\int d\mathbf{x}  e^{-\frac{1}{2\sigma^{2}}(\mathbf{x})^{\top}\mathbf{M}(\mathbf{x}) } = (2\pi\sigma^{2})^{m/2}\frac{1}{\sqrt{\Det{ \mathbf{M}}}}
\end{align}
where $\mathbf{x}$ is an $m$-dim vector (with zero mean),
and $\mathbf{M}$ is a square postive-definite matrix, and $\Det{ \mathbf{M}}$ is the determinant of $\mathbf{M}$.
\charles{We can simplify this considerably if we treat $\NORM$ above first}
%
Using \EQN~\ref{eqn:det_M}, we can rewrite $\IH$ in \EQN~\ref{eqn:I_4} as
\begin{align}
\label{eqn:I_5}
\IH &=   \red{\NORM}\left[\frac{(2\pi\sigma^{2})^{M/2}}{\sqrt{\Det{ \mathbf{M}}}}\right]^{N} \\ \nonumber
    &=   \red{\NORM}(2\pi\sigma^2)^{NM/2}\left[
         \sqrt{\Det{ 2\beta\sigma^2(\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT)+\IM}}\right]^{-N}  \\
%%\end{align}
%%%
%%\begin{align}
\label{eqn:I_6}
%%\IH 
   &=  \left( \frac{1}{2\pi\sigma^{2}} \right)^{MN/2}
       (2\pi \sigma^{2})^{M/2}
         \left[\sqrt{\Det{ \IM + 2\beta\sigma^{2}(\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT)}}\right]^{-N}  ,
\end{align}
where \EQN~\ref{eqn:I_6} follows by inserting $\XINORM$ from \EQN~\ref{eqn:xinorm}.
We can now identify $\sigma^{2}=\tfrac{1}{M}$ to obtain
\begin{align}
\nonumber
\IH &= \left[\sqrt{\Det{ \IM + 2\beta\sigma^{2}(\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT)}}\right]^{-N}   \\ 
\nonumber
    &= \left[\sqrt{\Det{ \IM  + \tfrac{2\beta}{MN}(\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT})}\right]^{-N} \\ 
\label{eqn:I_7}
    &= \left[\Det{ \IM  + \tfrac{2\beta}{M}(\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT})\right]^{-N/2}  .
\end{align}

\paragraph{Large-$N$ Approximation.}
Using the expression
%%\begin{align}
$
\det(\IM+\epsilon\mathbf{\Omega})\approx1+\epsilon\Trace{\mathbf{\Omega}}  ,
$
%%\end{align}
\michaeladdressed{@charles: I inlined that equation.}
which holds for an arbitrary matrix $\Omega$ for small $\epsilon$,
we can evaluate the determinant in \EQN~\ref{eqn:I_7} in the large-$N$ approximation, 
which gives
\begin{align}
  \label{eqn:I_8}
  \IH &\approx  \left[1  + \tfrac{2\beta}{M}(\Trace{\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT})\right]^{-N/2}   .
\end{align}
Inserting this into \EQN~\ref{eqn:Gan_lnI}, we obtain
\begin{align}
\label{eqn:Gan_lnI_final_mwm}
\beta\HANPP
   &=-\frac{1}{N}\ln \left[1  + \tfrac{2\beta}{M}(\Trace{\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT})\right]^{-N/2}   .
\end{align}
\michaeladdressed{@charles: I just added that equation.}


  
\paragraph{Matrix-Generalized ST Error $\GANMATHT$ when $M=1$.}
To start, observe that when $M=1$, \EQN~\ref{eqn:Gan_lnI_final_mwm} becomes
\begin{align}
\nonumber
\beta\GANMAT\vert_{M=1}
&= \beta\HANPP|\vert_{M=1}  \\ 
  &=  -\frac{1}{N}\ln  \left[1  + 2\beta(1-\tfrac{1}{N}\SMAT^{\top}\TMAT)\right]^{-N/2} \\ 
\nonumber
  &= \frac{1}{2}\ln  \left[1  + 2\beta(1-\tfrac{1}{N}\SMAT^{\top}\TMAT)\right] \\ 
\label{eqn:GANmat_m_equals_1}
  &=  \frac{1}{2}\ln \left[1 + 2\beta(1-\mathbf{R})\right]  ,
\end{align}
meaning that \EQN~\ref{eqn:Gan_lnI_final_mwm} reduces to \EQN~\ref{eqn:Gan2}, as desired.

\paragraph{FIX THIS SECTION}
\charles{We want $\GANMATHT$ to represent the total energy per training example, so it should NOT include the $\tfrac{1}{M}$
FIX THIS}
This ensures the Hamiltonian scales as $M$ so the Free Energy scales as $N \times M$, the
number of free paramaters in the system.
Notice that for the final \LayerQualitySquared Hamiltonian $\HBARE$, this will change.
\michael{MM TO DO: adjust phrasing to say what the dimensions of those matrices are, once I figure out how to describe $\mathbf{A}_{1}$ versus $\mathbf{A}_{2}$ above, maybe explicit there that there are two different $\mathbf{A}$ matrices.}
\charles{@michael: working on this, but IDK its  about A1 and A2; REWORKING THIS:}
\begin{align}
  \label{eqn:I_9}
    \IH\approx  \left[1  + \red{\frac{2\beta}{M}}\operatorname{Tr}[(\red{\IM}-\tfrac{1}{N}\SMAT^{\top}\TMAT)\right]^{-N/2} ,
\end{align}
for any $M>1$.
Given this, it follows from \EQN~{eqn:HANPP} and \EQN~\ref{eqn:Gan_lnI} that
\begin{align}
\label{eqn:GANmat}
\beta\GANMAT = \beta M \HANPP
  &=  \frac{M}{2}\ln \left[1 + \red{\frac{2\beta}{M}}(\IM-\mathbf{R})\right]  ,
\end{align}
\michaeladdressed{@charles: I added that equation.}
which is of the same functional form as \EQN~\ref{eqn:Gan2}, as desired:
\begin{align}
\label{eqn:GANHTmatR}
\GANMATHT = \IM-\OVERLAP = M-\OVERLAP
\end{align}

\michaeladdressed{MM TO DO: Revisit the following two paragraphs and where to put them, since at least some of this should be somewhere in the main text.}

% We will use this normalization for both the \TRACELOG  condition and deriving the
%  \Quality (Squared) \GeneratingFunction $\IZG$ in Section~\ref{sxn:matgen}.
%  In contrast, for modeling the \RTransform in SubSection~\ref{sxn:r_transforms},
%  we will use the original normalization, and divide the correlation matrix $\XMAT$
%  (or, really, $\XECS$) by $M$ when computing the matrix moments.
%  We make these choices because this is the normalization currently used
%  in the opensource~\WW package for these two features.


This form of the Hamiltonian, $\GANMAT$, however, is not symmetric, and we will
eventually want a symmetric operator or matrix.
Fortunately, the high-T form, $\GANMATHT$, can be made symmetric, as
shown below.
