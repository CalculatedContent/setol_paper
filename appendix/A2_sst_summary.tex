\subsection{Summary of the \StatisticalMechanicsOfGeneralization (\SMOG)}
\label{sxn:summary_sst92}


In this section, we derive the Annealed Hamiltonian for two variants of the ST model, in the high-T limit:
in Appendix~\ref{app:st-gen-err-annealed-ham}, we derive an expression for $\GANR$ for the ST \Perceptron model, when the students and teachers are modeled as $N$-vectors $\WVEC$ (as in~\cite{SST92}); and
in Appendix~\ref{sxn:appendix_Gan}, we derive an expression for $\GANMAT$ for Matrix-Generalized case, i.e., when the students and teachers are modeled as $N \times M$ matrices $\WMAT$ (as our \SETOL requires).
From these, we will obtain expressions for the Average ST \ModelGeneralizationError $\AVGSTGE$ and the Average NN \ModelGeneralizationError $\AVGNNGE$, as well as for the corresponding data-averaged errors.
Although the functional form for these quantities will be the same for the vector case and the matrix case, there are several important differences in the derivation of $\GANMAT$, most notably having to do with a normalization for the weight matrix. 


\subsubsection{Annealed Hamiltonian \texorpdfstring{$\GANR$}{H(R)} when Student and Teachers are Vectors}
\label{app:st-gen-err-annealed-ham}

In this section, we derive an expression for the Annealed Hamiltonian $\GANR$, 
in the AA and the high-T approximation,
when student and teachers are are modeled as $N$-vectors.
%
From this, we obtain an expression for the data-averaged ST error $\EPSL(R)$, 
which is the same as the expression given in \EQN~\ref{eqn:e0}.

The procedure starts by computing the associated quenched average of
the \FreeEnergy, defined for the model error as
\begin{align}
\nonumber
\langle-\beta F \rangle_{\AVGNDXI} 
   :=& \langle \ln Z \rangle_{\AVGNDXI} \\ 
\nonumber
   =&\left\langle \ln \int d\mu(\SVEC)e^{-\beta\DETOPSTLL} \right\rangle_{\AVGNDXI}  \\ 
\label{eqn:qaF}
   =& \frac{1}{N}\int d\mu(\NDXI)\ln\int d\mu(\SVEC)e^{-\beta\DETOPSTLL}   ,
\end{align}
where $d\mu(\NDXI):=\prod_{i=1}^{N}d\XI_{i} P(\NDXI)$ and
where the data-dependent ST error, $\DETOPSTLL$, is defined in \EQN~\ref{eqn:DE_L},
with an $\mathcal{L}=\ell_2$ loss.  \footnote{Also, recall that the Teacher $T$ is fixed and is not learned, so we do not integrate over $d\mu(\TVEC)$. In fact, for the (vector) Perceptron model, the Teacher $T$ weights are simply subsumed into the overlap $R$, and even in the more general cases, such as non-Linear/Boolean Perceptron, in the full Replica calculations, etc.  See the original literature for more details. \cite{Opper01,SST92,EngelAndVanDenBroeck} as well as \cite{MM17_TR}.}
\charles{Do we need a $1/N$ to ensure the free energy is extensive ? I don't think we do because at the end we want to evaluate the
  generalization accuracy, so we need to take the partial w/r.t. $N$. But we need to check this and section 4.2 carefully.
Of course, with the HCIZ integral, we have to be also be careful since we move the $1/N$ from the L.H.S. to the R.H.S of Tanaka just for this reason}
%
If we apply the AA (see \EQN~\ref{eqn:AA} and \EQN~\ref{eqn:Jensens}) to \EQN~\ref{eqn:qaF}, then we obtain
\begin{align}
\label{eqn:qaFan}
\langle-\beta F \rangle_{\AVGNDXI} \simeq 
%%   \ln \langle Z \rangle_{\AVGNDXI} = 
   \ln \frac{1}{\ND}\int d\mu(\NDXI) \int d\mu(\SVEC) e^{-\beta\DETOPSTLL}   .
\end{align}
%
Notice that we have interchanged the logarithm $(\ln)$ and
and the data average (the “disorder average”) 
$\langle\cdots\rangle_{\AVGNDXI}$ over the data; 
this is the essence of the AA, as it lets the disorder fluctuate rather than forcing the system to be quenched to the data.
%
We will now switch the order of integration in \EQN~\ref{eqn:qaFan}, giving
\begin{align}
\label{eqn:qaFan3}
\langle-\beta F \rangle_{\AVGNDXI} \simeq 
%%   \ln \langle Z \rangle_{\AVGNDXI} = 
   \ln \int d\mu(\SVEC)\frac{1}{\ND}\int d\mu(\NDXI)e^{-\beta\DETOPSTLL)}   .
\end{align}


We now recall the definition of the \AnnealedHamiltonian, $\GANR$ (see \EQN~\ref{eqn:Gan_def} in Section~\ref{sxn:mathP}, which is analogous to \EQN~(2.31) of \cite{SST92}):
\begin{align}
\label{eqn:Gan0}
\beta\GANR = \beta\HAN(\beta,\SVEC,\TVEC):=-\frac{1}{\ND}\ln \int d\mu(\NDXI)e^{-\beta\DETOPSTLL}  .
\end{align}
where we have denoted the Hamiltonian as $\HAN(\beta,\SVEC,\TVEC)$ to indicate the explicit dependence on $\beta$,
and we have added $\beta$ to the R.H.S. to because the L.H.S. is unitless.
%
Using this definition, we can express the \Annealed \PartitionFunction, $Z^{an}_{\ND}$, in the AA in terms of the \Annealed Hamiltonian $\GANR$
\michael{@charles: There the macro is \Annealed and elsewhere it is \AnnealedHamiltonian; does it matter here or elsewhere.}
(as in \EQN~\ref{eqn:Zan_def} in Section~\ref{sxn:mathP}, and as in \EQN~(2.31) of \cite{SST92}:
\begin{align}
 \label{eqn:Zan}
Z^{an}_{\ND}:=\ND\langle Z\rangle_{\AVGNDXI}=\int d\mu(\SVEC)e^{-\ND\beta\HAN(\beta,\SVEC.\TVEC)}  .
\end{align}

Following Section~\ref{sxn:mathP_errors}, we can write the \emph{Average Model \TrainingError} $\AVGSTTE$, in the AA,
in terms of \Annealed \PartitionFunction, $Z^{an}_{\ND}$:
\begin{align}
 \label{eqn:AVGSTTE_AA}
\AVGSTTE:= \red{-}\frac{1}{\ND}\dfrac{\partial }{\partial \beta} \ln Z^{an}_{\ND}
\end{align}


This now lets us write the Average Model \TrainingError $\AVGSTTE$ in the AA in terms of the \AnnealedHamiltonian $\GANR$:
  \begin{align}
  \nonumber
  \AVGSTTE
   =& \dfrac{1}{Z^{an}_{\ND}}\int d\mu(\SVEC)\dfrac{\partial \beta\HAN}{\partial \beta} e^{-\ND\beta\HAN(\beta,\SVEC,\TVEC)} \\ 
  \label{eqn:EtM2}
   =& \dfrac{1}{Z^{an}_{\ND}}\int d\mu(\SVEC)\langle  \DELBFell(\SVEC,\TVEC,\XI) \rangle_{\AVGNDXI} e^{-\ND\beta\HAN(\beta,\SVEC,\TVEC)}  ,
  \end{align}
  where $\DELBFell(\SVEC,\TVEC,\XI) \rangle^{\beta}_{\AVGNDXI}$
  is a \ThermalAverage but defined over the specific ST error in the AA for the chosen set of training data $\XItrain=\NDXI$, and is denoted by 
 \begin{align}
   \langle  \DELBFell(\SVEC,\TVEC,\XI) \rangle^{\beta}_{\AVGNDXI}:=\dfrac{\partial \beta\HAN}{\partial \beta}.
 \end{align}
 This is analogous to defining the average error $\DETOPXI$ as a \ThermalAverage, but as one over the data $\NDXIn$ instead of the weights.  This can be seen by expanding \EQN~\ref{eqn:Gan_def}, setting $\WVEC=\SVEC$, fixing $\TVEC$ (implicitly), and taking the partial derivative:
 \begin{align}
  \label{eqn:partial_Gan}
  \dfrac{\partial \beta\HAN}{\partial \beta}
  &=  \dfrac{\partial }{\partial \beta} \left(-\frac{1}{\ND}\ln \int d\mu(\NDXIn)e^{-\beta\DETOPXILL}\right) \\ \nonumber
  &=  -\frac{1}{\ND}\dfrac{\partial }{\partial \beta} \ln \int d\mu(\NDXIn)e^{-\beta\DETOPXILL} \\ \nonumber
  &=  -\frac{1}{\ND} \left( \int d\mu(\NDXIn)e^{-\beta\DETOPXILL}\right)^{-1}\dfrac{\partial }{\partial \beta} \int d\mu(\NDXIn)e^{-\beta\DETOPXILL} \\ \nonumber
    &=  -\frac{1}{\ND} \left(\int d\mu(\NDXIn)e^{-\beta\DETOPXILL}\right)^{-1} \int d\mu(\NDXIn)(-\DETOPXI)e^{-\beta\DETOPXILL}
% &=  \frac{1}{\ND} \langle\DETOPXI\rangle_{\AVGNDXIn}
 \end{align}

We can also write the \ModelGeneralizationError $\AVGSTGE$  as \BoltzmannWeightedAverage
of $\epsilon(R)$, weighted by $\HAN(\beta,S;T)$ (as in (2.32) in \cite{SST92}), as:
\begin{align}
\label{eqn:EgCanonical}
\AVGSTGE=\dfrac{1}{Z^{an}_{\ND}}\int d\mu(\SVEC)\epsilon(R)e^{-\ND\beta\HAN(\beta,\SVEC,\TVEC)} ,
\end{align}
where $\epsilon(R)=\epsilon(\SVEC)$ is the average ST error, for a fixed \Teacher T,
averaged over \emph{all} possible data inputs, i.e., not just over the specific training data.
(Note that we have dropped the subscript $train$ on $\XI$ since it is clear from the context.)


%Notice that $\GANR$ is like a \emph{data-averaged ST error}, $\epsilon(\SVEC,\TVEC)$, (\EQN (\ref{eqn:STerror}),)
%but now expressed as the logarithm of the (un-normalized) Boltzmann-weighted the data $\XImu$.
%\michael{Charles, I rewrote that sentence, since it was confusing, would you make sure I didnt mess it up.}
%\michael{Is \cite{SST92} the ref for that?}
%%\michael{Is this integral over $d\mu(\NDXI)$ the same as $\sum_{\mu=1}^{N_aD}$ above, or the latter is the empirical version of the former, or are they different?


%  Notice that for the model \GeneralizationError $\MGE$, we evaluate the data-averaged ST error $\epsilon(R)$ directly,
%  hereas for the   model \TrainingError $\MTE$,
%  we have to evaluate $\GANR$ first, which is like an (un-normalized) thermal average over the ST error.
%  (check this; maybe confusing ?)


In the high-T (small $\beta$) limit, the two model errors become formally equivalent
(i.e., $\AVGSTTE=\AVGSTGE$ as $T\rightarrow\infty$).
To show this, consider the \AnnealedHamiltonian $\GANR$, for the \LinearPerceptron with the $\ell_2$ loss. 
As shown in \EQN~(C6) of~\cite{SST92}, this takes a simple analytic form--in the \LargeN limit in $\ND$--in terms of the ST overlap $R$:
\begin{align}
\label{eqn:Gan2}
\GANR = \dfrac{1}{2}\ln\left[{1+2\beta(1-R)}\right]  .
\end{align}
\EQN~\ref{eqn:Gan2} holds in the AA, but not in the High-T limit.

If we evaluate $\dfrac{\partial \HAN}{\partial \beta}$ in the High-T (small $\beta$) limit, 
then we can use the approximation $(\ln[1+x]\simeq x+\cdots)$ to obtain the High-T approximation:
\begin{align}
\label{eqn:Gan3}
\beta\GANR \simeq 
\beta\GANHTR:=\beta(1-R),\;\;\;\;\beta\;\text{small}  .
\end{align}
where we now see that $\GANHTR$ no longer explicitly depends on $\beta$.
%
By \EQN~\ref{eqn:Gan_highT_final}, this gives 
\begin{align}
\label{eqn:Gan4}
\EPSL(R) =
\langle  \mathbf{E}_{\ell_2}(\SVEC,\TVEC,\XI \rangle_{\AVGNDXI} \simeq 1-R\;\;\text{as}\;\ND\rightarrow\infty  ,
\end{align}
which we recognize as the same as the data-averaged ST error $\epsilon(R)$ in \EQN~\ref{eqn:e0}.
\michael{@charles: also, there is a $1/N$ in \EQN~\ref{eqn:e0}; does that belong here.}


\subsubsection{Annealed Hamiltonian \texorpdfstring{$\GANMAT$}{H(R)} for the Matrix-Generalized ST Error}
\label{sxn:appendix_Gan}

In this section, we derive an expression for our matrix generalization of the \AnnealedHamiltonian of the \LinearPerceptron,
in the AA and the high-T approximation, when student and teachers are are modeled as $N \times M$ matrices $\WMAT$,
i.e., $\GANR\rightarrow\GANMAT$, which has the same form as \EQN~\ref{eqn:Gan0} for the vector case.

From this, we obtain an expression for the data-averaged ST error $\EPSL(R)$, again when the student and teachers are are modeled as matrices.
There is a subtle normalization issue here, about which we need to be careful.
However, when we normalize appropriately, we will obtain an expression
for ``data-averaged ST error'' (i.e., \EffectivePotential) $\EPSL(R)$ that is of the same form as we obtained in the vector case (as given in \EQN~\ref{eqn:Gan4} and \EQN~\ref{eqn:e0}).
%
The difference will be that in the vector case we take $R=\tfrac{1}{N}\SVEC^{\top}\TVEC$, while in the matrix case we take $R=\tfrac{1}{N}\SMAT^{\top}\TMAT$.

We will need to evaluate an average over the $\ND$ random $M$-dimensional training data vectors $\NDXI$,
which are i.i.d Gaussian with $0$ mean and $\sigma^{2}$ variance: 
\begin{equation}
  \label{eqn:XInorm}
  \Vert \NDXI \Vert^2 :=\sum_{\mu=1}^{\ND} \XI_\mu \XI_\mu^{\top} = \sigma^{2}\IM ,
\end{equation}
where each $\XI_{\mu}$ is a vector of length $M$, and $\IM$ is an $M \times M$ identity matrix.
%
The expected value of the squared norm is:
\begin{equation}
\mathbb{E}[\Vert \XI_{\mu} \Vert^2] = M \sigma^2 .
\end{equation}
If we let $\sigma^{2}\sim\tfrac{1}{M}$, 
then $\mathbb{E}[\Vert \XI_{\mu} \Vert^2]=1$, i.e., the data vectors can be normalized to $1$.
%
Let the probability distribution over the $N$ data vectors~be
\begin{align}
\nonumber
  P(\NDXI) &= \prod_{\mu=1}^{\ND} \left( \frac{1}{\sqrt{(2 \pi \sigma^2)^M}} \right) e^{-\frac{\|\XI_{\mu}\|^2}{2 \sigma^2}} \\ 
\nonumber
  &= \left( \frac{1}{\sqrt{(2 \pi \sigma^2)^M}} \right)^{\ND} \exp\left[-\frac{M}{2}\sum_{\mu=1}^{\ND}\|\XI_{\mu}\|^2\right] \\ 
  \label{eqn:pndx_vec}
  &= \XINORM \exp\left[-\frac{M}{2}\sum_{\mu=1}^{\ND}\|\XI_{\mu}\|^2\right] ,
\end{align}
where $M=N_f$ is the number of features in the data, where the normalization $\XINORM$ is
\begin{align}
\label{eqn:xinorm}
\XINORM 
:=\left( \frac{1}{\sqrt{(2 \pi \sigma^2)^M}} \right)^{\ND}
 =\left( \frac{M}{2\pi} \right)^{\ND M/2} .
\end{align}

\paragraph{The Total Data Sample Error $(\DETOPST)$ and the Matrix Normalization}
First, let us express the matrix-generalized \TotalDataSampleError, $\DETOPST$, for a single layer,
in operator form (for each  of the $\ND$ training examples)
%\begin{align}
% \label{eqn:DETOPST}
% \frac{1}{\ND}\DETOPST  := \Trace{\IM - \frac{1}{N}\SMAT^{\top}\TMAT} = M - \Trace{\OVERLAP} 
%\end{align} 
\begin{align}
 \frac{1}{\ND}\DETOPST := N\Trace{\IM - \frac{1}{N}\SMAT^{\top}\TMAT} = NM - N\Trace{\OVERLAP} 
\end{align}

where $\IM$ is a diagonal matrix of dimension $M$.
Note that the matrices are by default data-averaged empirical quantities, so we can drop the $1/\ND$ on the RHS.

Also, notice that $\DETOPST$ scales as $N\times M$, the total number of parameters in the system.
Also,  importantly, when all the overlaps are perfect, then the error is zero, i.e. if $\Trace{\OVERLAP}=M$ then $\DETOPST=0$.
                                                                        
We can define the data-dependent form (i.e., in the basis of the data $\XI$) as
\begin{align}
\nonumber
\DETOPNN
   :=& N\sum_{\mu=1}^{\ND} (\XI^{\mu})^{\top} \left( \IM - \frac{1}{N} \SMAT^{\top}\TMAT \right) \XI^{\mu} \\
\label{eqn:DETOPNN}
    =&  N\sum_{\mu=1}^{\ND} \sum_{i,j=1}^{M} \XI_i^{\mu} \left( \delta_{ij} - \frac{1}{N} [\SMAT^{\top}\TMAT]_{ij} \right) \XI_j^{\mu}  .
\end{align}

\paragraph{The \AnnealedHamiltonian (per-parameter, $\HANPP)$}
The definition of the \AnnealedHamiltonian, $\GANMAT$, for the idealized case must be extended to account for the $N\times M$ parameters per training example.  We then have that the total energy is then the sum of the entries of $M$ (feature) vectors, as expected by 
\SizeExtensivity in $N$ and
\SizeConsistency in $M$:
\begin{align}
 \label{eqn:hanpp}
 \Trace{\GANMAT}=M\left(N\Trace{\HANPP}\right)
\end{align}
where the \AnnealedHamiltonian per-parameter, $\HANPP$, is obtained from \EQN~\ref{eqn:Gan0} as
\begin{align}
\label{eqn:Gan_lnI}
\beta\HANPP
   &:=-\frac{1}{\ND}\ln   \int\mathcal{D}\NDXI \, e^{-\beta \DETOPST} P(\NDXI) \\
\nonumber
   &=-\frac{1}{\ND}\ln \IH ,
\end{align}
where
\begin{align}
\label{eqn:InsideGan}
\IH := \int\mathcal{D}\NDXI \, e^{-\beta \DETOPST} P(\NDXI) .
\end{align}
That is, $\HANPP$ represents the Energy or Error that each of the $N\times M$ parameters contributes
(averaged over the $N$ training examples $\NDXI$).

The goal will be to derive the high-Temperature \AnnealedHamiltonian, $\GANMATHT$, which is now  defined such that:
\begin{align}
 \label{eqn:hanpp2}
  \Trace{\GANMATHT}:=MN\left(\Trace{\HANPPHT  }\right)
\end{align}

If examining the the trace of $\HANPPHT$, then we can infer the functional form necessary to define
the matrix-generalized \EffectivePotential for each parameter:
\begin{align}
  \label{eqn:EPSL_mat}
  \EPSL(\OVERLAP):=\Trace{\HANPPHT},
\end{align}
which would be like a mean-field potential, but we need something different for the matrix case.

%%We now evaluate the integral over $d\mu(\NDXI)$ in \EQN~\ref{eqn:Gan0}. 

To evaluate the integral, notice that $\IH$ is really just an average over i.i.d. data, and so it is just a product over $\ND$ independent terms ($1$ for each training example).
\begin{align}
\IH := \int\mathcal{D}\NDXI  e^{-\beta \DETOPST} P(\NDXI)  \rightarrow\left[\int\mathcal{D}\XI \;[\cdots]\; \right]^{\ND} ,
\end{align}
as in \EQN~\ref{eqn:I_4} below.
Moreover, when taking $\ln \IH$, the $N$ term pulls down and become a prefactor
\begin{align}
-\ln \IH = -\ln\left[\int\mathcal{D}\XI \;[\cdots]\; \right]^{\ND}= -\ND\ln\left[\int\mathcal{D}\XI \;[\cdots]\; \right] .
\end{align}
Thus, as with the vector case, $\GANMAT$ is like a mean-field average over the data $\XI$, indepedent of the sample size $N$.
Also, since the final result must scale as $N\times M$, the integral should scale as $M$, i.e.,
$\left[\int\mathcal{D}\XI \;[\cdots]\; \right]\sim M$.


If we substitute $\DETOPNN$, \EQN~\ref{eqn:DETOPNN}, into the integral $\IH$, \EQN~\ref{eqn:InsideGan}, then we obtain
\begin{align}
\label{eqn:I_1} 
\IH 
  & =  \int \mathcal{D}\NDXI \, \exp \left( -\beta\sum_{\mu=1}^{\ND} N(\XI^{\mu})^{\top} \left(\IM-\frac{1}{N} \SMAT^{\top}\TMAT \right) \XI^{\mu} \right) P(\NDXI)  \\
\nonumber
%%  \IH
  & =  \int \mathcal{D}\NDXI \, \exp \left( -\beta\sum_{\mu=1}^{\ND} N(\XI^{\mu})^{\top} \left(\IM-\frac{1}{N} \SMAT^{\top}\TMAT \right) \XI^{\mu} \right) \NORM \exp\left( - \sum_{\mu=1}^{N} \frac{\|\XI^{\mu}\|^2}{2 \sigma^2} \right)  \\
    \nonumber
    %%  \IH
  &= \NORM \int \mathcal{D}\NDXI \, \exp \left(
    -\beta\sum_{\mu=1}^{\ND} N(\XI^{\mu})^{\top} (\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT) (\XI^{\mu}) 
    - \sum_{\mu=1}^{\ND} \frac{\|\XI^{\mu}\|^2}{2 \sigma^2} \right) \\ 
\nonumber
  &= \NORM \int \mathcal{D}\NDXI \, \exp \left(
    -\frac{1}{2\sigma^2}\sum_{\mu=1}^{\ND}2\beta\sigma^{2} N(\XI^{\mu})^{\top} (\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT) (\XI^{\mu}) 
    +  \Vert\XI^{\mu}\Vert^{2} \right) \\ 
\label{eqn:I_3} 
  &= \NORM \int \mathcal{D}\NDXI \, \exp \left(
    -\frac{1}{2\sigma^2}      
      \sum_{\mu=1}^{\ND}
          (\XI^{\mu})^{\top}[
      2\beta\sigma^{2}N (\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT)+\IM] (\XI^{\mu})\right)  .
\end{align}
%
By combining the exponents, we obtain
\begin{align}
\nonumber
\IH
  &=  \NORM\int \mathcal{D}\NDXI 
  \exp\left[
    -\frac{1}{2\sigma^2}\sum_{\mu=1}^{N}(\XI^{\mu})^{\top}
    \left(\mathbf{M}
    \right)
    \XI^{\mu}
    \right ]\\ 
\label{eqn:I_4} 
  &=  \NORM\int \mathcal{D}\XI  
 \exp\left[
    -\frac{1}{2\sigma^2}(\XI)^{\top}
    \left(\mathbf{M}
    \right)
    \XI
    \right]^{\ND}   ,
\end{align}
where $\mathbf{M}=2\beta\sigma^{2}N(\IM - \tfrac{1}{N}\SMAT^{\top}\TMAT)+\IM$ is an $M \times M$ matrix.
We now use the familiar property of multi-variant Gaussian integrals,
\begin{align}
\label{eqn:det_M}
\int d\mathbf{x}  e^{-\frac{1}{2\sigma^{2}}(\mathbf{x})^{\top}\mathbf{M}(\mathbf{x}) } = (2\pi\sigma^{2})^{M/2}\frac{1}{\sqrt{\Det{ \mathbf{M}}}}
\end{align}
where $\mathbf{x}$ is an $m$-dim vector (with zero mean),
and $\mathbf{M}$ is a square positive-definite matrix, and $\Det{ \mathbf{M}}$ is the determinant of $\mathbf{M}$.
%
Using \EQN~\ref{eqn:det_M}, we can rewrite $\IH$ in \EQN~\ref{eqn:I_4} as
\begin{align}
\label{eqn:I_5}
\IH &=   \NORM\left[\frac{(2\pi\sigma^{2})^{M/2}}{\sqrt{\Det{ \mathbf{M}}}}\right]^{\ND} \\ \nonumber
    &=   \NORM(2\pi\sigma^2)^{NM/2}\left[
         \sqrt{\Det{ 2\beta\sigma^2N(\IM-\tfrac{1}{}\SMAT^{\top}\TMAT)+\IM}}\right]^{-\ND}  \\
%%\end{align}
%%%
%%\begin{align}
\label{eqn:I_6}
%%\IH 
   &=  \left( \frac{1}{2\pi\sigma^{2}} \right)^{N M/2}
       (2\pi \sigma^{2})^{M/2}
         \left[\sqrt{\Det{ \IM + 2\beta\sigma^{2}N(\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT)}}\right]^{-\ND}  ,
\end{align}
where \EQN~\ref{eqn:I_6} follows by inserting $\XINORM$ from \EQN~\ref{eqn:xinorm}.
We can now identify $\sigma^{2}=\tfrac{1}{M}$ to obtain
\begin{align}
\nonumber
\IH &= \left[\sqrt{\Det{ \IM + 2\beta\sigma^{2}N(\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT)}}\right]^{-\ND}   \\ 
\nonumber
    &= \left[\sqrt{\Det{ \IM  + \tfrac{2\beta}{M}N(\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT})}\right]^{-\ND} \\ 
\label{eqn:I_7}
    &= \left[\Det{ \IM  + \tfrac{2\beta}{M}N(\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT})\right]^{-\ND/2}  .
\end{align}

\paragraph{The High-Temperature Limit.}
In the high-T approximation, $\beta$ becomes small, giving the expression
%%\begin{align}
$
\det(\IM+\epsilon\mathbf{\Omega})\approx1+\epsilon\Trace{\mathbf{\Omega}}  ,
$
%%\end{align}
which holds for an arbitrary matrix $\Omega$ for small $\epsilon$.
Using this, we can evaluate the determinant in \EQN~\ref{eqn:I_7} in the large-$N$ approximation, 
which gives
\begin{align}
  \label{eqn:I_8}
  \IH &\approx  \left[1  + \tfrac{2\beta}{M}N(\Trace{\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT})\right]^{-\ND/2}   .
\end{align}
Inserting this into \EQN~\ref{eqn:Gan_lnI}, we obtain
\begin{align}
\beta\HANPP
   &=-\frac{1}{\ND}\ln \left[1  + \tfrac{2\beta}{M}N(\Trace{\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT})\right]^{-\ND/2}  \\ \nonumber
      &=\frac{1}{2}\ln \left[1  + \tfrac{2\beta}{M}N(\Trace{\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT})\right]
      \label{eqn:Gan_lnI_final_mwm}
\end{align}

This form of the Hamiltonian, $\GANMAT$, however, is not symmetric, and we will
eventually want a symmetric operator or matrix.
Fortunately, the high-T form, $\GANMATHT$, can be made symmetric, as
shown below.  But first, let show that this result is consistent with the our previous Percpetron result.
  
\paragraph{Matrix-Generalized ST Error $\GANMATHT$ for $N=1$.}
To start, observe that when $N=1$, \EQN~\ref{eqn:Gan_lnI_final_mwm} becomes
\begin{align}
\nonumber
\beta\GANMAT\vert_{N=1}
&= \beta\HANPP\vert_{N=1}  \\ 
  &=  -\frac{1}{\ND}\ln  \left[1  + 2\beta\tfrac{1}{M}\Trace{(M-\SVEC^{\top}\TVEC}\right]^{-\ND/2} \\ 
\nonumber
  &= \frac{1}{2}\ln  \left[1  + 2\beta\tfrac{1}{M}\Trace{M-\SVEC^{\top}\TVEC}\right] \\ 
\label{eqn:GANmat_m_equals_1}
  &=  \frac{1}{2}\ln \left[1 + 2\beta(1-R)\right]  ,
\end{align}
where we recall that $\SVEC$ and $\TVEC$ are implicitly normalized to $1/m$, where here $m=M$.  This result shows that \EQN~\ref{eqn:Gan_lnI_final_mwm} reduces to \EQN~\ref{eqn:Gan2}, as desired.

\charles{We want $\GANMATHT$ to represent the total energy per training example, so it should NOT include the $\tfrac{1}{M}$
FIX THIS}
This ensures the Hamiltonian scales as $M$ so the Free Energy scales as $N \times M$, the
number of free paramaters in the system.
Notice that for the final \LayerQualitySquared Hamiltonian $\HBARE$, this will change.
\michael{MM TO DO: adjust phrasing to say what the dimensions of those matrices are, once I figure out how to describe $\mathbf{A}_{1}$ versus $\mathbf{A}_{2}$ above, maybe explicit there that there are two different $\mathbf{A}$ matrices.}
\charles{@michael: working on this, but IDK its  about A1 and A2; REWORKING THIS:}
\begin{align}
  \label{eqn:I_9}
    \IH\approx  \left[1  + \frac{2\beta}{M}N\Trace{\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT}\right]^{-\ND/2} ,
\end{align}
for any $M>1$.
Given this, it follows from \EQN~\ref{eqn:Gan_lnI_final_mwm} and \EQN~\ref{eqn:Gan_lnI} that we can define 
\begin{align}
\label{eqn:GANmat}
\beta\GANMAT \vert_{N=1}
  &:=  \frac{M}{2}\ln\det \left[1 + \frac{2\beta}{M}(\IM-\mathbf{R})\right]  ,
\end{align}
and reduces to the same functional form as \EQN~\ref{eqn:Gan2}, as desired (recalling that $\SVEC$ and $\TVEC$ are implicitly normalized by $M=m$).

To obtain the high-Temperature form, we use
\begin{align}
\ln\det\!\Bigl[\IM + \epsilon\mathbb{M}\Bigr]
\approx \mathrm{Tr}\!\bigl(\epsilon\mathbb{M}\bigr)
\quad(\epsilon\ll 1),
\end{align}
with $\epsilon=\tfrac{2\beta}{M}$ and $\mathbb{M}=\IM-\OVERLAP$.   
We now obtain the following result for the matrix-generalized high-T of $\GANMATHT$ using
\begin{align}
\label{eqn:GANHTmatRN1}
\Trace{\GANMATHT}\vert_{N=1} = \Trace{\IM-\OVERLAP} = M-\Trace{\OVERLAP}
\end{align}

The final expression for $\GANMATHT$ is
\begin{align}
\label{eqn:GANHTmatR}
\GANMATHT = N(\IM-\OVERLAP)
\end{align}

% We will use this normalization for both the \TRACELOG  condition and deriving the
%  \Quality (Squared) \GeneratingFunction $\IZG$ in Section~\ref{sxn:matgen}.
%  In contrast, for modeling the \RTransform in SubSection~\ref{sxn:r_transforms},
%  we will use the original normalization, and divide the correlation matrix $\XMAT$
%  (or, really, $\XECS$) by $M$ when computing the matrix moments.
%  We make these choices because this is the normalization currently used
%  in the opensource~\WW package for these two features.



