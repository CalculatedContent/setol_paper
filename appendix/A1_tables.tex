\subsection{Data Vectors, Weight Matrices, and Other Symbols}
\label{sxn:appendix_A}

See Table~\ref{tab:dimensions} for a summary of various vectors and matrices, including their dimensions;
see Table~\ref{tab:symbols} for a summary of various various symbols used throughout the text; and 
see Table~\ref{tab:energies} for a summary of types of ``Energies'' used throughout the text.

%\paragraph{Dimensions of various matrices and vectors.}
%See Table~\ref{tab:dimensions} for a summary of various matrices and vectors used, including their dimensions.

%\renewcommand{\arraystretch}{1.5} % Increase row spacing
\renewcommand{\arraystretch}{1.2} % Increase row spacing

\begin{center}
\begin{table}[ht]
  \begin{tabular}{| l | c | r |}
    \hline
    Number of NN Layers & index $L$ & $N_{L}$ \\ \hline
    Number of Data Examples & index $\mu$ & $\ND$ \\ \hline
    Number of (input) Features & index $i,j$ & $m$ \\ \hline
    Actual Data (Matrix) & $D$ & $n \times m$ \\ \hline
    Model Data (Matrix) & $\mathcal{D}$ & $n \times m$ \\ \hline    
    \Teacher \Perceptron Weight Vector & $\TVEC$ & $m$ \\ \hline    
    \Student \Perceptron Weight Vector & $\SVEC$ & $m$ \\ \hline        
    Actual Input Data Vector & $\DATA$ & $N_{f}\times 1$ \\ \hline
    Gaussian model of Input Data Vector & $\boldsymbol{{\xi}}_{\mu}$ & $N_{f}\times 1$ \\ \hline
    Actual Input Data Label & $\MY_{\mu}$ & $+1|-1$ \\ \hline
    Model Input Data Label & $y_{\mu}$ & $+1|-1$ \\ \hline      
    General Weight Matrix & $\mathbf{W}$ & $N\times M$ \\ \hline
    General \CorrelationMatrix & $\mathbf{X}=\frac{1}{N}\mathbf{W}^{\top}\mathbf{W}$ & $M\times M$ \\ \hline
    Input Layer Weight Matrix & $\mathbf{W}_{1}$ & $N \times M$ \\ \hline
    Hidden Layer Weight Matrix & $\mathbf{W}_{2}$ & $N\times M$ \\ \hline
    Output Layer Weight Matrix & $\mathbf{W}_{3}$ & $M\times 2$ \\ \hline
    \Teacher Weight Matrix & $\mathbf{T}$ & $N\times M$ \\ \hline
    \Student Weight Matrix & $\mathbf{S}$ & $N\times M$ \\ \hline
    \StudentTeacher Overlap Matrix & $\OVERLAP=\tfrac{1}{N}\mathbf{S}^T\mathbf{T}$ & $M\times M$ \\ \hline              
    Inner \Student \CorrelationMatrix & $\mathbf{A}_M=\tfrac{1}{N}\mathbf{S}^{\top}\mathbf{S}$ & $M\times M$  \\ \hline
    Outer \Student \CorrelationMatrix & $\mathbf{A}_N=\tfrac{1}{N}\mathbf{S}\mathbf{S}^{\top}$ & $N\times N$  \\ \hline
   Inner ~\ECS \Student \CorrelationMatrix & $\AECSM$ & $M\times M$  \\ \hline
   Outer ~\ECS \Student \CorrelationMatrix &$\AECSN$ & $N \times N$  \\ \hline
   ~\ECS \Teacher \CorrelationMatrix & $\XECS$ & $M\times M$  \\ \hline
    \hline
  \end{tabular}
  \caption{Summary of of various vectors and matrices, including their dimensions.}
\label{tab:dimensions}
\end{table}
\end{center}


%\pagebreak
%\paragraph{Various other symbols.}
%See Table~\ref{tab:symbols} for a summary of various other symbols used throughout the text.

\renewcommand{\arraystretch}{1.35} % Increase row spacing

\begin{center}
\begin{table}[ht]
  \begin{tabular}{| l | c |}
    \hline
    \Perceptron \StudentTeacher (ST) Overlap & $R=\SVEC^{\top}\TVEC=\sum_{i}s_{i}t_{i}$ \\ \hline
    \StudentTeacher (ST) Overlap Operator& $\OVERLAP=\tfrac{1}{N}\SMAT^{\top}\TMAT$ \\ \hline       
    Matrix Generalized ST Overlap & $\tfrac{1}{N^2}\OLAPSQD$  \\ \hline
    \StudentTeacher \SelfOverlap & $\eta(\NDXI)=\mathbf{y}^{\top}_{T}\mathbf{y}_{S}$  \\ \hline
    $\ell_2$-Energy or $\ell_2$-Error & $\DETSTLL$ \\ \hline
    $\ell_2$-Energy Operator Form & $\DETOPSTLL:=\sum_{\XI}\DETSTLL$ \\ \hline
    $\ell_2$-Energy Matrix Operator Form & $\DETOPST:=N(\IM-\tfrac{1}{N}\SMAT^{\top}\TMAT)$\\ \hline
    \EffectivePotential  & $\epsilon(R)=\epsilon(S,T)=\langle\DETOPSTLL\rangle_{\AVGNDXI}$ \\ \hline
    \LinearPerceptron  $\epsilon(R)$ at high-T, large-$N$ & $\epsilon(R)=1-R$ \\ \hline
    \AnnealedApproximation (AA)& $\langle\ln Z\rangle_{\AVGNDXI}\approx\ln\langle Z\rangle_{\AVGNDXI}$ \\ \hline
    \AnnealedHamiltonian &  $\GAN$ \\ \hline
    \AnnealedHamiltonian at high-T &  $\GANHT=\EPSL(\WVEC)$ \\ \hline  
   %\Annealed Partition Function & $Z^{an}{_\ND}=\THRMAVGw{n\GAN}$ \\ \hline
   % \Annealed Partition Function at high-T & $\ZANHT=\THRMAVGw{n\EPSLSTw}$ \\ \hline    
    Average \StudentTeacher \GeneralizationError & $\AVGGE^{ST}=\THRMAVG{\epsilon(R)}$ \\ \hline
    Average \StudentTeacher \GeneralizationAccuracy & $1-\AVGGE^{ST}=\THRMAVG{\eta(R)}$ \\ \hline
    Matrix Layer \QualitySquared & $\QT=\THRMAVGIZ{\OLAPTOLAP}$ \\ \hline
    Equivalent Notation for Averages & $\langle\cdots\rangle_{A}=\int\cdots d\mu(\mathbf{A})=\mathbb{E}_{\mathbf{A}}[\cdots]$\\ \hline
    Projection Operator onto ~\ECS & $\mathbf{P}^{ecs} := \sum |\LambdaECS_{i}\rangle\langle\LambdaECS_{i}|,\;i=1\cdots\MECS$ \\ \hline
    Average over~\ECS \Student Correlation Matrices & $ \langle \cdots \rangle^{\beta}_{\AECS}=\int \cdots d\mu(\AECS)=  \mathbb{E}_{\AECS}[\cdots]$ \\    \hline


    \TRACELOG Trace-Log-Determinant Relation & $\Trace{\ln\mathbf{A}}=\ln\det\mathbf{A}$ \\ \hline
    Effective Correlation Measure Transform & $d\mu(\mathbf{S})\rightarrow d\mu(\AECS)$ \\ \hline
    HCIZ Integral (Tanaka's Notation)&${\mathbb{E}_{\AECS}}[\exp(\ND\beta\Trace{\TMAT^{\top}\AMAT\TMAT)}$\\ \hline
      HCIZ Integral (\LargeN in $N$ explicit) & 
$\Expected[\AECS]{\exp\!\bigl(\ND\beta N\,\mathrm{Tr}\bigl[\tfrac{1}{N}\,\TMAT^{\top}\,\AECSN\,\TMAT\bigr]\bigr)}$
\\ \hline
    \LayerQualitySquared \GeneratingFunction & $\IZGINF := \ND\beta \sum_{\mu=1}^{\MECS}\int^{\LambdaECS_{\mu}}_{\LambdaECS_{\min}} dz R(z)$
 \\ \hline
    \GEN & $G_{A}(\gamma)=\int_{\LambdaECSmin}^{\LambdaECS}R_{A}(z)dz$ \\ \hline
%    \RTransform (for Levy Matrix)& $R(z)=\nred{b}z^{\alpha-1}$ where \red{$\alpha=3$?or$2$} \\ \hline
    Eigenvalue for $\mathbf{X}=\tfrac{1}{N}\mathbf{W}^{\top}\mathbf{W}$ & $\lambda$ or $\lambda_{i}$ for $i=1\cdots M$ \\ \hline
    \PowerLaw ESD Tail for $\mathbf{X}$ & $\rho_{tail}(\lambda)\sim\lambda^{-\alpha}$ \\ \hline
    \EffectiveCorrelationSpace ESD Tail for $\mathbf{X}$ & $\rho^{ECS}_{tail}(\LambdaECS),\;\Trace{\ln\prod_{j=1}^{\MECS}\LambdaECS_{j}}=0$ \\ \hline
    Schatten Norm & $\Vert\mathbf{X}\Vert^{\alpha}_{\alpha}=\sum_{j}\lambda_{j}^{\alpha}$ \\ \hline
    ECS Tail Norm & $\tfrac{1}{\MECS}\sum_{i}^{\MECS}\LambdaECS_{i},\;\;\LambdaECS_{i}\in\rho^{ECS}_{tail}(\lambda)$\\ \hline
    Spectral Norm & $\Vert\mathbf{X}\Vert_{\infty}=\lambda_{max}$ \\ \hline
    \WW Start of PL Tail & $\lambda^{PL}_{min}=\lambda_{min}$ \\ \hline
    Start of~\ECS Tail & $\lambda^{ECS}_{min}=\lambda^{|detX|=1}_{min}$ \\ \hline
   ~\ECS-PL Gap between start of tails  & $\Delta\lambda_{min}:=\lambda^{ECS}_{min}-\lambda^{|detX|=1}_{min}$ \\ \hline            
    \WW~\ALPHA (layer) quality metric & $\alpha$ \\ \hline
    \WW~\ALPHAHAT (layer) quality metric & $\hat{\alpha}=\alpha\log_{10}(\lambda_{max})$ \\ \hline
  \end{tabular}
  \caption{Summary of various various symbols used throughout the text. 
          }
\label{tab:symbols}
\end{table}
\end{center}


%%\pagebreak
%%\paragraph{Summary of types of 'Energies'}
%%with simplified examples of the notation, and references to definitions.
%%This is meant to be a guide to understanding how the varies Energies are defined and used.
%%Please refer to the text for exact definitions, dependent variables, etc.

\newcommand{\hthinline}{\rule{0pt}{2.5ex}} % Adjust the 2.5ex to control the amount of space

%\nred{Check $\mathcal{E}(R):=\langle \Delta E\rangle_{\XI}$.  Is this only at AA and highT?}

\begin{table}[ht]
  \hspace*{-2cm}
\begin{tabular}{|p{10cm}|p{6.2cm}|p{2.25cm}|}
\hline
\textbf{Explanation} & \textbf{Examples} & \textbf{Refs} \\
\hline
\textbf{\EnergyLandscape or NN Output function} & $\NNOUT$ & Sec.~\ref{sxn:htsr_setup}\\
\hthinline
The output of the NN given a single input data point. & &\ref{eqn:dnn_energy},\ref{eqn:T_ENN},\ref{eqn:S_ENN},\ref{eqn:nflow} \\
\hline
\textbf{Energy or \StudentTeacher (ST) Error} & $\DEL$, $\DELBF$ & Sec.~\ref{sxn:SMOG_main},~\ref{sxn:summary_sst92} \\
\hthinline
The squared between the output of a \Student NN and its prescribed Teacher label $\Yt$ for a single data point &$\DEL(\XImu):=(\Yt-\SOUT(\XImu))^2 $ & \ref{eqn:DEy} \\
And as the total error for a sample of $\ND$ data points  &  $\DELBF=\sum_{\mu=1}^{n} \DEL(\XImu)$ & \ref{eqn:detopxy}  \\
Or between the outputs of the \Student and the \Teacher NNs. & $
\DELBF:=\sum_{\mu=1}^{n} (\SOUT(\XImu) - \TOUT(\XImu))^2$ & \ref{eqn:DE_L},\ref{eqn:DE},\ref{eqn:DETOPNN} \\
\hline

\textbf{\AnnealedHamiltonian (and Potentials)} & $\HAN$ $(\mathcal{E}(R), \EPSL(R))$ & Sec.~\ref{sxn:mathP_annealed},\ref{sxn:summary_sst92} \\
\hthinline
The \EffectivePotential (for the Error) is defined as:  &
$\EPSL(R)=\tfrac{1}{\ND}\mathcal{E}(R)=\langle \DELBF\rangle_{\AVGNDXI}$ & ~\ref{eqn:epslR}\\
The \AnnealedHamiltonian (for the Error):  &
$\beta\HAN:=\tfrac{1}{\ND}\ln\langle e^{-\beta\DELBF}\rangle_{\NDXI}$ & ~\ref{eqn:Gan_def},~\ref{eqn:Gan0} \\
At high-T, the relation between $\HANHT$ and $\EPSL(R)$ is &
$\HANHT(R):=\EPSLR$ & ~\ref{eqn:Gan_highT} \\
\hthinline
The full ST model Hamiltonian: &
$\beta\HAN(\beta,R):= \tfrac{1}{2}\ln[{1+2\beta(1-R)}]$ & ~\ref{eqn:Gan2} \\
At high-T, the ST model Hamiltonian is: &
$\beta\HANHT(R):=\beta(1-R)$ & ~\ref{eqn:epslR},~\ref{eqn:Gan3} \\
\hthinline
The per-parameter ST model Hamiltonian: &
$\beta\HANPP:=\tfrac{1}{2}\ln[1 + \tfrac{2\beta N}{M}\Trace{\IM-\mathbf{R}}]$  & ~\ref{eqn:Gan_lnI_final_mwm} \\
At high-T, the matrix-generalized ST model Hamiltonian is: &
$\GANMATHT:=N(\IM-\OVERLAP)$&
~\ref{eqn:GANHTmatR} \\
The \LayerQualitySquared Hamiltonian (for the Accuracy): &
$\HBARE:=\OLAPTOLAP$ & ~\ref{eqn:RG},~\ref{eqn:HBARE} \\
\hline
\textbf{Different Average Model Errors} & $\AVGE$ & Sec.~\ref{sxn:mathP_errors}\\
Empirical Training, \Teacher, and Test Errors  & $\AVGEMPTE$, $\AVGE^{T}\approx\AVGEMPGE$ & \ref{eqn:Eg_train},\ref{eqn:Eg_test},\ref{eqn:emp_gen_error}\\
\hthinline
Empirical \GeneralizationGap & $\AVGE^{emp}_{gap}:=\AVGEMPTE-\AVGEMPGE$ & \ref{eqn:gen_gap} \\
\hthinline
\StudentTeacher Training and Generalization Errors  & $\AVGSTTE$, $\AVGSTGE$ & \ref{eqn:EtM2},\ref{eqn:EgCanonical}\\
\hthinline
Neural Network (MLP) Training and Generalization Errors, the (abstract) matrix generalization of ST error
& $\AVGNNTE$, $\AVGNNGE$ & \\
\hline
\textbf{Average Training and/or \GeneralizationError} & $\AVGTE, \AVGGE$ & Sec.~\ref{sxn:mathP_errors}\\ 
In the AA and at High-T, these are the same, 
and are just the \ThermalAverage of $\EPSL(R)$ & $\newline \AVGTE^{an,hT}=\AVGGE^{an,hT}=\THRMAVG{\EPSL(R)}$ &
\ref{eqn:avgte_anhT},\ref{eqn:avgge_anhT}\\
For the ST model, we always assume AA and High-T & $\AVGSTGE=\AVGGE^{an,hT}$ & \\
Likewise, when generalizing $\AVGSTGE$ to matrices,  & $\AVGSTGE\rightarrow\AVGNNGE=\AVGGE^{an,hT}$ & \\
\hline
\textbf{Layer Qualities} & $\Q$, $\QT$ & Sec.~\ref{sxn:matgen_quality_hciz_A},~\ref{sxn:quality} \\
\hthinline
For the ST \Perceptron, $\Q^{ST}$ is the generalization accuracy & $\Q^{ST}:= 1-\AVGSTGE = 1-\THRMAVG{\EPSL(R)}$ & \\
in terms of the \SelfOverlap $\ETA(R)$ & $\Q^{ST}:= \THRMAVG{\ETA(R)}$ & \\
In the AA, and at high-T, $\THRMAVG{\EPSL(R)}=1-R$ & $\Q^{ST}:= 1-\AVGGE^{an,ht} = \THRMAVG{R}$ & \ref{eqn:QST_final} \\
For an MLP / NN, we approximate the total accuracy as a product of layer qualities $\Q$ (in the AA, at high-T) &  $\Q^{NN}:=\prod \Q^{NN}_{L}$ &  \ref{eqn:ProductNorm}\\
For a matrix, the \LayerQualitySquared $\QT$ & $\QT:=\THRMAVGIZ{\OLAPTOLAP}$ &\ref{eqn:QT_1}\\
We approximate $\Q$ using the quality squared & $\Q:=\sqrt{\QT}\approx Q^{NN}_{L}$ & \ref{eqn:QT},\ref{eqn:QT_2}\\
\hline
\end{tabular}
  \caption{Summary of types of ``Energies,'' with simplified examples of the notation, and references to definitions.
          }
\label{tab:energies}
\end{table}

\clearpage
\renewcommand{\arraystretch}{1.0} % reset row spacing
