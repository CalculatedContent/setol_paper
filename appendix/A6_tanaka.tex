\subsection{Tanaka's Result}
\label{sxn:tanaka}

In this section, we will rederive the result by Tanaka~\cite{Tanaka2007,Tanaka2008} that we use in our main derivation,
and, importantly, explain how to address the missing Temperature term.
For completeness, we restate it here using the notation of the main text:
%Tanaka states the as have the following HCIZ (Harish-Chandra/Itzykson-Zuber) integral~\cite{BP2001}
\begin{equation}
  \label{eqn:hciz}
  \lim_{N \gg 1} \frac{1}{N} \ln 
\underbrace{
  \Expected[\mathbf{\AMAT}]{
    \exp\left(\frac{\ND\beta}{2}
    \Trace{\mathbf{W}^{\top} \AMATN \mathbf{W}}
    \right)
  }
 }_{\text{HCIZ Integral}}
  = \frac{\ND\beta}{2} \sum_{i=1}^{M} \GNORM(\lambda_{i})
\end{equation}
where 
$\mathbf{W}$ is the $N\times M$ \Teacher weight matrix, 
$\AMAT=\AMAT_N$ is the $N\times N$ \Student (correlation) matrix, 
but $\beta$ is now the inverse-Temperature (because we are working with real matrices), $\ND$ is the size of the training set, 
and we have added the  $(\tfrac{1}{2})$ prefactor (which will be clear later).
$\GNORM(\lambda)$ is a complex analytic function of the eigenvalues $\lambda$ of (the \Teacher Correlation matrix) $\XMAT$, 
whose functional form will depend on the structure of the limiting form of (the \Student) ESD $\rho_{\AMAT}^{\infty}(\lambda)$.
We may also write it as $\GNORM(\XMAT)$ below.
We call it perhaps somewhat imprecisely a  \emph{\GEN} because the final results for the \LayerQuality  $\Q$ will take the form of a \emph{Tail norm} in many cases.

To apply this result, we note that
while the term $\beta$ is just a constant in~\cite{Tanaka2008}
($1$ or $2$, depending on whether the random matrix is real or complex),
it is not actually inverse Temperature $\beta=\tfrac{1}{T}$ in the original derivation.
Still, we seek a final result that is linear in $\beta=\tfrac{1}{T}$,
so that we can easily evaluate $\QT$ in the high-T limit, i.e.
$\QT
=\tfrac{\partial}{\partial \ND}\tfrac{1}{\beta}\IZGINF
=\tfrac{\partial}{\partial \beta}\tfrac{1}{\ND}\IZGINF$
(see \ref{eqn:IZG_QT}).
We can introduce the new term $\ND\beta$ by
simply changing the scale of $\AMAT_N$ since the final result is a sum of \RTransforms, which by definition
are linear, i.e., $\GNORM(\ND\beta\lambda)=\ND\beta \GNORM(\lambda)$, however, it is instructive
to rederive the final result, with $\ND\beta$ explicitly included.


\paragraph{Notation.}

We start by rewriting the Tanaka result, \EQN~(\ref{eqn:hciz}),
in our notation for the expected value $\Expected[\AMAT]{\cdots}$ operator, as follows:
\begin{equation}
\label{eqn:hciz2}
  \tfrac{1}{2N}\IZGINF = 
  \tfrac{1}{N}\lim_{N\gg 1} \ln \underbrace{ \int d\mu(\mathbf{\AMAT})\left[\exp\left(\frac{\ND\beta}{2}\Trace{\mathbf{W}^{\top}\AMATN\mathbf{W}}\right)\right] }_{\mbox{HCIZ Integral}} 
  = \ND\beta \tfrac{1}{2}\sum_{i=1}^{M}\GNORM(\lambda_{i})   .
\end{equation}
where we have added a $\tfrac{1}{2}$ for technical convenience (to make the connection with the LDP, below).
If we denote the internal HCIZ integral as 
%%MM%% $\HCIZ$,  which
%%MM%% denoting the \PartitionFunction for our matrix generalization of the ST model.
%%MM%% This gives
\begin{equation}
\label{eqn:hciz_def}
  \HCIZ := \int d\mu(\mathbf{\AMAT})\left[\exp\left(\frac{\ND\beta}{2}\Trace{\mathbf{W}^{\top}\AMATN\mathbf{W}}\right)\right]  ,
 \end{equation}
then it holds that 
\begin{equation}
  \label{eqn:hciz_def2}
  \IZG := \tfrac{1}{N}\ln\HCIZ  ,
\end{equation}
from which it follows that %%MM%% or, equivalently,
\begin{equation}
\label{eqn:hciz_def3}
  \IZGINF := \lim_{N \gg 1}\tfrac{1}{N}\ln\HCIZ  .
\end{equation}

The SPA approximates the \PartitionFunction $\HCIZ$, which is now an HCIZ integral,  by its peak value.
For this, $\GNORM(\lambda)$ itself must either not explicitly depend on $N$ and/or at least not grow faster than $N$.
%\nred{For that reason, I think we need to normalize the eigenvalue as $\lambda/N$, or rather $\lambda/M$, or maybe $\lambda/\MECS$, as with the normalization constrain on $\XECS$.}

The trick here is we can choose an \RTransform of $\mathbf{\AMAT}$
that is a simple analytic expression based on the observed
empirical spectral density (ESD) of the $\mathbf{X}$.
And this can readily be done for the ESDs for a wide range of layer weight matrices
observed in modern DNNs because their ESDs are \HeavyTailed \PowerLaw\cite{MM19_HTSR_ICML}.
We can then readily express the \Quality $\Q$ of the \Teacher
layer in a simple functional form, (i.e  an approximate Shatten Norm).

Importantly, the matrices $\mathbf{X}$  and $\mathbf{\AMAT}$ must be well approximated
by low rank matrices since the derivation in Tanaka requires this.  Fortunately,
this appears to be generally true for the layers in very well trained DNNs,
which is what allows us to apply this withing the~\ECS.
In fact, technically we need to integrate over $d\mu(\AECS)$; this is straightforward
as this simply changes the lower bound on the integral from $0\rightarrow\LambdaECSmin$,
both above and in the subsequent derivation.

Finally, we note that $\GNORM(\mathbf{X})$ is kind of \emph{Generalized Norm} because 
it can be evaluated as a sum over a function of the $M$ eigenvalues $\lambda_{\mu}$ of the \Teacher
correlation matrix $\mathbf{X}=\frac{1}{N}\mathbf{W}^{\top}\mathbf{W}$.
$\GNORM(\mathbf{X})$ will turn out to be an expression similar to the Frobenius Norm or the
Shatten Norm of $\mathbf{X}$, depending on the functional form we choose to represent the
limiting form of the \Student ESD, $\rho_{\AMAT}^{\infty}(\lambda)$ and the associated \RTransform $R(z)$, and various approximations made thereafter,


\input{appendix/A61_tanaka}
\input{appendix/A62_tanaka}
\input{appendix/A63_tanaka}
\input{appendix/A64_tanaka}
%\input{appendix/A65_tanaka}
